{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7ec93d9-f040-48dc-9ecf-d14653486529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbo.DimCurrency\t[CurrencyKey] , [CurrencyAlternateKey] , [CurrencyName] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88a1a5b-6c17-4182-96c4-650bd42e64a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, sha2, concat_ws, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- 1. Define Paths and Mappings ---\n",
    "\n",
    "# Source Path (Input)\n",
    "CURRENCY_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/gold/dim/dim_currency/\"\n",
    "\n",
    "# Define the Primary Key column for matching existing rows\n",
    "PK_COL = \"CurrencyName\"\n",
    "\n",
    "# Columns needed for the Gold table\n",
    "# Note: Source columns are in stg_currency, Target columns are in dbo.DimCurrency\n",
    "CURRENCY_MAPPING = [\n",
    "    (\"CurrencyCode\", \"CurrencyAlternateKey\"), # Source PK becomes Target Alternate Key\n",
    "    (\"Name\", \"CurrencyName\"),\n",
    "    (\"ModifiedDate\", \"ModifiedDate\")\n",
    "]\n",
    "\n",
    "# Configure Authentication (use the DFS endpoint)\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "print(\"Configured Spark authentication.\")\n",
    "\n",
    "\n",
    "# --- 2. Read and Prepare Source Data (df_source) ---\n",
    "print(f\"\\nReading Currency data from: {CURRENCY_PATH}\")\n",
    "df_source = spark.read.format(\"delta\").load(CURRENCY_PATH)\n",
    "\n",
    "# Select and Rename columns\n",
    "selected_expr = [col(src).alias(tgt) for src, tgt in CURRENCY_MAPPING if src in df_source.columns]\n",
    "df_source = df_source.select(*selected_expr)\n",
    "\n",
    "# Create a hash for change detection (simplest way to see if data changed)\n",
    "# Use the core business columns for hashing\n",
    "hash_cols = [\"CurrencyAlternateKey\", \"CurrencyName\"]\n",
    "df_source = df_source.withColumn(\n",
    "    \"__row_hash\",\n",
    "    sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in hash_cols]), 256)\n",
    ")\n",
    "\n",
    "# Add required DimCurrency columns (placeholders)\n",
    "df_source = df_source.withColumn(\"LoadTS\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"IsCurrent\", lit(True))\n",
    "df_source = df_source.withColumn(\"CurrencyKey\", lit(None).cast(\"long\")) # Placeholder for surrogate key\n",
    "\n",
    "\n",
    "# --- 3. Check and Create Target Table (if needed) ---\n",
    "target_exists = DeltaTable.isDeltaTable(spark, TARGET_PATH)\n",
    "\n",
    "if not target_exists:\n",
    "    print(f\"Target table not found. Creating initial DimCurrency table at: {TARGET_PATH}\")\n",
    "    # We must include all required DimCurrency columns for the first write\n",
    "    final_cols = [\"CurrencyKey\", \"CurrencyAlternateKey\", \"CurrencyName\", \"ModifiedDate\",\"LoadTS\", \"__row_hash\", \"IsCurrent\"]\n",
    "    \n",
    "    # Select only the required columns and write the initial table\n",
    "    df_source.select(*final_cols).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(TARGET_PATH)\n",
    "    print(\"Initial DimCurrency table created. Skipping merge.\")\n",
    "\n",
    "else:\n",
    "    # --- 4. Perform MERGE (Incremental Upsert) ---\n",
    "    print(f\"\\nTarget table exists. Performing MERGE (Incremental Load)...\")\n",
    "    \n",
    "    dt_target = DeltaTable.forPath(spark, TARGET_PATH)\n",
    "    \n",
    "    # Define the condition to match an existing row (Primary Key)\n",
    "    join_cond = f\"target.{PK_COL} = source.{PK_COL}\"\n",
    "    \n",
    "    # Define the condition to detect a change (Row Hash)\n",
    "    change_cond = \"target.__row_hash != source.__row_hash\"\n",
    "\n",
    "    dt_target.alias(\"target\").merge(\n",
    "        df_source.alias(\"source\"),\n",
    "        join_cond\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=change_cond,\n",
    "        set = {\n",
    "            # Only update if the hash has changed\n",
    "            \"CurrencyName\": \"source.CurrencyName\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\"\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"CurrencyKey\": \"NULL\", # Key will be populated elsewhere or remains NULL\n",
    "            \"CurrencyAlternateKey\": \"source.CurrencyAlternateKey\",\n",
    "            \"CurrencyName\": \"source.CurrencyName\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\",\n",
    "            \"IsCurrent\": \"source.IsCurrent\"\n",
    "        }\n",
    "    ) \\\n",
    "    .execute()\n",
    "    \n",
    "    print(\"MERGE (Incremental Load) complete. ✅\")\n",
    "\n",
    "# --- 5. Final Validation (Optional) ---\n",
    "# Read the final Gold table to check row count\n",
    "# tgt = spark.read.format(\"delta\").load(TARGET_PATH)\n",
    "# print(f\"\\nFinal DimCurrency rows: {tgt.count()}\")\n",
    "# display(tgt.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b450c2e2-666a-40ce-991b-4a6734955272",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765131854517}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read table root\n",
    "df = spark.read.format(\"delta\").load(\"wasbs://project@scrgvkrmade.blob.core.windows.net/gold/dim/dim_currency\")\n",
    "\n",
    "# read a specific partition (also allowed)\n",
    "df_part = spark.read.format(\"delta\").load(\n",
    "    \"wasbs://project@scrgvkrmade.blob.core.windows.net/gold/dim/dim_currency\"\n",
    ")\n",
    "display(df_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2c675d-e450-41f5-ad9c-6ace1f6c1929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, trim, current_timestamp, lit, year, to_timestamp\n",
    "\n",
    "# # --- 1. Define Paths and Mappings (The Setup) ---\n",
    "# # NOTE: In Spark, it's easiest to define multiple paths as separate strings.\n",
    "\n",
    "# # Source Paths (Input)\n",
    "# CURRENCY_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/\"\n",
    "# CURRENCY_RATE_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency_rate/\"\n",
    "\n",
    "# # Target Path (Output - Gold Layer)\n",
    "# TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/gold/dim/dim_currency/\"\n",
    "\n",
    "# # Define the join key (must be present in both source DataFrames)\n",
    "# JOIN_KEY = \"CurrencyCode\" # Assuming this is the common field\n",
    "\n",
    "# # Define Mappings (Source Column Name, Target Column Name)\n",
    "\n",
    "# # Columns to select from the Currency table\n",
    "# CURRENCY_MAPPING = [\n",
    "#     (\"CurrencyCode\", \"CurrencyAlternateKey\"), # Used as the Primary Key\n",
    "#     (\"CurrencyName\", \"CurrencyName\"),\n",
    "#     (\"ModifiedDate\", \"ModifiedDate\"),\n",
    "#     (\"CurrentFlag\", \"IsCurrent\") # Placeholder: assuming IsCurrent logic later\n",
    "# ]\n",
    "\n",
    "# # Columns to select from the Currency Rate table\n",
    "# CURRENCY_RATE_MAPPING = [\n",
    "#     (\"FromCurrencyCode\", \"CurrencyCode\"), # Rename this to match the join key\n",
    "#     (\"AverageRate\", \"LatestRateToUSD_AverageRate\"),\n",
    "#     (\"CurrencyRateDate\", \"LatestRateToUSD_Date\")\n",
    "# ]\n",
    "\n",
    "# # --- 2. Read and Prepare DataFrames ---\n",
    "\n",
    "# # A) Read Currency Data (stg_currency)\n",
    "# print(f\"Reading Currency data from: {CURRENCY_PATH}\")\n",
    "# df_currency = spark.read.format(\"delta\").load(CURRENCY_PATH)\n",
    "\n",
    "# # Apply Currency Mapping (Select and Rename)\n",
    "# selected_currency_expr = [col(src).alias(tgt) for src, tgt in CURRENCY_MAPPING if src in df_currency.columns]\n",
    "# df_currency_prep = df_currency.select(*selected_currency_expr)\n",
    "\n",
    "# # B) Read Currency Rate Data (stg_currency_rate)\n",
    "# print(f\"Reading Currency Rate data from: {CURRENCY_RATE_PATH}\")\n",
    "# df_rate = spark.read.format(\"delta\").load(CURRENCY_RATE_PATH)\n",
    "\n",
    "# # Apply Currency Rate Mapping (Select and Rename)\n",
    "# # NOTE: We select the maximum rate for each currency for the latest date.\n",
    "# df_rate.createOrReplaceTempView(\"stg_currency_rate_temp\")\n",
    "# df_rate_prep = spark.sql(f\"\"\"\n",
    "#     SELECT \n",
    "#         FromCurrencyCode,\n",
    "#         MAX(AverageRate) as LatestRateToUSD_AverageRate,\n",
    "#         MAX(CurrencyRateDate) as LatestRateToUSD_Date\n",
    "#     FROM \n",
    "#         stg_currency_rate_temp\n",
    "#     GROUP BY \n",
    "#         FromCurrencyCode\n",
    "# \"\"\")\n",
    "# # Now rename the key column for joining\n",
    "# df_rate_prep = df_rate_prep.withColumnRenamed(\"FromCurrencyCode\", JOIN_KEY)\n",
    "\n",
    "\n",
    "# # --- 3. Join the DataFrames ---\n",
    "# print(f\"Joining DataFrames on key: {JOIN_KEY}\")\n",
    "\n",
    "# # Perform a LEFT JOIN, keeping all currency codes even if they have no rate\n",
    "# df_joined = df_currency_prep.alias(\"c\").join(\n",
    "#     df_rate_prep.alias(\"r\"),\n",
    "#     col(f\"c.{JOIN_KEY}\") == col(f\"r.{JOIN_KEY}\"),\n",
    "#     \"left\"\n",
    "# ).select(\n",
    "#     # Select all columns from the left (currency) table and the rate columns\n",
    "#     col(\"c.CurrencyAlternateKey\"),\n",
    "#     col(\"c.CurrencyName\"),\n",
    "#     col(\"c.ModifiedDate\"),\n",
    "#     col(\"r.LatestRateToUSD_AverageRate\"),\n",
    "#     col(\"r.LatestRateToUSD_Date\")\n",
    "# )\n",
    "\n",
    "# # --- 4. Final Cleaning and Audit Columns ---\n",
    "# # Add audit and partition columns\n",
    "# df_final = df_joined \\\n",
    "#     .withColumn(\"LoadTS\", current_timestamp()) \\\n",
    "#     .withColumn(\"_year\", year(to_timestamp(col(\"ModifiedDate\")))) \\\n",
    "#     .withColumn(\"IsCurrent\", lit(True)) # Placeholder value\n",
    "\n",
    "# # Final Check\n",
    "# print(f\"Final Gold table schema:\")\n",
    "# df_final.printSchema()\n",
    "# # display(df_final.limit(5)) # Uncomment in Databricks to view data\n",
    "\n",
    "# # --- 5. Write Data to Gold Layer ---\n",
    "# print(f\"Writing data to Gold layer: {TARGET_PATH}\")\n",
    "\n",
    "# # Overwrite mode for simplicity (as in the previous example)\n",
    "# df_final.write.format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"overwriteSchema\", \"true\") \\\n",
    "#     .partitionBy(\"_year\") \\\n",
    "#     .save(TARGET_PATH)\n",
    "\n",
    "# print(\"Data integration and loading complete! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc9a9d6d-98a9-47a0-9df9-acf7af7214ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Simple DimCurrency loader - tailored to your Silver columns\n",
    "# -------------------------\n",
    "from pyspark.sql.functions import col, sha2, concat_ws, current_timestamp, lit\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- 1. Paths & mapping (edit paths if needed) ---\n",
    "CURRENCY_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/\"\n",
    "TARGET_PATH   = \"abfss://project@scrgvkrmade.dfs.core.windows.net/gold/dim/dim_currency/\"\n",
    "\n",
    "# Primary key in the TARGET after mapping (we'll alias CurrencyCode -> CurrencyAlternateKey)\n",
    "PK_COL = \"CurrencyAlternateKey\"\n",
    "\n",
    "# Manual mapping: (source_column_name, target_column_name)\n",
    "CURRENCY_MAPPING = [\n",
    "    (\"CurrencyCode\", \"CurrencyAlternateKey\"),  # CurrencyCode exists in your source\n",
    "    (\"Name\", \"CurrencyName\"),                  # Name exists in your source\n",
    "    (\"ModifiedDate\", \"ModifiedDate\")\n",
    "]\n",
    "\n",
    "# --- 2. Read source (Delta preferred) ---\n",
    "print(\"Reading source from:\", CURRENCY_PATH)\n",
    "try:\n",
    "    df_source = spark.read.format(\"delta\").load(CURRENCY_PATH)\n",
    "    print(\"Loaded source as Delta\")\n",
    "except Exception:\n",
    "    df_source = spark.read.parquet(CURRENCY_PATH)\n",
    "    print(\"Loaded source as Parquet\")\n",
    "\n",
    "print(\"Source columns:\", df_source.columns)\n",
    "\n",
    "# --- 3. Select & rename based on mapping (skip missing) ---\n",
    "selected_expr = []\n",
    "selected_target_cols = []\n",
    "for src_col, tgt_col in CURRENCY_MAPPING:\n",
    "    if src_col in df_source.columns:\n",
    "        selected_expr.append(col(src_col).alias(tgt_col))\n",
    "        selected_target_cols.append(tgt_col)\n",
    "    else:\n",
    "        print(\"Skipping missing source column:\", src_col)\n",
    "\n",
    "# also bring through audit cols that exist in your source\n",
    "for a in [\"__ingest_ts\",\"__source_file\",\"__source_path\",\"__batch_id\",\"__row_hash\"]:\n",
    "    if a in df_source.columns and a not in selected_target_cols:\n",
    "        selected_expr.append(col(a))\n",
    "        selected_target_cols.append(a)\n",
    "\n",
    "if not selected_expr:\n",
    "    raise RuntimeError(\"No valid mapping columns found. Check CURRENCY_MAPPING.\")\n",
    "\n",
    "df = df_source.select(*selected_expr)\n",
    "print(\"Selected columns for processing:\", df.columns)\n",
    "display(df.limit(5))\n",
    "\n",
    "# --- 4. Compute row hash if needed (you already have __row_hash in source; keep it) ---\n",
    "# (We will still compute a business hash for safety)\n",
    "hash_cols = [c for c in (\"CurrencyAlternateKey\",\"CurrencyName\") if c in df.columns]\n",
    "if not hash_cols:\n",
    "    hash_cols = df.columns\n",
    "df = df.withColumn(\"__business_hash\", sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in hash_cols]), 256))\n",
    "\n",
    "# Add LoadTS\n",
    "df = df.withColumn(\"LoadTS\", current_timestamp())\n",
    "\n",
    "print(\"Prepared df columns:\", df.columns)\n",
    "df = df.withColumn(\"IsCurrent\", col(\"IsCurrent\") if \"IsCurrent\" in df.columns else current_timestamp() * 0 + 1)  # placeholder if not present\n",
    "target_exists = True\n",
    "try:\n",
    "    spark.read.format(\"delta\").load(TARGET_PATH)\n",
    "except Exception:\n",
    "    target_exists = False\n",
    "\n",
    "if not target_exists:\n",
    "    print(\"Target not found -> creating initial DimCurrency at:\", TARGET_PATH)\n",
    "    # Build initial DataFrame: CurrencyKey (NULL) + mapped columns + meta\n",
    "    from pyspark.sql.functions import lit\n",
    "    write_df = df.select(*[c for c in selected_target_cols if c in df.columns])\n",
    "    write_df = write_df.withColumn(\"CurrencyKey\", lit(None).cast(\"long\")).withColumn(\"LoadTS\", current_timestamp()).withColumn(\"__row_hash\", col(\"__row_hash\") if \"__row_hash\" in df.columns else col(\"__business_hash\")).withColumn(\"IsCurrent\", lit(True))\n",
    "    # reorder: CurrencyKey first\n",
    "    cols_out = [\"CurrencyKey\"] + [c for c in write_df.columns if c != \"CurrencyKey\"]\n",
    "    write_df = write_df.select(*cols_out)\n",
    "    write_df.write.format(\"delta\").mode(\"overwrite\").save(TARGET_PATH)\n",
    "    print(\"Initial DimCurrency created.\")\n",
    "else:\n",
    "    # MERGE upsert\n",
    "    print(\"Target exists -> merging into:\", TARGET_PATH)\n",
    "    # ensure PK present in df\n",
    "    if PK_COL not in df.columns:\n",
    "        raise RuntimeError(f\"PK '{PK_COL}' not in selected dataframe columns. Make sure CurrencyCode is mapped to {PK_COL}.\")\n",
    "\n",
    "    # create temp view and run SQL MERGE for clarity\n",
    "    tmp_view = \"_src_currency_tmp\"\n",
    "    df.createOrReplaceTempView(tmp_view)\n",
    "\n",
    "    match_cond = f\"target.`{PK_COL}` = source.`{PK_COL}`\"\n",
    "    # update when hash differs\n",
    "    update_pairs = []\n",
    "    for tgt in [c for c in selected_target_cols if c in df.columns]:\n",
    "        update_pairs.append(f\"target.`{tgt}` = source.`{tgt}`\")\n",
    "    update_pairs.append(\"target.LoadTS = source.LoadTS\")\n",
    "    update_pairs.append(\"target.__row_hash = source.__row_hash\")\n",
    "    update_sql = \",\\n    \".join(update_pairs)\n",
    "\n",
    "    insert_cols = [\"CurrencyKey\"] + [c for c in selected_target_cols if c in df.columns] + [\"LoadTS\",\"__row_hash\",\"IsCurrent\"]\n",
    "    insert_cols_sql = \", \".join([f\"`{c}`\" for c in insert_cols])\n",
    "    insert_vals_sql = \", \".join([\"NULL\"] + [f\"source.`{c}`\" for c in selected_target_cols if c in df.columns] + [\"source.LoadTS\",\"source.__row_hash\",\"true\"])\n",
    "\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO delta.`{TARGET_PATH}` AS target\n",
    "    USING (SELECT * FROM {tmp_view}) AS source\n",
    "    ON {match_cond}\n",
    "    WHEN MATCHED AND target.__row_hash <> source.__row_hash\n",
    "      THEN UPDATE SET\n",
    "        {update_sql}\n",
    "    WHEN NOT MATCHED\n",
    "      THEN INSERT ({insert_cols_sql}) VALUES ({insert_vals_sql})\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(merge_sql)\n",
    "    print(\"Merge complete ✅\")\n",
    "\n",
    "# --- 6. Quick validation ---\n",
    "try:\n",
    "    tgt = spark.read.format(\"delta\").load(TARGET_PATH)\n",
    "    print(\"Target rows approx:\", tgt.count())\n",
    "    display(tgt.limit(5))\n",
    "except Exception as e:\n",
    "    print(\"Could not read target for validation:\", e)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f012fd-b403-44a9-95f6-6e033dc1b3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, sha2, concat_ws, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- 1. Define Paths and Mappings ---\n",
    "\n",
    "# Source Path (Input)\n",
    "CURRENCY_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/gold/dim/dim_currency_1/\"\n",
    "\n",
    "# Define the Primary Key column for matching existing rows\n",
    "PK_COL = \"CurrencyAlternateKey\"\n",
    "\n",
    "# Core business columns needed for the Gold table\n",
    "CORE_MAPPING = [\n",
    "    (\"CurrencyCode\", \"CurrencyAlternateKey\"), # Source PK becomes Target Alternate Key\n",
    "    (\"CurrencyName\", \"CurrencyName\"),\n",
    "    (\"ModifiedDate\", \"ModifiedDate\")\n",
    "]\n",
    "\n",
    "# All Audit/Lineage columns you want to carry forward\n",
    "AUDIT_COLUMNS = [\n",
    "    \"_ingestion_ts\",\n",
    "    \"_ingestion_date\",\n",
    "    \"_source_file\",\n",
    "    \"_source_path\",\n",
    "    \"_job_id\",\n",
    "    \"_run_id\",\n",
    "    \"_batch_id\",\n",
    "    \"_year\",\n",
    "    \"__ingest_ts\",\n",
    "    \"__source_file\",\n",
    "    \"__source_path\",\n",
    "    \"__batch_id\",\n",
    "    \"__row_hash\"\n",
    "]\n",
    "\n",
    "# Combine all required columns (including audit)\n",
    "ALL_REQUIRED_COLUMNS = [src for src, tgt in CORE_MAPPING] + AUDIT_COLUMNS\n",
    "\n",
    "\n",
    "# Configure Authentication\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "print(\"Configured Spark authentication.\")\n",
    "\n",
    "# --- 2. Read and Prepare Source Data (df_source) ---\n",
    "print(f\"\\nReading Currency data from: {CURRENCY_PATH}\")\n",
    "df_source = spark.read.format(\"delta\").load(CURRENCY_PATH)\n",
    "\n",
    "# Select and Rename core columns, and select all audit columns\n",
    "selected_expr = [col(src).alias(tgt) for src, tgt in CORE_MAPPING if src in df_source.columns]\n",
    "selected_expr.extend([col(c) for c in AUDIT_COLUMNS if c in df_source.columns])\n",
    "\n",
    "df_source = df_source.select(*selected_expr)\n",
    "\n",
    "# Ensure the required partition column is present (using _year from audit)\n",
    "if \"_year\" not in df_source.columns:\n",
    "    df_source = df_source.withColumn(\"_year\", lit(year(current_timestamp())))\n",
    "\n",
    "\n",
    "# Create a hash for change detection (using the core business columns only)\n",
    "hash_cols = [\"CurrencyAlternateKey\", \"CurrencyName\"]\n",
    "# If __row_hash already exists in the Silver layer, use it. Otherwise, compute it.\n",
    "if \"__row_hash\" not in df_source.columns:\n",
    "    df_source = df_source.withColumn(\n",
    "        \"__row_hash\",\n",
    "        sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in hash_cols]), 256)\n",
    "    )\n",
    "\n",
    "# Add internal Gold audit columns\n",
    "df_source = df_source.withColumn(\"LoadTS\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"IsCurrent\", lit(True))\n",
    "df_source = df_source.withColumn(\"CurrencyKey\", lit(None).cast(\"long\")) \n",
    "\n",
    "\n",
    "# --- 3. Check and Create Target Table (if needed) ---\n",
    "target_exists = DeltaTable.isDeltaTable(spark, TARGET_PATH)\n",
    "\n",
    "# List of columns to insert/update in Gold\n",
    "GOLD_COLUMNS = [\"CurrencyKey\", \"CurrencyAlternateKey\", \"CurrencyName\", \n",
    "                \"LoadTS\", \"__row_hash\", \"IsCurrent\", \"ModifiedDate\",\n",
    "                \"_year\"] + AUDIT_COLUMNS # All audit columns are included here\n",
    "\n",
    "if not target_exists:\n",
    "    print(f\"Target table not found. Creating initial DimCurrency table at: {TARGET_PATH}\")\n",
    "    \n",
    "    # Filter the source DataFrame to only include columns we need for the Gold table\n",
    "    insert_df = df_source.select(*[c for c in GOLD_COLUMNS if c in df_source.columns])\n",
    "    \n",
    "    insert_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"_year\") \\\n",
    "        .save(TARGET_PATH)\n",
    "    print(\"Initial DimCurrency table created. Skipping merge.\")\n",
    "\n",
    "else:\n",
    "    # --- 4. Perform MERGE (Incremental Upsert) ---\n",
    "    print(f\"\\nTarget table exists. Performing MERGE (Incremental Load)...\")\n",
    "    \n",
    "    dt_target = DeltaTable.forPath(spark, TARGET_PATH)\n",
    "    \n",
    "    # Define the condition to match an existing row (Primary Key)\n",
    "    join_cond = f\"target.{PK_COL} = source.{PK_COL}\"\n",
    "    \n",
    "    # Define the condition to detect a change (Row Hash)\n",
    "    change_cond = \"target.__row_hash != source.__row_hash\"\n",
    "\n",
    "    # Create the dictionary for all columns to insert/update\n",
    "    update_set = {\n",
    "        # Core Columns (UPDATE if hash changed)\n",
    "        \"CurrencyName\": \"source.CurrencyName\",\n",
    "        \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "        \"__row_hash\": \"source.__row_hash\",\n",
    "        \"LoadTS\": \"source.LoadTS\",\n",
    "        \n",
    "        # Audit Columns (Update audit fields when business data changes)\n",
    "        \"_ingestion_ts\": \"source._ingestion_ts\",\n",
    "        \"_ingestion_date\": \"source._ingestion_date\",\n",
    "        \"_source_file\": \"source._source_file\",\n",
    "        \"_source_path\": \"source._source_path\",\n",
    "        \"_job_id\": \"source._job_id\",\n",
    "        \"_run_id\": \"source._run_id\",\n",
    "        \"_batch_id\": \"source._batch_id\",\n",
    "        \"_year\": \"source._year\",\n",
    "        \"__ingest_ts\": \"source.__ingest_ts\",\n",
    "        \"__source_file\": \"source.__source_file\",\n",
    "        \"__source_path\": \"source.__source_path\",\n",
    "        \"__batch_id\": \"source.__batch_id\"\n",
    "    }\n",
    "    \n",
    "    # Insert values (for new rows)\n",
    "    insert_values = {col_name: f\"source.{col_name}\" for col_name in GOLD_COLUMNS if col_name in df_source.columns}\n",
    "    \n",
    "    # Execute Merge\n",
    "    dt_target.alias(\"target\").merge(\n",
    "        df_source.alias(\"source\"),\n",
    "        join_cond\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=change_cond,\n",
    "        set = update_set\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values = insert_values\n",
    "    ) \\\n",
    "    .execute()\n",
    "    \n",
    "    print(\"MERGE (Incremental Load) complete. ✅\")\n",
    "    \n",
    "print(\"DimCurrency job finished. All audit columns carried forward.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
