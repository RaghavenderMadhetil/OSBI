{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eeb602f-e303-49b1-89af-8476dc1c93c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. SETUP: Import the necessary tools \n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, trim, to_timestamp, coalesce, sha2, \n",
    "    concat_ws, year, when \n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "import datetime\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "\n",
    "SOURCE_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/HumanResources.Employee/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/dim/dim_employee/\"\n",
    "\n",
    "PK_RAW = \"EmployeeNationalIDAlternateKey,EmployeeAlternateKey\" \n",
    "PRIMARY_KEYS = [c.strip() for c in PK_RAW.split(\",\") if c.strip()]\n",
    "\n",
    "# 1c. Define Column Renaming and Mapping (Bronze to Silver)\n",
    "COLUMN_MAP = {\n",
    "    \"BusinessEntityID\": \"EmployeeAlternateKey\",\n",
    "    \"NationalIDNumber\": \"EmployeeNationalIDAlternateKey\", \n",
    "    \"JobTitle\": \"Title\", \n",
    "    \"HireDate\": \"HireDate\", \n",
    "    \"BirthDate\": \"BirthDate\", \n",
    "    \"MaritalStatus\": \"MaritalStatus\", \n",
    "    \"Gender\": \"Gender\", \n",
    "    \"SalariedFlag\": \"SalariedFlag\",\n",
    "    \"VacationHours\": \"VacationHours\",\n",
    "    \"SickLeaveHours\": \"SickLeaveHours\",\n",
    "    \"LoginID\": \"LoginID\", \n",
    "    \"CurrentFlag\": \"CurrentFlag_Source\", \n",
    "    \"ModifiedDate\": \"ModifiedDate\"\n",
    "}\n",
    "\n",
    "# 1d. Setup Storage Access (Authentication) - Replace with your key!\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", account_key)\n",
    "PK_COLS = PRIMARY_KEYS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d059773a-4ddc-46b5-a2b2-4dba293d6aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_source = (spark.read\n",
    "              .option(\"mergeSchema\", \"true\")\n",
    "              .option(\"recursiveFileLookup\", \"true\")\n",
    "              .parquet(SOURCE_PATH))\n",
    "\n",
    "\n",
    "print(f\"Read OK. Rows: {df_source.count()} Columns: {df_source.columns}\")\n",
    "\n",
    "\n",
    "\n",
    "for source_col, target_col in COLUMN_MAP.items():\n",
    "    if source_col in df_source.columns:\n",
    "        df_source = df_source.withColumnRenamed(source_col, target_col)\n",
    "        \n",
    "df_source = df_source.withColumn(\"EmployeeNationalIDAlternateKey\", trim(coalesce(col(\"EmployeeNationalIDAlternateKey\"), lit(\"N/A\"))))\n",
    "df_source = df_source.withColumn(\"Title\", trim(coalesce(col(\"Title\"), lit(\"Unknown Title\"))))\n",
    "df_source = df_source.withColumn(\"HireDate\", to_timestamp(col(\"HireDate\")))\n",
    "df_source = df_source.withColumn(\"ModifiedDate\", to_timestamp(col(\"ModifiedDate\")))\n",
    "df_source = df_source.withColumn(\"BirthDate\", to_timestamp(col(\"BirthDate\")))\n",
    "df_source = df_source.withColumn(\"VacationHours\", col(\"VacationHours\").cast(\"integer\"))\n",
    "df_source = df_source.withColumn(\"SickLeaveHours\", col(\"SickLeaveHours\").cast(\"integer\"))\n",
    "df_source = df_source.dropDuplicates(PK_COLS)\n",
    "\n",
    "\n",
    "df_source = df_source.withColumn(\"LoadTS\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"__ingest_ts\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"__source_path\", lit(SOURCE_PATH)) \n",
    "df_source = df_source.withColumn(\"__target_path\", lit(TARGET_PATH)) \n",
    "df_source = df_source.withColumn(\"__batch_id\", lit(\"Batch-\" + datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')))\n",
    "df_source = df_source.withColumn(\"_year\", year(col(\"ModifiedDate\")))\n",
    "hash_cols = [\"EmployeeNationalIDAlternateKey\",\"EmployeeAlternateKey\",\"Title\", \"MaritalStatus\", \"Gender\", \"VacationHours\", \"SickLeaveHours\"]\n",
    "df_source = df_source.withColumn(\n",
    "    \"__row_hash\", sha2(concat_ws(\"||\", *[coalesce(col(c).cast(\"string\"), lit(\"\")) for c in hash_cols]), 256)\n",
    ")\n",
    "#df_source = df_source.withColumn(\"EmployeeKey\", lit(None).cast(\"long\"))\n",
    "df_source = df_source.withColumn(\"IsCurrent\", lit(True))\n",
    "df_source = df_source.withColumn(\"StartDate\", col(\"HireDate\"))\n",
    "df_source = df_source.withColumn(\"EndDate\", lit(None).cast(\"timestamp\"))\n",
    "df_source = df_source.withColumn(\"Status\", when(col(\"CurrentFlag_Source\") == lit(False), lit(\"INACTIVE\")).otherwise(lit(\"ACTIVE\")))\n",
    "#df_source = df_source.drop(\"CurrentFlag_Source\")\n",
    "\n",
    "\n",
    "# --- 4. LOAD INTO DELTA LAKE USING MERGE ---\n",
    "\n",
    "# 4a. EXPLICIT ALL_TARGET_COLS List\n",
    "ALL_TARGET_COLS = [\n",
    "    \"EmployeeAlternateKey\", \"EmployeeNationalIDAlternateKey\",\n",
    "    \"Title\", \"HireDate\", \"BirthDate\", \"MaritalStatus\", \"Gender\", \"SalariedFlag\", \n",
    "    \"VacationHours\", \"SickLeaveHours\", \"LoginID\", \"ModifiedDate\", \n",
    "    \"StartDate\", \"EndDate\", \"IsCurrent\", \"Status\",\"CurrentFlag_Source\",\n",
    "    \"LoadTS\", \"__row_hash\", \"_year\", \"__ingest_ts\", \"__source_path\", \n",
    "    \"__target_path\", \"__batch_id\"\n",
    "]\n",
    "\n",
    "target_exists = DeltaTable.isDeltaTable(spark, TARGET_PATH)\n",
    "\n",
    "if not target_exists:\n",
    "    print(f\"\\n4. Target table not found. Creating initial table at: {TARGET_PATH}\")\n",
    "    \n",
    "    # 4b. Initial Write: Select only the required columns and save\n",
    "    df_source.select(*[c for c in ALL_TARGET_COLS if c in df_source.columns]).write \\\n",
    "        .format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(\"_year\").save(TARGET_PATH)\n",
    "    print(\"Initial table created. âœ…\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n4. Target table exists. Performing Incremental MERGE...\")\n",
    "    dt_target = DeltaTable.forPath(spark, TARGET_PATH)\n",
    "    \n",
    "    # Join condition uses the correct column name\n",
    "    join_cond = \" AND \".join([f\"target.{c} = source.{c}\" for c in PK_COLS])\n",
    "    change_cond = \"target.__row_hash != source.__row_hash\"\n",
    "    \n",
    "    dt_target.alias(\"target\").merge(df_source.alias(\"source\"), join_cond) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=change_cond,\n",
    "        set = {\n",
    "            # Update mutable data and audit columns \"EmployeeAlternateKey\", \"EmployeeNationalIDAlternateKey\",\n",
    "            \"EmployeeAlternateKey\": \"source.EmployeeAlternateKey\", \"EmployeeNationalIDAlternateKey\": \"source.EmployeeNationalIDAlternateKey\", \n",
    "            \"Title\": \"source.Title\", \"MaritalStatus\": \"source.MaritalStatus\", \n",
    "            \"Gender\": \"source.Gender\", \"VacationHours\": \"source.VacationHours\", \n",
    "            \"SickLeaveHours\": \"source.SickLeaveHours\", \"Status\": \"source.Status\",\n",
    "            \"LoadTS\": \"source.LoadTS\", \"__row_hash\": \"source.__row_hash\",\n",
    "            \"CurrentFlag_Source\":\"source.CurrentFlag_Source\"\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        # --- EXPLICIT INSERT VALUES FOR EMPLOYEE ---\n",
    "        values = {\n",
    "            # Keys and Identifiers\n",
    "           # \"EmployeeKey\": \"source.EmployeeKey\",\n",
    "            \"EmployeeAlternateKey\": \"source.EmployeeAlternateKey\",\n",
    "            \"EmployeeNationalIDAlternateKey\": \"source.EmployeeNationalIDAlternateKey\",\n",
    "            \n",
    "            # Data Columns\n",
    "            \"Title\": \"source.Title\",\n",
    "            \"HireDate\": \"source.HireDate\",\n",
    "            \"BirthDate\": \"source.BirthDate\",\n",
    "            \"MaritalStatus\": \"source.MaritalStatus\",\n",
    "            \"Gender\": \"source.Gender\",\n",
    "            \"SalariedFlag\": \"source.SalariedFlag\",\n",
    "            \"VacationHours\": \"source.VacationHours\",\n",
    "            \"SickLeaveHours\": \"source.SickLeaveHours\",\n",
    "            \"LoginID\": \"source.LoginID\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            \n",
    "            # SCD/Status Columns\n",
    "            \"StartDate\": \"source.StartDate\",\n",
    "            \"EndDate\": \"source.EndDate\",\n",
    "            \"IsCurrent\": \"source.IsCurrent\",\n",
    "            \"Status\": \"source.Status\",\n",
    "            \n",
    "            # Audit Columns\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\",\n",
    "            \"_year\": \"source._year\",\n",
    "            \"__ingest_ts\": \"source.__ingest_ts\",\n",
    "            \"__source_path\": \"source.__source_path\",\n",
    "            \"__target_path\": \"source.__target_path\",\n",
    "            \"__batch_id\": \"source.__batch_id\"\n",
    "        }\n",
    "    ) \\\n",
    "    .execute()\n",
    "    \n",
    "    print(\"MERGE (Incremental Load) complete.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_BronzeToSliver_Merge_emp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
