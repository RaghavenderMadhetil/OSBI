{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309cfd32-b221-4d26-8e61-2ce29d48fce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_Utils - all helper functions for CSV â†’ Parquet conversion\n",
    "\n",
    "from pyspark.sql.functions import trim, col, year, to_timestamp, coalesce, lit\n",
    "\n",
    "def read_metadata(jdbc_url, user, password):\n",
    "    df = (spark.read\n",
    "                 .format(\"jdbc\")\n",
    "                 .option(\"url\", jdbc_url)\n",
    "                 .option(\"dbtable\", \"dbo.sql_table_metadata\")\n",
    "                 .option(\"user\", user)\n",
    "                 .option(\"password\", password)\n",
    "                 .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "                 .load())\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_column_list(metadata_df, file_name):\n",
    "    row = metadata_df.filter(metadata_df[\"file_name\"] == file_name).first()\n",
    "    if not row:\n",
    "        raise Exception(f\"No metadata found for file {file_name}\")\n",
    "    return [c.strip() for c in row[\"column_list\"].split(\",\")]\n",
    "\n",
    "\n",
    "def get_year_column(metadata_df, file_name):\n",
    "    row = metadata_df.filter(metadata_df[\"file_name\"] == file_name).first()\n",
    "    if \"year_column\" in row.asDict():\n",
    "        return row[\"year_column\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "def add_headers(df, columns):\n",
    "    # rename based on metadata\n",
    "    new_names = []\n",
    "    for i, c in enumerate(df.columns):\n",
    "        if i < len(columns):\n",
    "            new_names.append(columns[i])\n",
    "        else:\n",
    "            new_names.append(f\"_c{i}\")\n",
    "    return df.toDF(*new_names)\n",
    "\n",
    "\n",
    "def extract_year(df, year_col):\n",
    "    if year_col and year_col in df.columns:\n",
    "        try:\n",
    "            df = df.withColumn(\"_year\", year(to_timestamp(col(year_col))))\n",
    "        except:\n",
    "            df = df.withColumn(\"_year\", col(year_col).cast(\"int\"))\n",
    "    else:\n",
    "        df = df.withColumn(\"_year\", lit(9999)) # fallback\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet(df, output_path):\n",
    "    (df.coalesce(1)\n",
    "       .write\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"compression\", \"snappy\")\n",
    "       .parquet(output_path))\n",
    "    print(\"PARQUET WRITTEN:\", output_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
