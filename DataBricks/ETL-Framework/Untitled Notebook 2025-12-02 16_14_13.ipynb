{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2bfc9f-a7dd-4855-a2ce-63543cb7c678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generic Silver Loader: NO HARDCODED COLUMNS\n",
    "import traceback, datetime\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, to_timestamp, year, lit, concat_ws, sha2, current_timestamp\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ---------------------------\n",
    "# WIDGETS (ADF can override these)\n",
    "# ---------------------------\n",
    "dbutils.widgets.text(\"Source_path\", \"\")\n",
    "dbutils.widgets.text(\"Target_path\", \"\")\n",
    "dbutils.widgets.text(\"pk_columns\", \"\")         # JSON list: [\"id\"] or [\"code\"]\n",
    "dbutils.widgets.text(\"year_column\", \"\")        # optional, blank allowed\n",
    "dbutils.widgets.text(\"merge_flag\", \"true\")\n",
    "dbutils.widgets.text(\"batch_name\", \"Batch-1\")\n",
    "\n",
    "Source_path = dbutils.widgets.get(\"Source_path\").strip()\n",
    "Target_path = dbutils.widgets.get(\"Target_path\").strip()\n",
    "year_column = dbutils.widgets.get(\"year_column\").strip()\n",
    "batch_name = dbutils.widgets.get(\"batch_name\").strip()\n",
    "\n",
    "# Parse PK columns\n",
    "try:\n",
    "    pk_columns = json.loads(dbutils.widgets.get(\"pk_columns\"))\n",
    "except:\n",
    "    pk_columns = []\n",
    "\n",
    "merge_flag = dbutils.widgets.get(\"merge_flag\").lower() in (\"true\",\"1\",\"yes\")\n",
    "\n",
    "print(\"Source:\", Source_path)\n",
    "print(\"Target:\", Target_path)\n",
    "print(\"PK:\", pk_columns)\n",
    "\n",
    "# ---------------------------\n",
    "# READ SOURCE parquet (generic)\n",
    "# ---------------------------\n",
    "try:\n",
    "    df = (spark.read\n",
    "          .option(\"mergeSchema\",\"true\")\n",
    "          .option(\"recursiveFileLookup\",\"true\")\n",
    "          .parquet(Source_path))\n",
    "    print(\"Source columns:\", df.columns)\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    raise Exception(\"Failed to read parquet.\")\n",
    "\n",
    "display(df.limit(5))\n",
    "\n",
    "# ---------------------------\n",
    "# GENERIC CLEANING (NO HARDCODING)\n",
    "# ---------------------------\n",
    "\n",
    "# 1) Trim all string columns (universal)\n",
    "string_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "# 2) Convert year column only if user passed it\n",
    "if year_column and year_column in df.columns:\n",
    "    try:\n",
    "        df = df.withColumn(year_column, to_timestamp(col(year_column)))\n",
    "        df = df.withColumn(\"_year\", year(col(year_column)))\n",
    "    except:\n",
    "        df = df.withColumn(\"_year\", lit(datetime.datetime.utcnow().year))\n",
    "else:\n",
    "    df = df.withColumn(\"_year\", lit(datetime.datetime.utcnow().year))\n",
    "\n",
    "# 3) Add audit columns if missing\n",
    "if \"__ingest_ts\" not in df.columns:\n",
    "    df = df.withColumn(\"__ingest_ts\", current_timestamp())\n",
    "\n",
    "if \"__source_path\" not in df.columns:\n",
    "    df = df.withColumn(\"__source_path\", lit(Source_path))\n",
    "\n",
    "if \"__batch_id\" not in df.columns:\n",
    "    df = df.withColumn(\"__batch_id\", lit(batch_name))\n",
    "\n",
    "# 4) Compute row hash (generic, no hardcoding)\n",
    "#    Use PK if provided, else use ALL columns\n",
    "if pk_columns:\n",
    "    hash_cols = [c for c in pk_columns if c in df.columns]\n",
    "    if not hash_cols:\n",
    "        hash_cols = df.columns\n",
    "else:\n",
    "    hash_cols = df.columns\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"__row_hash\",\n",
    "    sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in hash_cols]), 256)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# REMOVE DUPLICATES (generic)\n",
    "# ---------------------------\n",
    "if pk_columns:\n",
    "    valid_pk = [p for p in pk_columns if p in df.columns]\n",
    "    if valid_pk:\n",
    "        df = df.dropDuplicates(valid_pk)\n",
    "    else:\n",
    "        df = df.dropDuplicates([\"__row_hash\"])\n",
    "else:\n",
    "    df = df.dropDuplicates([\"__row_hash\"])\n",
    "\n",
    "display(df.limit(5))\n",
    "\n",
    "# ---------------------------\n",
    "# WRITE / MERGE INTO DELTA (generic)\n",
    "# ---------------------------\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_exists = True\n",
    "try:\n",
    "    spark.read.format(\"delta\").load(Target_path)\n",
    "except:\n",
    "    delta_exists = False\n",
    "\n",
    "if not merge_flag:\n",
    "    print(\"MERGE disabled → simple overwrite\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"_year\").save(Target_path)\n",
    "\n",
    "else:\n",
    "    if not delta_exists:\n",
    "        print(\"Delta not found → creating initial table\")\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"_year\").save(Target_path)\n",
    "    else:\n",
    "        print(\"Delta found → MERGE\")\n",
    "        dt = DeltaTable.forPath(spark, Target_path)\n",
    "\n",
    "        # MERGE key: pk_columns → fallback __row_hash\n",
    "        if pk_columns:\n",
    "            merge_keys = [p for p in pk_columns if p in df.columns]\n",
    "        else:\n",
    "            merge_keys = [\"__row_hash\"]\n",
    "\n",
    "        join_cond = \" AND \".join([f\"target.{c} = source.{c}\" for c in merge_keys])\n",
    "\n",
    "        dt.alias(\"target\").merge(\n",
    "            df.alias(\"source\"),\n",
    "            join_cond\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# ---------------------------\n",
    "# SHOW TARGET\n",
    "# ---------------------------\n",
    "try:\n",
    "    tgt = spark.read.format(\"delta\").load(Target_path)\n",
    "    print(\"Target count:\", tgt.count())\n",
    "    display(tgt.limit(5))\n",
    "except:\n",
    "    print(\"Target read failed.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-12-02 16_14_13",
   "widgets": {
    "Source_path": {
     "currentValue": "a04_BronzeToSilver_1",
     "nuid": "dd7d0f12-f9d5-4804-9997-7fd8b654d32d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Source_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Source_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_path": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/",
     "nuid": "1a255208-0640-4f14-89c2-6f8af69fafd6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "batch_name": {
     "currentValue": "Batch-1",
     "nuid": "152d89e4-8f71-4f6b-bec3-828be117639c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Batch-1",
      "label": null,
      "name": "batch_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Batch-1",
      "label": null,
      "name": "batch_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "merge_flag": {
     "currentValue": "true",
     "nuid": "fede1091-e587-464a-9990-b35e6cb9d2d5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "true",
      "label": null,
      "name": "merge_flag",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "true",
      "label": null,
      "name": "merge_flag",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "pk_columns": {
     "currentValue": "CurrencyCode",
     "nuid": "fbc46beb-f316-4dbd-8cdb-1a99766659c7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "pk_columns",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "pk_columns",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "year_column": {
     "currentValue": "ModifiedDate",
     "nuid": "b975254b-103d-4bb8-900b-697d05f843e7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "year_column",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "year_column",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
