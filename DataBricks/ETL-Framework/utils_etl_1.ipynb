{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1d183b-f43c-44b9-9362-653438bd0bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# utils_etl.py  (FINAL FIXED VERSION)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import trim, col, year, to_timestamp, coalesce, lit\n",
    "import datetime\n",
    "\n",
    "def read_sql_metadata(jdbc_url: str, jdbc_user: str, jdbc_pass: str, table_name: str = \"dbo.sql_table_metadata\") -> DataFrame:\n",
    "    df = (spark.read.format(\"jdbc\")\n",
    "            .option(\"url\", jdbc_url)\n",
    "            .option(\"dbtable\", table_name)\n",
    "            .option(\"user\", jdbc_user)\n",
    "            .option(\"password\", jdbc_pass)\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "            .load())\n",
    "    return df\n",
    "\n",
    "def metadata_to_map(meta_df: DataFrame) -> dict:\n",
    "    rows = meta_df.collect()\n",
    "    m = {}\n",
    "    for r in rows:\n",
    "        d = r.asDict()\n",
    "        fname = d.get(\"file_name\")\n",
    "        cols_str = d.get(\"column_list\") or \"\"\n",
    "        cols_list = [c.strip() for c in cols_str.split(\",\") if c.strip() != \"\"]\n",
    "        year_col = d.get(\"year_column\") if \"year_column\" in d else None\n",
    "        table_name = d.get(\"table_name\") if \"table_name\" in d and d.get(\"table_name\") else (fname.split(\".\")[0] if fname else None)\n",
    "        m[fname] = {\"columns\": cols_list, \"year_column\": year_col, \"table_name\": table_name}\n",
    "    return m\n",
    "\n",
    "def add_headers(df: DataFrame, columns: list) -> DataFrame:\n",
    "    new_names = []\n",
    "    for i, old in enumerate(df.columns):\n",
    "        if i < len(columns):\n",
    "            new_names.append(columns[i])\n",
    "        else:\n",
    "            new_names.append(f\"_c{i}\")\n",
    "    return df.toDF(*new_names)\n",
    "\n",
    "def extract_year_column(df: DataFrame, year_column_hint: str = None):\n",
    "    current_year = datetime.datetime.utcnow().year\n",
    "    cols = df.columns\n",
    "    used = None\n",
    "    df2 = df\n",
    "\n",
    "    if year_column_hint and year_column_hint in cols:\n",
    "        used = year_column_hint\n",
    "        try:\n",
    "            df2 = df2.withColumn(\"_year\", year(to_timestamp(col(used))))\n",
    "        except Exception:\n",
    "            df2 = df2.withColumn(\"_year\", col(used).cast(\"int\"))\n",
    "    else:\n",
    "        candidates = [c for c in cols if \"year\" in c.lower()]\n",
    "        if candidates:\n",
    "            used = candidates[0]\n",
    "            df2 = df2.withColumn(\"_year\", col(used).cast(\"int\"))\n",
    "        else:\n",
    "            date_candidates = [c for c in cols if \"date\" in c.lower()]\n",
    "            if date_candidates:\n",
    "                used = date_candidates[0]\n",
    "                try:\n",
    "                    df2 = df2.withColumn(\"_year\", year(to_timestamp(col(used))))\n",
    "                except Exception:\n",
    "                    df2 = df2.withColumn(\"_year\", col(used).cast(\"int\"))\n",
    "            else:\n",
    "                df2 = df2.withColumn(\"_year\", lit(current_year))\n",
    "\n",
    "    df2 = df2.withColumn(\"_year\", coalesce(col(\"_year\").cast(\"int\"), lit(current_year)))\n",
    "    return df2, used\n",
    "\n",
    "def write_parquet_by_year(df_with_year: DataFrame, bronze_base_path: str, table_name: str,\n",
    "                          compression: str = \"snappy\", coalesce_out: bool = True, write_mode: str = \"overwrite\"):\n",
    "    years = [r[\"_year\"] for r in df_with_year.select(\"_year\").distinct().collect()]\n",
    "    for y in years:\n",
    "        out_path = f\"{bronze_base_path.rstrip('/')}/{table_name}/{y}\"\n",
    "        df_year = df_with_year.filter(col(\"_year\") == y).drop(\"_year\")\n",
    "        writer = df_year.coalesce(1) if coalesce_out else df_year\n",
    "        writer.write.mode(write_mode).option(\"compression\", compression).parquet(out_path)\n",
    "        print(f\"Wrote parquet to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eebc31d2-27e6-4401-9779-56fea9560e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 2 â€” add DBFS FileStore to sys.path and import utils_etl\n",
    "import sys, importlib\n",
    "\n",
    "dbfs_pkg_path = \"/dbfs/FileStore\"   # must match where we wrote utils_etl.py\n",
    "fs_import_path = \"/FileStore\"       # to show files via dbutils.ls\n",
    "\n",
    "if dbfs_pkg_path not in sys.path:\n",
    "    sys.path.insert(0, dbfs_pkg_path)\n",
    "    print(\"Inserted to sys.path:\", dbfs_pkg_path)\n",
    "\n",
    "# quick check listing\n",
    "print(\"Files in /FileStore:\")\n",
    "for f in dbutils.fs.ls(fs_import_path):\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "# import and reload\n",
    "try:\n",
    "    import utils_etl\n",
    "    importlib.reload(utils_etl)\n",
    "    print(\"\\nImported utils_etl OK. Functions available:\")\n",
    "    print([n for n in dir(utils_etl) if not n.startswith(\"_\")])\n",
    "except Exception as e:\n",
    "    print(\"Import failed:\", e)\n",
    "    print(\"sys.path (first 10):\", sys.path[:10])\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb628ab9-ea10-47f8-b920-8c40a7f25ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.put(\"/FileStore/utils_etl.py\", module_code, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a18fa19-dffd-4162-97f5-7f7be8c7b656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import importlib, utils_etl\n",
    "importlib.reload(utils_etl)\n",
    "print(\"Imported OK!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils_etl_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
