{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82153951-3888-4096-b9c8-2c14bb578820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SIMPLE SILVER LAYER - FIXED (no merge, beginner friendly)\n",
    "# =====================================================\n",
    "\n",
    "import json, datetime, traceback\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, current_timestamp, lit,\n",
    "    concat_ws, sha2, to_timestamp, year\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Widgets (ADF will pass these)\n",
    "# ---------------------------\n",
    "dbutils.widgets.text(\"Source_path\", \"\")\n",
    "dbutils.widgets.text(\"Target_path\", \"\")\n",
    "dbutils.widgets.text(\"pk_columns\", \"\")      # example: [\"CurrencyAlternateKey\"] OR CurrencyAlternateKey\n",
    "dbutils.widgets.text(\"table_name\", \"\")\n",
    "dbutils.widgets.text(\"direct_account_key\", \"\")   # optional: pass storage key if needed\n",
    "\n",
    "# ---------------------------\n",
    "# Read widgets\n",
    "# ---------------------------\n",
    "Source_path = dbutils.widgets.get(\"Source_path\").strip()\n",
    "Target_path = dbutils.widgets.get(\"Target_path\").strip()\n",
    "pk_raw = dbutils.widgets.get(\"pk_columns\").strip()\n",
    "table_name = dbutils.widgets.get(\"table_name\").strip()\n",
    "direct_key = dbutils.widgets.get(\"direct_account_key\").strip()\n",
    "\n",
    "print(\"Source_path :\", Source_path)\n",
    "print(\"Target_path :\", Target_path)\n",
    "print(\"PK Columns  :\", pk_raw)\n",
    "print(\"Table Name  :\", table_name)\n",
    "\n",
    "# ---------------------------\n",
    "# Quick validation\n",
    "# ---------------------------\n",
    "if not Source_path:\n",
    "    raise RuntimeError(\"Source_path widget is required (e.g. abfss://.../bronze/.../).\")\n",
    "if not Target_path:\n",
    "    raise RuntimeError(\"Target_path widget is required (e.g. abfss://.../silver/.../).\")\n",
    "\n",
    "# ---------------------------\n",
    "# OPTIONAL: set storage key if provided (for ABFSS)\n",
    "# ---------------------------\n",
    "if direct_key:\n",
    "    k = direct_key.strip().strip('\"').strip(\"'\")\n",
    "    # try to infer storage account from Source_path or Target_path\n",
    "    acct = None\n",
    "    for p in (Source_path, Target_path):\n",
    "        if p and \"@\" in p:\n",
    "            try:\n",
    "                acct = p.split(\"@\",1)[1].split(\".\")[0]; break\n",
    "            except:\n",
    "                pass\n",
    "    if not acct:\n",
    "        acct = \"scrgvkrmade\"   # fallback (change if you use different account)\n",
    "    print(\"Configuring storage key for account:\", acct)\n",
    "    spark.conf.set(f\"fs.azure.account.key.{acct}.dfs.core.windows.net\", k)\n",
    "else:\n",
    "    print(\"No direct_account_key passed. Ensure cluster has storage permissions (MSI / mount / secret).\")\n",
    "\n",
    "# ---------------------------\n",
    "# parse pk_columns inline (JSON array or CSV)\n",
    "# ---------------------------\n",
    "pk_cols = []\n",
    "if pk_raw:\n",
    "    try:\n",
    "        parsed = json.loads(pk_raw)\n",
    "        if isinstance(parsed, list):\n",
    "            pk_cols = [str(x).strip() for x in parsed if str(x).strip()]\n",
    "        else:\n",
    "            # not a list, fallback to CSV split\n",
    "            pk_cols = [c.strip() for c in str(pk_raw).split(\",\") if c.strip()]\n",
    "    except Exception:\n",
    "        pk_cols = [c.strip() for c in str(pk_raw).split(\",\") if c.strip()]\n",
    "\n",
    "print(\"Parsed PK columns:\", pk_cols)\n",
    "\n",
    "# ---------------------------\n",
    "# READ PARQUET FROM BRONZE (recursive)\n",
    "# ---------------------------\n",
    "print(\"Reading parquet from Source_path (recursive):\", Source_path)\n",
    "try:\n",
    "    df = (spark.read\n",
    "              .option(\"mergeSchema\", \"true\")\n",
    "              .option(\"recursiveFileLookup\", \"true\")\n",
    "              .parquet(Source_path))\n",
    "    print(\"Read OK. Rows:\", df.count(), \"Columns:\", df.columns)\n",
    "    display(df.limit(5))\n",
    "except Exception:\n",
    "    print(\"Failed to read parquet from Source_path. Full error:\")\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"Parquet read failed. Check Source_path and storage permissions.\")\n",
    "\n",
    "# ---------------------------\n",
    "# BASIC CLEANING\n",
    "# - trim strings\n",
    "# - add audit fields if missing\n",
    "# - create __row_hash\n",
    "# - add _year (from ModifiedDate if present else fallback)\n",
    "# ---------------------------\n",
    "# trim all string columns\n",
    "string_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "# audit columns\n",
    "if \"__ingest_ts\" not in df.columns:\n",
    "    df = df.withColumn(\"__ingest_ts\", current_timestamp())\n",
    "\n",
    "if \"__source_path\" not in df.columns:\n",
    "    df = df.withColumn(\"__source_path\", lit(Source_path))\n",
    "\n",
    "if \"__batch_id\" not in df.columns:\n",
    "    df = df.withColumn(\"__batch_id\", lit(\"Batch-\" + datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')))\n",
    "\n",
    "# compute __row_hash using PK columns if present else all columns\n",
    "from pyspark.sql.functions import sha2, concat_ws\n",
    "\n",
    "cols_for_hash = []\n",
    "if pk_cols:\n",
    "    cols_for_hash = [c for c in pk_cols if c in df.columns]\n",
    "if not cols_for_hash:\n",
    "    cols_for_hash = df.columns\n",
    "\n",
    "# protect against empty columns list\n",
    "if not cols_for_hash:\n",
    "    raise RuntimeError(\"No columns available to compute __row_hash (empty schema?).\")\n",
    "\n",
    "df = df.withColumn(\"__row_hash\", sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in cols_for_hash]), 256))\n",
    "\n",
    "print(\"After basic cleaning. Columns now:\", df.columns)\n",
    "display(df.limit(5))\n",
    "\n",
    "# ---------------------------\n",
    "# DEDUPE\n",
    "# - use provided PK columns if they exist in DF\n",
    "# - else dedupe by __row_hash\n",
    "# ---------------------------\n",
    "valid_pk = [c for c in pk_cols if c in df.columns]\n",
    "if valid_pk:\n",
    "    print(\"Dropping duplicates using PK columns:\", valid_pk)\n",
    "    before = df.count()\n",
    "    df = df.dropDuplicates(valid_pk)\n",
    "    after = df.count()\n",
    "    print(f\"Removed {before - after} duplicates.\")\n",
    "else:\n",
    "    print(\"No valid PK found â†’ deduping by __row_hash\")\n",
    "    before = df.count()\n",
    "    df = df.dropDuplicates([\"__row_hash\"])\n",
    "    after = df.count()\n",
    "    print(f\"Removed {before - after} duplicates by __row_hash.\")\n",
    "\n",
    "display(df.limit(5))\n",
    "\n",
    "# ---------------------------\n",
    "# ADD _year\n",
    "# ---------------------------\n",
    "if \"ModifiedDate\" in df.columns:\n",
    "    try:\n",
    "        df = df.withColumn(\"ModifiedDate\", to_timestamp(col(\"ModifiedDate\")))\n",
    "        df = df.withColumn(\"_year\", year(col(\"ModifiedDate\")))\n",
    "    except Exception:\n",
    "        df = df.withColumn(\"_year\", lit(datetime.datetime.utcnow().year))\n",
    "else:\n",
    "    df = df.withColumn(\"_year\", lit(datetime.datetime.utcnow().year))\n",
    "\n",
    "# ---------------------------\n",
    "# WRITE TO SILVER (overwrite)\n",
    "# ---------------------------\n",
    "print(\"Writing Delta to Target_path (overwrite):\", Target_path)\n",
    "try:\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"overwriteSchema\", \"true\")\n",
    "       .partitionBy(\"_year\")\n",
    "       .save(Target_path))\n",
    "    print(\"Write completed.\")\n",
    "except Exception:\n",
    "    print(\"Failed to write Delta to Target_path. Error:\")\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"Delta write failed. Check Target_path and storage permissions.\")\n",
    "\n",
    "# ---------------------------\n",
    "# VALIDATE: read back small sample from target\n",
    "# ---------------------------\n",
    "print(\"Validating output by reading back Target_path:\", Target_path)\n",
    "try:\n",
    "    tgt = spark.read.format(\"delta\").load(Target_path)\n",
    "    print(\"Rows in Silver:\", tgt.count())\n",
    "    display(tgt.limit(10))\n",
    "except Exception:\n",
    "    print(\"Could not read back Delta target. Listing files in target path for debugging:\")\n",
    "    try:\n",
    "        for f in dbutils.fs.ls(Target_path):\n",
    "            print(\"-\", f.path)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        print(\"Also failed to list target path. Check path correctness and permissions.\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_BronzeToSliver_Final",
   "widgets": {
    "Source_path": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/HumanResources.Employee/",
     "nuid": "0be3b543-88b4-4a99-9b2d-aa749671b3be",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Source_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Source_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_path": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_employee/",
     "nuid": "1ccc512b-0873-4a78-ac7a-4d99bce26cc1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "direct_account_key": {
     "currentValue": "E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==",
     "nuid": "fda9585b-b0d5-4045-8093-db9d6a5353de",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "direct_account_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "direct_account_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "pk_columns": {
     "currentValue": "Name",
     "nuid": "f24042bc-6fa7-4a13-b6aa-7de393bcfb95",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "pk_columns",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "pk_columns",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "stg_employee",
     "nuid": "363e460d-21a7-4e87-bda5-2625410df4b0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
