{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cce8728-e514-48da-b284-77b42eda950c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8\tResellerSales\tCurrency.parquet\tstg.currency\tabfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Currency/\tBronze\tRS_Bronze_01\tResellerSales_Bronze\tstg.currency\tabfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/\tDatabricks/Transformation\tParallel\tBlocked\tCurrency.parquet (Raw Bronze)\tSilver\tstg.currency\tModifiedDate\t[\"CurrencyCode\",\"CurrencyAlternateKey\",\"CurrencyName\",\"ModifiedDate\",\"__ingest_ts\",\"__source_file\",\"__source_path\",\"__batch_id\",\"__row_hash\"]\ttrue\tfalse\t[\"CurrencyAlternateKey\"]\t2025-12-02 07:40:46.9809009\n",
    "# Source_path : abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Currency/\n",
    "# Target_path : abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/\n",
    "# column_list : [\"CurrencyCode\",\"CurrencyAlternateKey\",\"CurrencyName\",\"ModifiedDate\",\"__ingest_ts\",\"__source_file\",\"__source_path\",\"__batch_id\",\"__row_hash\"]\n",
    "# direct_account_key : E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\n",
    "# domain: ResellerSales\n",
    "# file_name: stg.currency\n",
    "# include_layer :false\n",
    "# layer:Bronze\n",
    "# table_name:stg.currency\n",
    "# year_column:ModifiedDate\n",
    "# incremental_flag : false\n",
    "# merge_flag : true\n",
    "# pk_columns : CurrencyAlternateKey\n",
    "# 04_BronzeToSilver\n",
    "# Purpose: read bronze parquet (recursive), clean/transform, and MERGE into silver Delta.\n",
    "# Save this as a Databricks Python notebook (e.g. /Repos/.../04_BronzeToSilver)\n",
    "\n",
    "import sys, importlib, json, traceback, uuid\n",
    "from pyspark.sql.functions import col, trim, to_timestamp, year, lit, concat_ws, sha2, current_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Optional utils module if present\n",
    "if \"/dbfs/FileStore\" not in sys.path:\n",
    "    sys.path.insert(0, \"/dbfs/FileStore\")\n",
    "try:\n",
    "    import utils_etl\n",
    "    importlib.reload(utils_etl)\n",
    "    print(\"utils_etl imported (optional).\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------\n",
    "# WIDGETS / DEFAULT PARAMETERS\n",
    "# (ADF will override these at runtime — defaults set to the values you provided)\n",
    "# ---------------------------\n",
    "dbutils.widgets.text(\"Source_path\", \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Currency/\")\n",
    "dbutils.widgets.text(\"Target_path\", \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/\")\n",
    "dbutils.widgets.text(\"column_list\", '[\"CurrencyCode\",\"CurrencyAlternateKey\",\"CurrencyName\",\"ModifiedDate\",\"__ingest_ts\",\"__source_file\",\"__source_path\",\"__batch_id\",\"__row_hash\"]')\n",
    "dbutils.widgets.text(\"direct_account_key\", \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\")\n",
    "dbutils.widgets.text(\"domain\", \"ResellerSales\")\n",
    "dbutils.widgets.text(\"file_name\", \"stg.currency\")\n",
    "dbutils.widgets.text(\"include_layer\", \"false\")\n",
    "dbutils.widgets.text(\"layer\", \"Bronze\")\n",
    "dbutils.widgets.text(\"table_name\", \"stg.currency\")\n",
    "dbutils.widgets.text(\"year_column\", \"ModifiedDate\")\n",
    "dbutils.widgets.text(\"incremental_flag\", \"false\")\n",
    "dbutils.widgets.text(\"merge_flag\", \"true\")\n",
    "dbutils.widgets.text(\"pk_columns\", '[\"CurrencyAlternateKey\"]')\n",
    "dbutils.widgets.text(\"watermark\", \"\")   # optional; not used since incremental_flag=false\n",
    "dbutils.widgets.text(\"batch_name\", \"Batch B\")  # optional audit\n",
    "\n",
    "# ---------------------------\n",
    "# Read widget values\n",
    "# ---------------------------\n",
    "params = {k: dbutils.widgets.get(k).strip() for k in [\n",
    "    \"Source_path\",\"Target_path\",\"column_list\",\"direct_account_key\",\"domain\",\"file_name\",\"include_layer\",\n",
    "    \"layer\",\"table_name\",\"year_column\",\"incremental_flag\",\"merge_flag\",\"pk_columns\",\"watermark\",\"batch_name\"\n",
    "]}\n",
    "\n",
    "print(\"Parameters (summary):\")\n",
    "print(\" source:\", params[\"Source_path\"])\n",
    "print(\" target:\", params[\"Target_path\"])\n",
    "print(\" table_name:\", params[\"table_name\"])\n",
    "print(\" merge_flag:\", params[\"merge_flag\"], \" incremental_flag:\", params[\"incremental_flag\"])\n",
    "print(\" pk_columns:\", params[\"pk_columns\"])\n",
    "print(\" column_list:\", params[\"column_list\"])\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def parse_list(txt):\n",
    "    if not txt:\n",
    "        return []\n",
    "    s = txt.strip()\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        try:\n",
    "            arr = json.loads(s)\n",
    "            return [str(x).strip() for x in arr]\n",
    "        except Exception:\n",
    "            # fallback\n",
    "            s2 = s.strip(\"[]\")\n",
    "            return [p.strip().strip('\"').strip(\"'\") for p in s2.split(\",\") if p.strip()]\n",
    "    return [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "\n",
    "column_list = parse_list(params[\"column_list\"])\n",
    "pk_cols = parse_list(params[\"pk_columns\"])\n",
    "merge_flag = params[\"merge_flag\"].lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "incremental_flag = params[\"incremental_flag\"].lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "modified_col = params[\"year_column\"] if params[\"year_column\"] else None\n",
    "\n",
    "# ---------------------------\n",
    "# Configure storage (set spark.conf for ABFSS)\n",
    "# ---------------------------\n",
    "STORAGE_ACCOUNT = \"scrgvkrmade\"\n",
    "raw_key = params[\"direct_account_key\"]\n",
    "if raw_key:\n",
    "    if (raw_key.startswith('\"') and raw_key.endswith('\"')) or (raw_key.startswith(\"'\") and raw_key.endswith(\"'\")):\n",
    "        raw_key = raw_key[1:-1]\n",
    "    raw_key = raw_key.strip()\n",
    "    # basic validation (heuristic)\n",
    "    if not raw_key or len(raw_key) < 20:\n",
    "        raise Exception(\"direct_account_key looks invalid.\")\n",
    "    spark.conf.set(f\"fs.azure.account.key.{STORAGE_ACCOUNT}.dfs.core.windows.net\", raw_key)\n",
    "    print(\"Spark configured for ABFSS on\", STORAGE_ACCOUNT)\n",
    "else:\n",
    "    print(\"No direct_account_key provided — ensure cluster has access to storage.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Resolve source & target paths (use as-is if full abfss provided)\n",
    "# ---------------------------\n",
    "source_path = params[\"Source_path\"]\n",
    "target_path = params[\"Target_path\"]\n",
    "if not source_path.lower().startswith(\"abfss://\"):\n",
    "    raise Exception(\"Source_path must be full abfss path to bronze table root (e.g. .../Sales.Currency/).\")\n",
    "if not target_path.lower().startswith(\"abfss://\"):\n",
    "    raise Exception(\"Target_path must be full abfss path to silver target (e.g. .../silver/stg_currency/).\")\n",
    "\n",
    "# ---------------------------\n",
    "# Read parquet recursively (all year folders)\n",
    "# ---------------------------\n",
    "print(\"Reading parquet recursively from bronze source:\", source_path)\n",
    "try:\n",
    "    src_df = (spark.read\n",
    "              .option(\"mergeSchema\",\"true\")\n",
    "              .option(\"recursiveFileLookup\",\"true\")\n",
    "              .parquet(source_path))\n",
    "    print(\"Read rows:\", src_df.count(), \"columns:\", src_df.columns)\n",
    "    display(src_df.limit(5))\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    raise Exception(\"Failed to read parquet from bronze source: \" + str(e))\n",
    "\n",
    "# ---------------------------\n",
    "# Keep only required columns (if column_list provided)\n",
    "# ---------------------------\n",
    "if column_list:\n",
    "    # intersection preserves order from column_list\n",
    "    select_cols = [c for c in column_list if c in src_df.columns]\n",
    "    missing = [c for c in column_list if c not in src_df.columns]\n",
    "    if missing:\n",
    "        print(\"Warning - these requested columns are missing in bronze source and will be omitted:\", missing)\n",
    "    if not select_cols:\n",
    "        raise Exception(\"None of the requested columns exist in source.\")\n",
    "    df = src_df.select(*select_cols)\n",
    "else:\n",
    "    df = src_df\n",
    "\n",
    "print(\"Selected columns:\", df.columns)\n",
    "\n",
    "# ---------------------------\n",
    "# Transformations & cleaning\n",
    "# - Trim string fields\n",
    "# - Normalize CurrencyCode to upper case\n",
    "# - Cast ModifiedDate to timestamp\n",
    "# - Fill / handle nulls where sensible\n",
    "# - Drop exact duplicate rows by PK (if PK exists)\n",
    "# ---------------------------\n",
    "from pyspark.sql.functions import upper, coalesce\n",
    "\n",
    "# Trim all string columns\n",
    "string_cols = [c for c,dtype in df.dtypes if dtype == 'string']\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "# Normalize CurrencyCode (if present)\n",
    "if \"CurrencyCode\" in df.columns:\n",
    "    df = df.withColumn(\"CurrencyCode\", upper(col(\"CurrencyCode\")))\n",
    "\n",
    "# Convert ModifiedDate to timestamp (if present)\n",
    "if modified_col and modified_col in df.columns:\n",
    "    df = df.withColumn(modified_col, to_timestamp(col(modified_col)))\n",
    "    print(\"Casted\", modified_col, \"to timestamp.\")\n",
    "\n",
    "# Add _year column derived from ModifiedDate if possible else try existing _year\n",
    "if modified_col and modified_col in df.columns:\n",
    "    df = df.withColumn(\"_year\", year(col(modified_col)))\n",
    "else:\n",
    "    # try to detect _year from path or existing column; fallback to literal year\n",
    "    if \"_year\" not in df.columns:\n",
    "        import datetime\n",
    "        df = df.withColumn(\"_year\", lit(datetime.datetime.utcnow().year))\n",
    "\n",
    "# Fill nulls for non-key columns where appropriate (simple approach)\n",
    "# For example, CurrencyName -> empty string if null\n",
    "if \"CurrencyName\" in df.columns:\n",
    "    df = df.withColumn(\"CurrencyName\", coalesce(col(\"CurrencyName\"), lit(\"\")))\n",
    "\n",
    "# Drop duplicates based on pk_cols if provided\n",
    "if pk_cols:\n",
    "    existing_pks = [p for p in pk_cols if p in df.columns]\n",
    "    if existing_pks:\n",
    "        before = df.count()\n",
    "        df = df.dropDuplicates(existing_pks)\n",
    "        after = df.count()\n",
    "        print(f\"Dropped duplicates by PK {existing_pks}: {before - after} rows removed.\")\n",
    "    else:\n",
    "        print(\"PK columns provided but none found in source columns:\", pk_cols)\n",
    "\n",
    "# ---------------------------\n",
    "# Ensure audit columns exist; do not overwrite if present\n",
    "# Add/maintain:\n",
    "#   __ingest_ts (if missing) -> current_timestamp\n",
    "#   __source_file (if missing) -> blank or derive from _source_file if present\n",
    "#   __source_path (if missing) -> source_path\n",
    "#   __batch_id (if missing) -> batch_name from widget\n",
    "# ---------------------------\n",
    "if \"__ingest_ts\" not in df.columns:\n",
    "    df = df.withColumn(\"__ingest_ts\", current_timestamp())\n",
    "if \"__source_file\" not in df.columns:\n",
    "    # try to derive from file_name widget or leave empty\n",
    "    _srcfile = params[\"file_name\"] if params[\"file_name\"] else \"\"\n",
    "    df = df.withColumn(\"__source_file\", lit(_srcfile))\n",
    "if \"__source_path\" not in df.columns:\n",
    "    df = df.withColumn(\"__source_path\", lit(source_path))\n",
    "if \"__batch_id\" not in df.columns:\n",
    "    df = df.withColumn(\"__batch_id\", lit(params[\"batch_name\"] if params[\"batch_name\"] else \"\"))\n",
    "\n",
    "# ---------------------------\n",
    "# Compute deterministic row hash __row_hash (if not present) using sha2 on concat of PK(s) + all columns\n",
    "# ---------------------------\n",
    "# Build concatenation expression: prefer PK columns then all columns\n",
    "cols_for_hash = []\n",
    "if pk_cols:\n",
    "    cols_for_hash = [c for c in pk_cols if c in df.columns]\n",
    "# fallback to all columns\n",
    "if not cols_for_hash:\n",
    "    cols_for_hash = df.columns\n",
    "\n",
    "# create concatenated string of selected columns\n",
    "concat_cols_expr = concat_ws(\"||\", *[col(c).cast(\"string\") for c in cols_for_hash])\n",
    "df = df.withColumn(\"__row_hash\", sha2(concat_cols_expr, 256))\n",
    "\n",
    "print(\"After transformations. Columns now:\", df.columns)\n",
    "display(df.limit(5))\n",
    "\n",
    "# ---------------------------\n",
    "# MERGE into Delta Silver target (partition by _year)\n",
    "# ---------------------------\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "\n",
    "# Ensure target_path exists or will be created\n",
    "print(\"Preparing to write/merge to target path:\", target_path)\n",
    "\n",
    "# If merge_flag true -> perform MERGE using pk_cols\n",
    "if merge_flag:\n",
    "    if not pk_cols:\n",
    "        raise Exception(\"merge_flag=true requires pk_columns to be provided.\")\n",
    "    # verify pk exist in df\n",
    "    pk_in_df = [p for p in pk_cols if p in df.columns]\n",
    "    if not pk_in_df:\n",
    "        raise Exception(\"None of the provided pk_columns exist in the dataframe. pk_columns: \" + str(pk_cols))\n",
    "    # check if delta exists at target_path by trying to read\n",
    "    delta_exists = False\n",
    "    try:\n",
    "        _ = spark.read.format(\"delta\").load(target_path)\n",
    "        delta_exists = True\n",
    "    except Exception:\n",
    "        delta_exists = False\n",
    "\n",
    "    if not delta_exists:\n",
    "        # initial write - partition by _year\n",
    "        print(\"Target delta does not exist — creating initial Delta at target path (partitioned by _year).\")\n",
    "        (df.write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"overwrite\")\n",
    "           .option(\"overwriteSchema\", \"true\")\n",
    "           .partitionBy(\"_year\")\n",
    "           .save(target_path))\n",
    "        print(\"Initial Delta created at\", target_path)\n",
    "    else:\n",
    "        # perform MERGE\n",
    "        print(\"Target delta exists — performing MERGE.\")\n",
    "        try:\n",
    "            dt = DeltaTable.forPath(spark, target_path)\n",
    "            src_temp = \"_src_df_for_merge_\" + uuid.uuid4().hex[:8]\n",
    "            df.createOrReplaceTempView(src_temp)\n",
    "\n",
    "            # Build SQL merge strings\n",
    "            pk_on = \" AND \".join([f\"target.`{k}` = source.`{k}`\" for k in pk_in_df])\n",
    "            update_cols = [c for c in df.columns if c not in pk_in_df]\n",
    "            # build update set clause\n",
    "            update_set = \", \".join([f\"target.`{c}` = source.`{c}`\" for c in update_cols])\n",
    "            insert_cols = \", \".join([f\"`{c}`\" for c in df.columns])\n",
    "            insert_values = \", \".join([f\"source.`{c}`\" for c in df.columns])\n",
    "\n",
    "            merge_sql = f\"\"\"\n",
    "              MERGE INTO delta.`{target_path}` AS target\n",
    "              USING (SELECT * FROM {src_temp}) AS source\n",
    "              ON {pk_on}\n",
    "              WHEN MATCHED THEN\n",
    "                UPDATE SET {update_set}\n",
    "              WHEN NOT MATCHED THEN\n",
    "                INSERT ({insert_cols}) VALUES ({insert_values})\n",
    "            \"\"\"\n",
    "            print(\"Running MERGE SQL...\")\n",
    "            spark.sql(merge_sql)\n",
    "            print(\"MERGE completed successfully.\")\n",
    "        except Exception:\n",
    "            print(\"MERGE via SQL failed; attempting DeltaTable API fallback.\")\n",
    "            try:\n",
    "                dt = DeltaTable.forPath(spark, target_path)\n",
    "                dt.alias(\"target\").merge(df.alias(\"source\"), pk_on) \\\n",
    "                  .whenMatchedUpdateAll() \\\n",
    "                  .whenNotMatchedInsertAll() \\\n",
    "                  .execute()\n",
    "                print(\"MERGE via DeltaTable API completed.\")\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "                raise\n",
    "else:\n",
    "    # merge_flag false -> simple write; overwrite for full load, append for incremental\n",
    "    if incremental_flag:\n",
    "        write_mode = \"append\"\n",
    "    else:\n",
    "        write_mode = \"overwrite\"\n",
    "\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(write_mode)\n",
    "       .option(\"overwriteSchema\", \"true\" if write_mode == \"overwrite\" else \"false\")\n",
    "       .partitionBy(\"_year\")\n",
    "       .save(target_path))\n",
    "    print(f\"Wrote data to {target_path} with mode {write_mode}.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Done - show sample from target\n",
    "# ---------------------------\n",
    "print(\"Validating target (reading back small sample):\")\n",
    "try:\n",
    "    tgt = spark.read.format(\"delta\").load(target_path)\n",
    "    print(\"Target row count (approx):\", tgt.count())\n",
    "    display(tgt.limit(5))\n",
    "except Exception:\n",
    "    print(\"Failed to read back Delta; listing target path instead:\")\n",
    "    try:\n",
    "        for f in dbutils.fs.ls(target_path):\n",
    "            print(\" -\", f.path)\n",
    "    except Exception as e:\n",
    "        print(\"Also failed to list target path:\", e)\n",
    "\n",
    "print(\"04_BronzeToSilver completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e9b2cc-5a4f-48d1-9881-046d5d3ff197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Robust end-to-end cell (no defs). Validates Source_path and avoids \"Path must be absolute\" error.\n",
    "import json, traceback, datetime\n",
    "from pyspark.sql.functions import col, trim, to_timestamp, year, lit, concat_ws, sha2, current_timestamp, coalesce\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ---------------------------\n",
    "# Widgets (ADF will pass these)\n",
    "# ---------------------------\n",
    "dbutils.widgets.text(\"domain\", \"\")\n",
    "dbutils.widgets.text(\"file_name\", \"\")            # e.g. Sales.Currency or Sales.Currency.csv\n",
    "dbutils.widgets.text(\"column_list\", \"\")          # JSON array OR CSV (not used here)\n",
    "dbutils.widgets.text(\"year_column\", \"\")          # optional\n",
    "dbutils.widgets.text(\"table_name\", \"\")           # optional override\n",
    "dbutils.widgets.text(\"batch_name\", \"\")           # optional batch id/name from ADF\n",
    "dbutils.widgets.text(\"direct_account_key\", \"\")   # optional storage account key (base64)\n",
    "dbutils.widgets.text(\"Source_path\", \"\")          # optional full path (preferred)\n",
    "dbutils.widgets.text(\"Target_path\", \"\")\n",
    "dbutils.widgets.text(\"include_layer\", \"false\")   # optional (true/false)\n",
    "dbutils.widgets.text(\"layer\", \"Bronze\")          # optional layer name\n",
    "dbutils.widgets.text(\"merge_flag\", \"true\")\n",
    "dbutils.widgets.text(\"incremental_flag\", \"false\")\n",
    "\n",
    "# ---------------------------\n",
    "# Read widget values\n",
    "# ---------------------------\n",
    "domain = dbutils.widgets.get(\"domain\").strip()\n",
    "file_name = dbutils.widgets.get(\"file_name\").strip()\n",
    "year_column = dbutils.widgets.get(\"year_column\").strip()\n",
    "table_name = dbutils.widgets.get(\"table_name\").strip()\n",
    "batch_name = dbutils.widgets.get(\"batch_name\").strip()\n",
    "direct_account_key = dbutils.widgets.get(\"direct_account_key\").strip()\n",
    "Source_path = dbutils.widgets.get(\"Source_path\").strip()\n",
    "Target_path = dbutils.widgets.get(\"Target_path\").strip()\n",
    "include_layer = dbutils.widgets.get(\"include_layer\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "layer = dbutils.widgets.get(\"layer\").strip()\n",
    "merge_flag = dbutils.widgets.get(\"merge_flag\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "incremental_flag = dbutils.widgets.get(\"incremental_flag\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "\n",
    "print(\"Raw widget summary:\")\n",
    "print(\" domain:\", domain, \" file_name:\", file_name, \" Source_path:\", Source_path)\n",
    "print(\" Target_path:\", Target_path, \" include_layer:\", include_layer, \" layer:\", layer)\n",
    "print(\" merge_flag:\", merge_flag, \" incremental:\", incremental_flag)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: check absolute path\n",
    "# ---------------------------\n",
    "def is_absolute_path(p):\n",
    "    if not p or not isinstance(p, str):\n",
    "        return False\n",
    "    low = p.lower()\n",
    "    return low.startswith(\"abfss://\") or low.startswith(\"wasbs://\") or low.startswith(\"/dbfs/\") or low.startswith(\"adl://\") or low.startswith(\"s3://\")\n",
    "\n",
    "# ---------------------------\n",
    "# If user supplied direct_account_key, configure both blob + dfs endpoints\n",
    "# ---------------------------\n",
    "# Best-effort extract account name from Target_path or Source_path; fallback to scrgvkrmade\n",
    "storage_account = None\n",
    "for p in (Source_path, Target_path):\n",
    "    if p and \"@\" in p and (\"dfs.core.windows.net\" in p or \"blob.core.windows.net\" in p):\n",
    "        try:\n",
    "            storage_account = p.split(\"@\",1)[1].split(\".\")[0]\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "if not storage_account:\n",
    "    # fallback to the account we have used in examples\n",
    "    storage_account = \"scrgvkrmade\"\n",
    "\n",
    "if direct_account_key:\n",
    "    # clean key (strip quotes/newlines)\n",
    "    k = direct_account_key.strip()\n",
    "    if (k.startswith('\"') and k.endswith('\"')) or (k.startswith(\"'\") and k.endswith(\"'\")):\n",
    "        k = k[1:-1]\n",
    "    k = k.strip()\n",
    "    print(\"Setting storage account keys for account:\", storage_account)\n",
    "    spark.conf.set(f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\", k)\n",
    "    spark.conf.set(f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\", k)\n",
    "else:\n",
    "    print(\"No direct_account_key provided — ensure cluster identity/secret scope/mount exists for storage access.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build Source_path if not absolute (best-effort)\n",
    "# ---------------------------\n",
    "if not is_absolute_path(Source_path):\n",
    "    # If Source_path was empty or wrong, attempt to construct using known pattern:\n",
    "    # abfss://project@<account>.dfs.core.windows.net/<layer>/<domain>/<file_name>/\n",
    "    container = \"project\"\n",
    "    acct = storage_account\n",
    "    # layer include\n",
    "    layer_part = layer if include_layer else \"\"\n",
    "    # prefer table_name if provided else file_name (strip extension)\n",
    "    target_folder = table_name if table_name else file_name\n",
    "    # try to sanitize\n",
    "    if target_folder:\n",
    "        # remove leading slashes\n",
    "        target_folder = target_folder.strip(\"/ \")\n",
    "    else:\n",
    "        # if no file_name/domain provided, fail with instructive message\n",
    "        msg = (\"Source_path not provided and cannot auto-build path because \"\n",
    "               \"both file_name and domain are empty. Please pass widget Source_path \"\n",
    "               \"or set domain/file_name. Example:\\n\"\n",
    "               \"abfss://project@{acct}.dfs.core.windows.net/bronze/<domain>/<file_name>/\").format(acct=acct)\n",
    "        raise RuntimeError(msg)\n",
    "    # assemble\n",
    "    # assume Bronze layer root unless layer param differs\n",
    "    root_layer = layer if layer else \"Bronze\"\n",
    "    Source_path = f\"abfss://{container}@{acct}.dfs.core.windows.net/{root_layer}/{domain}/{target_folder}/\"\n",
    "    print(\"Auto-built Source_path ->\", Source_path)\n",
    "else:\n",
    "    print(\"Using provided absolute Source_path ->\", Source_path)\n",
    "\n",
    "# final validate\n",
    "if not is_absolute_path(Source_path):\n",
    "    raise RuntimeError(\"Final Source_path is not an absolute path: \" + str(Source_path))\n",
    "\n",
    "# ---------------------------\n",
    "# READ SOURCE parquet (recursive)\n",
    "# ---------------------------\n",
    "try:\n",
    "    print(\"Reading parquet data from ->\", Source_path)\n",
    "    src_df = (spark.read\n",
    "              .option(\"mergeSchema\",\"true\")\n",
    "              .option(\"recursiveFileLookup\",\"true\")\n",
    "              .parquet(Source_path))\n",
    "    print(\"Read OK. Columns:\", src_df.columns)\n",
    "    display(src_df.limit(5))\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"Failed to read parquet from Source_path: \" + str(Source_path))\n",
    "\n",
    "# ---------------------------\n",
    "# CLEAN / TRANSFORM using ALL available columns (generic)\n",
    "# ---------------------------\n",
    "df = src_df\n",
    "# trim strings\n",
    "string_cols = [c for c,d in df.dtypes if d == \"string\"]\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, trim(col(c)))\n",
    "# year -> timestamp and _year if available\n",
    "if year_column and year_column in df.columns:\n",
    "    try:\n",
    "        df = df.withColumn(year_column, to_timestamp(col(year_column)))\n",
    "        df = df.withColumn(\"_year\", year(col(year_column)))\n",
    "    except Exception:\n",
    "        if \"_year\" not in df.columns:\n",
    "            df = df.withColumn(\"_year\", lit(datetime.datetime.utcnow().year))\n",
    "else:\n",
    "    if \"_year\" not in df.columns:\n",
    "        df = df.withColumn(\"_year\", lit(datetime.datetime.utcnow().year))\n",
    "# audit columns\n",
    "if \"__ingest_ts\" not in df.columns:\n",
    "    df = df.withColumn(\"__ingest_ts\", current_timestamp())\n",
    "if \"__source_file\" not in df.columns:\n",
    "    df = df.withColumn(\"__source_file\", lit(Source_path.split(\"/\")[-1] or Source_path))\n",
    "if \"__source_path\" not in df.columns:\n",
    "    df = df.withColumn(\"__source_path\", lit(Source_path))\n",
    "if \"__batch_id\" not in df.columns:\n",
    "    dbatch = batch_name if batch_name else \"Batch-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    df = df.withColumn(\"__batch_id\", lit(dbatch))\n",
    "\n",
    "# compute generic __row_hash using all columns\n",
    "concat_expr = concat_ws(\"||\", *[col(c).cast(\"string\") for c in df.columns])\n",
    "df = df.withColumn(\"__row_hash\", sha2(concat_expr, 256))\n",
    "\n",
    "print(\"After generic cleaning. Columns now:\", df.columns)\n",
    "display(df.limit(5))\n",
    "\n",
    "# ---------------------------\n",
    "# DEDUPE (use pk if provided via pk_columns in column_list widget or fallback to __row_hash)\n",
    "# ---------------------------\n",
    "# try to read pk_columns from column_list if user passed json and included pk info (best-effort)\n",
    "pk_list = []\n",
    "try:\n",
    "    maybe_cols = dbutils.widgets.get(\"column_list\").strip()\n",
    "    if maybe_cols:\n",
    "        # try json parse\n",
    "        try:\n",
    "            parsed = json.loads(maybe_cols)\n",
    "            if isinstance(parsed, dict) and parsed.get(\"pk_columns\"):\n",
    "                pk_list = parsed.get(\"pk_columns\", [])\n",
    "            elif isinstance(parsed, list):\n",
    "                # nothing special; user passed columns list; no pk\n",
    "                pk_list = []\n",
    "        except Exception:\n",
    "            pk_list = []\n",
    "except Exception:\n",
    "    pk_list = []\n",
    "\n",
    "if pk_list:\n",
    "    valid_pk = [p for p in pk_list if p in df.columns]\n",
    "else:\n",
    "    valid_pk = []\n",
    "\n",
    "if valid_pk:\n",
    "    try:\n",
    "        before = df.count()\n",
    "    except Exception:\n",
    "        before = None\n",
    "    df = df.dropDuplicates(valid_pk)\n",
    "    if before is not None:\n",
    "        after = df.count()\n",
    "        print(f\"Dropped duplicates by PK {valid_pk}: {before-after} rows removed.\")\n",
    "else:\n",
    "    # fallback: dedupe by __row_hash\n",
    "    df = df.dropDuplicates([\"__row_hash\"])\n",
    "    print(\"Dropped duplicates by __row_hash.\")\n",
    "\n",
    "# ---------------------------\n",
    "# WRITE / MERGE into Delta\n",
    "# ---------------------------\n",
    "print(\"Preparing to write/merge to target Delta:\", Target_path)\n",
    "delta_exists = True\n",
    "try:\n",
    "    _ = spark.read.format(\"delta\").load(Target_path)\n",
    "except Exception:\n",
    "    delta_exists = False\n",
    "\n",
    "if not delta_exists:\n",
    "    print(\"Target Delta not found -> creating initial Delta.\")\n",
    "    (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .partitionBy(\"_year\")\n",
    "        .save(Target_path))\n",
    "    print(\"Initial Delta created at:\", Target_path)\n",
    "else:\n",
    "    # choose merge key: try CurrencyAlternateKey if present, else __row_hash\n",
    "    merge_pks = [c for c in [\"CurrencyAlternateKey\"] if c in df.columns]\n",
    "    if not merge_pks:\n",
    "        merge_pks = [\"__row_hash\"]\n",
    "    join_cond = \" AND \".join([f\"target.`{c}` = source.`{c}`\" for c in merge_pks])\n",
    "    print(\"Merging using keys:\", merge_pks)\n",
    "    dt = DeltaTable.forPath(spark, Target_path)\n",
    "    dt.alias(\"target\").merge(\n",
    "        df.alias(\"source\"),\n",
    "        join_cond\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "    print(\"MERGE completed into:\", Target_path)\n",
    "\n",
    "# ---------------------------\n",
    "# final sample\n",
    "# ---------------------------\n",
    "try:\n",
    "    tgt = spark.read.format(\"delta\").load(Target_path)\n",
    "    print(\"Target approx row count:\", tgt.count())\n",
    "    display(tgt.limit(5))\n",
    "except Exception as e:\n",
    "    print(\"Could not read target after write/merge:\", e)\n",
    "    try:\n",
    "        for f in dbutils.fs.ls(Target_path):\n",
    "            print(\"-\", f.path)\n",
    "    except Exception as e2:\n",
    "        print(\"Also failed to list target path:\", e2)\n",
    "\n",
    "print(\"End.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_BronzeToSilver_2",
   "widgets": {
    "Source_path": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/HumanResources.Employee/",
     "nuid": "5cdb67cf-be90-45ef-b4e8-692473f10c52",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Currency/",
      "label": null,
      "name": "Source_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Currency/",
      "label": null,
      "name": "Source_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_path": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_employee/",
     "nuid": "663fa8d4-c0ee-4ac5-a457-1e0552b89b39",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "batch_name": {
     "currentValue": "Batch B",
     "nuid": "e3f9eb1b-0c40-41ab-8941-42d1d1eacbf3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Batch B",
      "label": null,
      "name": "batch_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Batch B",
      "label": null,
      "name": "batch_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "column_list": {
     "currentValue": "[\"BusinessEntityID\",\"NationalIDNumber\",\"LoginID\",\"OrganizationNode\",\"OrganizationLevel\",\"JobTitle\",\"BirthDate\",\"MaritalStatus\",\"Gender\",\"HireDate\",\"SalariedFlag\",\"VacationHours\",\"SickLeaveHours\",\"CurrentFlag\",\"rowguid\",\"ModifiedDate\", \"__ingest_ts\",\"__source_file\",\"__source_path\",\"__batch_id\",\"__row_hash\"]",
     "nuid": "df295323-c861-4167-b237-e877eb47f94b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "[\"CurrencyCode\",\"CurrencyAlternateKey\",\"CurrencyName\",\"ModifiedDate\",\"__ingest_ts\",\"__source_file\",\"__source_path\",\"__batch_id\",\"__row_hash\"]",
      "label": null,
      "name": "column_list",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "[\"CurrencyCode\",\"CurrencyAlternateKey\",\"CurrencyName\",\"ModifiedDate\",\"__ingest_ts\",\"__source_file\",\"__source_path\",\"__batch_id\",\"__row_hash\"]",
      "label": null,
      "name": "column_list",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "direct_account_key": {
     "currentValue": "E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==",
     "nuid": "55cd6bf0-4473-4c37-9025-082125999e53",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==",
      "label": null,
      "name": "direct_account_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==",
      "label": null,
      "name": "direct_account_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "domain": {
     "currentValue": "ResellerSales",
     "nuid": "575b844c-6a59-42cb-96f1-0ecf2e775ed3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ResellerSales",
      "label": null,
      "name": "domain",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ResellerSales",
      "label": null,
      "name": "domain",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "file_name": {
     "currentValue": "stg.currency",
     "nuid": "7d5fe1b3-004e-48d1-a41f-08184bd62586",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "stg.currency",
      "label": null,
      "name": "file_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "stg.currency",
      "label": null,
      "name": "file_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "include_layer": {
     "currentValue": "false",
     "nuid": "5ee4656b-8cce-424b-823e-c4e862c1df8f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "include_layer",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": null,
      "name": "include_layer",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "incremental_flag": {
     "currentValue": "false",
     "nuid": "f024aaf7-2251-4d04-a556-1fc9f20703ea",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "incremental_flag",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": null,
      "name": "incremental_flag",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "layer": {
     "currentValue": "Bronze",
     "nuid": "a0d966f2-f074-49d2-a3c5-189af1bea57b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Bronze",
      "label": null,
      "name": "layer",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Bronze",
      "label": null,
      "name": "layer",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "merge_flag": {
     "currentValue": "true",
     "nuid": "e5756b18-2e1e-4c71-942d-17293567db5e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "true",
      "label": null,
      "name": "merge_flag",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "true",
      "label": null,
      "name": "merge_flag",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "pk_columns": {
     "currentValue": "[\"NationalIDNumber\"]",
     "nuid": "a5b50b56-294d-4b58-86eb-f790638e4051",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "[\"CurrencyAlternateKey\"]",
      "label": null,
      "name": "pk_columns",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "[\"CurrencyAlternateKey\"]",
      "label": null,
      "name": "pk_columns",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "stg.employee",
     "nuid": "f1e9fc06-b1f1-4c63-9f57-c4fa04fbf734",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "stg.currency",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "stg.currency",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "watermark": {
     "currentValue": "",
     "nuid": "a7e1dacb-c257-4c87-97bb-ccf985d123eb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "watermark",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "watermark",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "year_column": {
     "currentValue": "ModifiedDate",
     "nuid": "2627bbde-edd9-422b-9d2f-c4dd8b21b6e0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ModifiedDate",
      "label": null,
      "name": "year_column",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ModifiedDate",
      "label": null,
      "name": "year_column",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
