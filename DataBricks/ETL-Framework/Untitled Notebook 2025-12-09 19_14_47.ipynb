{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b55e77-8c71-46c7-960c-b801e6bd8fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ResellerKey','GeographyKey','ResellerAlternateKey','Phone','BusinessType','ResellerName','NumberEmployees','OrderFrequency','OrderMonth','FirstOrderYear','LastOrderYear','ProductLine','AddressLine1','AddressLine2','AnnualSales','BankName','MinPaymentType','MinPaymentAmount','AnnualRevenue','YearOpened'\tSales.Store.csv :[\"BusinessEntityID\", \"Name\", \"SalesPersonID\", \"Demographics\", \"rowguid\", \"ModifiedDate\"]\tSales.Customer.csv:[\"CustomerID\", \"PersonID\", \"StoreID\", \"TerritoryID\", \"AccountNumber\", \"rowguid\", \"ModifiedDate\"]\tPerson.Address.csv:[\"AddressID\", \"AddressLine1\", \"AddressLine2\", \"City\", \"StateProvinceID\", \"PostalCode\", \"SpatialLocation\", \"rowguid\", \"ModifiedDate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e17160-7d97-40a5-ab84-88cb9c8c69ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. SETUP: Imports and Configurations... (omitted for brevity)\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, trim, to_timestamp, coalesce, sha2, \n",
    "    concat_ws, year\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# 1a. Define File Locations (Paths) - ASSUMED LOCATIONS\n",
    "SOURCE_PATH_STORE = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Store/\"\n",
    "SOURCE_PATH_CUSTOMER = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Customer/\"\n",
    "SOURCE_PATH_ADDRESS = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Person.Address/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/dim/dim_reseller/\"\n",
    "\n",
    "# 1b. Define the Unique Identifier (Primary Key)\n",
    "PK_RAW = \"ResellerAlternateKey\"\n",
    "PRIMARY_KEYS = [c.strip() for c in PK_RAW.split(\",\") if c.strip()]\n",
    "\n",
    "# 1d. Setup Storage Access (Authentication) - REPLACE WITH YOUR KEY!\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", account_key)\n",
    "PK_COLS = PRIMARY_KEYS\n",
    "print(f\"Using Primary Key for MERGE: {PK_COLS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c7ffaa-50d7-4bdb-9004-5fa875f6b9a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # 2a. Read Source DataFrames\n",
    "    df_store = spark.read.option(\"mergeSchema\", \"true\").parquet(SOURCE_PATH_STORE)\n",
    "    df_customer = spark.read.option(\"mergeSchema\", \"true\").parquet(SOURCE_PATH_CUSTOMER)\n",
    "    df_address = spark.read.option(\"mergeSchema\", \"true\").parquet(SOURCE_PATH_ADDRESS)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading source data. Exiting: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ===> ACTION: PRINT SCHEMA HERE TO CONFIRM COLUMN NAMES <===\n",
    "print(\"\\n--- SCHEMA CHECK: df_store ---\")\n",
    "df_store.printSchema()\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "# 2b. Register DataFrames as Temporary Views\n",
    "df_store.createOrReplaceTempView(\"Store\")\n",
    "df_customer.createOrReplaceTempView(\"Customer\")\n",
    "df_address.createOrReplaceTempView(\"Address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f308a68-4f5a-41eb-9991-2dc2e485315c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2c. Define and Execute the Spark SQL Join Query\n",
    "print(\"2. Joining data using Spark SQL...\")\n",
    "\n",
    "# **!!! UPDATE THE COLUMN NAMES BELOW based on the SCHEMA CHECK !!!**\n",
    "# Assuming the column is correctly named 'BusinessEntityID' as per your initial plan:\n",
    "spark_sql_query = \"\"\"\n",
    "SELECT\n",
    "    -- Keys & Basic Attributes from Store (Base Reseller)\n",
    "    s.BusinessEntityID AS ResellerAlternateKey,\n",
    "    s.Name AS ResellerName,\n",
    "    s.Phone,\n",
    "    s.ModifiedDate AS ModifiedDate_Store,\n",
    "    \n",
    "    -- Address Attributes\n",
    "    a.AddressLine1,\n",
    "    a.AddressLine2,\n",
    "    a.ModifiedDate AS ModifiedDate_Addr,\n",
    "    \n",
    "    -- Geography Link\n",
    "    c.TerritoryID AS TerritoryID_Reseller\n",
    "    \n",
    "FROM\n",
    "    Store s\n",
    "LEFT JOIN\n",
    "    Customer c ON s.BusinessEntityID = c.StoreID       -- Link Store to Customer\n",
    "LEFT JOIN\n",
    "    Address a ON c.AddressID = a.AddressID             -- Link Customer to Address\n",
    "\"\"\"\n",
    "# If the column is NOT 'BusinessEntityID', change it in the SQL above. \n",
    "# For example, if it's 'StoreID', you would change s.BusinessEntityID to s.StoreID.\n",
    "\n",
    "df_final = spark.sql(spark_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f871ab0-a7cd-46e6-aaa2-031f97294431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1a. Define File Locations (Paths) - ASSUMED LOCATIONS\n",
    "SOURCE_PATH_STORE = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Store/\"\n",
    "SOURCE_PATH_CUSTOMER = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Customer/\"\n",
    "SOURCE_PATH_ADDRESS = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Person.Address/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/dim/dim_reseller/\"\n",
    "\n",
    "# 1b. Define the Unique Identifier (Primary Key)\n",
    "PK_RAW = \"ResellerAlternateKey\"\n",
    "PRIMARY_KEYS = [c.strip() for c in PK_RAW.split(\",\") if c.strip()]\n",
    "\n",
    "# 1d. Setup Storage Access (Authentication) - REPLACE WITH YOUR KEY!\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", account_key)\n",
    "PK_COLS = PRIMARY_KEYS\n",
    "print(f\"Using Primary Key for MERGE: {PK_COLS}\")\n",
    "\n",
    "# 2a. Read Source DataFrames\n",
    "df_store = spark.read.option(\"mergeSchema\", \"true\").parquet(SOURCE_PATH_STORE)\n",
    "df_customer = spark.read.option(\"mergeSchema\", \"true\").parquet(SOURCE_PATH_CUSTOMER)\n",
    "df_address = spark.read.option(\"mergeSchema\", \"true\").parquet(SOURCE_PATH_ADDRESS)\n",
    "\n",
    "# 2b. Register DataFrames as Temporary Views\n",
    "df_store.createOrReplaceTempView(\"Store\")\n",
    "df_customer.createOrReplaceTempView(\"Customer\")\n",
    "df_address.createOrReplaceTempView(\"Address\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# *** CRITICAL DEBUGGING SECTION ***\n",
    "print(\"\\n--- SCHEMA CHECK: Store Table Columns ---\")\n",
    "df_store.printSchema()\n",
    "print(\"------------------------------------------\\n\")\n",
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312213e3-9060-4e2c-ab70-80539d273a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "spark_sql_query = \"\"\"\n",
    "SELECT\n",
    "    -- KEYS: CHANGE 'BusinessEntityID' to 'StoreID' here\n",
    "    s.StoreID AS ResellerAlternateKey, \n",
    "    s.Name AS ResellerName,\n",
    "    s.Phone,\n",
    "    s.ModifiedDate AS ModifiedDate_Store,\n",
    "    \n",
    "    -- Address Attributes\n",
    "    a.AddressLine1,\n",
    "    a.AddressLine2,\n",
    "    a.ModifiedDate AS ModifiedDate_Addr,\n",
    "    \n",
    "    -- Geography Link\n",
    "    c.TerritoryID AS TerritoryID_Reseller\n",
    "    \n",
    "FROM\n",
    "    Store s\n",
    "LEFT JOIN\n",
    "    Customer c ON s.StoreID = c.StoreID       -- CHANGE 'BusinessEntityID' to 'StoreID' here\n",
    "LEFT JOIN\n",
    "    Address a ON c.AddressID = a.AddressID             \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc37ce9-ff24-416c-8402-51dd6a080b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1a. Define File Locations (Paths) - ASSUMED LOCATIONS\n",
    "SOURCE_PATH_STORE = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Store/\"\n",
    "SOURCE_PATH_CUSTOMER = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Customer/\"\n",
    "SOURCE_PATH_ADDRESS = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Person.Address/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/dim/dim_reseller/\"\n",
    "\n",
    "\n",
    "# 1d. Setup Storage Access (Authentication) - REPLACE WITH YOUR KEY!\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", account_key)\n",
    "\n",
    "# 2. Read parquet with mergeSchema (if needed) and create temp views\n",
    "df_store = spark.read.option(\"mergeSchema\", \"true\").parquet(SOURCE_PATH_STORE)\n",
    "df_customer = spark.read.option(\"mergeSchema\", \"true\").parquet(SOURCE_PATH_CUSTOMER)\n",
    "df_address = spark.read.option(\"mergeSchema\", \"true\").parquet(SOURCE_PATH_ADDRESS)\n",
    "\n",
    "df_store.createOrReplaceTempView(\"src_store\")\n",
    "df_customer.createOrReplaceTempView(\"src_customer\")\n",
    "df_address.createOrReplaceTempView(\"src_address\")\n",
    "\n",
    "print(\"Temp views created: src_store, src_customer, src_address\")\n",
    "print(\"TARGET_PATH =\", TARGET_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73daab09-503b-4013-ba5f-97365373d114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# show schema for each temp view\n",
    "print(\"=== src_store schema ===\")\n",
    "spark.table(\"src_store\").printSchema()\n",
    "print(\"=== src_store sample rows ===\")\n",
    "spark.table(\"src_store\").limit(10).toPandas()\n",
    "\n",
    "print(\"=== src_customer schema ===\")\n",
    "spark.table(\"src_customer\").printSchema()\n",
    "print(\"=== src_customer sample rows ===\")\n",
    "spark.table(\"src_customer\").limit(10).toPandas()\n",
    "\n",
    "print(\"=== src_address schema ===\")\n",
    "spark.table(\"src_address\").printSchema()\n",
    "print(\"=== src_address sample rows ===\")\n",
    "spark.table(\"src_address\").limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55edc0eb-fc20-4a24-85a1-25cc3c335176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this Python cell in your notebook (assumes src_store, src_customer, src_address temp views exist).\n",
    "import re\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def show_schema_and_sample(view_name, n=5):\n",
    "    print(f\"\\n--- {view_name} schema ---\")\n",
    "    df = spark.table(view_name)\n",
    "    df.printSchema()\n",
    "    print(f\"--- sample rows ({view_name}) ---\")\n",
    "    display(df.limit(n))\n",
    "\n",
    "# 1) show schemas & a few rows to inspect\n",
    "show_schema_and_sample(\"src_store\")\n",
    "show_schema_and_sample(\"src_customer\")\n",
    "show_schema_and_sample(\"src_address\")\n",
    "\n",
    "# 2) helper to find candidate key columns by name heuristics\n",
    "def find_candidates(cols):\n",
    "    patterns = [\n",
    "        r'\\bstore\\b', r'\\bstoreid\\b', r'\\bstore_id\\b', r'\\bid\\b', r'\\baddressid\\b',\n",
    "        r'\\breseller\\b', r'\\bbusiness\\b', r'\\bentity\\b', r'\\bkey\\b'\n",
    "    ]\n",
    "    candidates = []\n",
    "    for c in cols:\n",
    "        score = 0\n",
    "        lower = c.lower()\n",
    "        if re.search(r'\\bstoreid\\b|\\bstore_id\\b', lower): score += 10\n",
    "        if re.search(r'\\breseller\\b', lower): score += 8\n",
    "        if re.search(r'\\bbusinessentity\\b|\\bbusiness_entity\\b', lower): score += 8\n",
    "        if re.search(r'\\bbusiness\\b|\\bentity\\b', lower): score += 5\n",
    "        if re.search(r'\\bid\\b', lower): score += 3\n",
    "        if re.search(r'\\bkey\\b', lower): score += 2\n",
    "        if score>0:\n",
    "            candidates.append((c, score))\n",
    "    # sort descending by score, then by name\n",
    "    candidates.sort(key=lambda x: (-x[1], x[0]))\n",
    "    return [c for c,s in candidates]\n",
    "\n",
    "# get columns\n",
    "cols_store = spark.table(\"src_store\").columns\n",
    "cols_cust = spark.table(\"src_customer\").columns\n",
    "cols_addr = spark.table(\"src_address\").columns\n",
    "\n",
    "cand_store = find_candidates(cols_store)\n",
    "cand_cust = find_candidates(cols_cust)\n",
    "\n",
    "print(\"\\nCandidate key columns found in src_store (best first):\", cand_store)\n",
    "print(\"Candidate key columns found in src_customer (best first):\", cand_cust)\n",
    "\n",
    "# 3) try to pick best pair to join on:\n",
    "chosen_store_col = None\n",
    "chosen_cust_col = None\n",
    "\n",
    "# prefer exact matching names between store and customer (case-insensitive)\n",
    "lower_cust = {c.lower(): c for c in cols_cust}\n",
    "for c in cand_store:\n",
    "    if c.lower() in lower_cust:\n",
    "        chosen_store_col = c\n",
    "        chosen_cust_col = lower_cust[c.lower()]\n",
    "        break\n",
    "\n",
    "# else, if customer has obvious StoreID-like column, use it and match to the best store candidate\n",
    "if not chosen_store_col and cand_cust:\n",
    "    chosen_cust_col = cand_cust[0]\n",
    "    # try to find store column named like chosen_cust_col\n",
    "    if chosen_cust_col.lower() in {c.lower() for c in cols_store}:\n",
    "        chosen_store_col = [c for c in cols_store if c.lower()==chosen_cust_col.lower()][0]\n",
    "    else:\n",
    "        # fallback: pick top store candidate\n",
    "        chosen_store_col = cand_store[0] if cand_store else None\n",
    "\n",
    "# If still nothing, pick the top candidates if present\n",
    "if not chosen_store_col and cand_store:\n",
    "    chosen_store_col = cand_store[0]\n",
    "if not chosen_cust_col and cand_cust:\n",
    "    chosen_cust_col = cand_cust[0]\n",
    "\n",
    "print(\"\\nAuto-chosen join columns:\")\n",
    "print(\"  src_store ->\", chosen_store_col)\n",
    "print(\"  src_customer ->\", chosen_cust_col)\n",
    "\n",
    "# 4) if we have candidates, create vw_reseller_source using those names (with safe quoting)\n",
    "if not chosen_store_col or not chosen_cust_col:\n",
    "    raise RuntimeError(\"No good key candidates found. Inspect the printed schemas above and pick the correct join columns.\")\n",
    "\n",
    "# Build and run the SQL to create vw_reseller_source using the chosen names.\n",
    "store_col = chosen_store_col\n",
    "cust_col = chosen_cust_col\n",
    "\n",
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW vw_reseller_source AS\n",
    "SELECT\n",
    "  COALESCE(CAST(s.`{store_col}` AS BIGINT), CAST(c.`{cust_col}` AS BIGINT)) AS ResellerAlternateKey,\n",
    "  trim(s.`Name`) AS ResellerName,\n",
    "  trim(coalesce(s.`Phone`, 'N/A')) AS Phone,\n",
    "  a.AddressLine1 AS AddressLine1,\n",
    "  CASE WHEN a.AddressLine2 IS NULL OR a.AddressLine2 = '' THEN NULL ELSE a.AddressLine2 END AS AddressLine2,\n",
    "  to_timestamp(coalesce(a.ModifiedDate, s.ModifiedDate)) AS ModifiedDate,\n",
    "  coalesce(c.TerritoryID, -1) AS GeographyKey,\n",
    "  year(to_timestamp(coalesce(a.ModifiedDate, s.ModifiedDate))) AS _year,\n",
    "  current_timestamp() AS LoadTS,\n",
    "  sha2(concat_ws('||',\n",
    "       coalesce(trim(s.`Name`), ''),\n",
    "       coalesce(trim(coalesce(s.`Phone`, 'N/A')), ''),\n",
    "       coalesce(a.AddressLine1, ''),\n",
    "       coalesce(a.AddressLine2, '')\n",
    "     ), 256) AS __row_hash,\n",
    "  current_timestamp() AS __ingest_ts,\n",
    "  '{SOURCE_PATH_STORE if 'SOURCE_PATH_STORE' in globals() else ''}' AS __source_path,\n",
    "  '{TARGET_PATH if 'TARGET_PATH' in globals() else ''}' AS __target_path,\n",
    "  concat('Batch-', date_format(current_timestamp(), 'yyyyMMddHHmmss')) AS __batch_id,\n",
    "  NULL AS ResellerKey,\n",
    "  NULL AS BusinessType,\n",
    "  NULL AS NumberEmployees,\n",
    "  NULL AS OrderFrequency,\n",
    "  NULL AS OrderMonth,\n",
    "  NULL AS FirstOrderYear,\n",
    "  NULL AS LastOrderYear,\n",
    "  NULL AS ProductLine,\n",
    "  NULL AS AnnualSales,\n",
    "  NULL AS BankName,\n",
    "  NULL AS MinPaymentType,\n",
    "  NULL AS MinPaymentAmount,\n",
    "  NULL AS AnnualRevenue,\n",
    "  NULL AS YearOpened\n",
    "FROM\n",
    "  (SELECT DISTINCT * FROM src_store) s\n",
    "LEFT JOIN\n",
    "  (SELECT DISTINCT * FROM src_customer) c\n",
    "  ON CAST(s.`{store_col}` AS BIGINT) = CAST(c.`{cust_col}` AS BIGINT)\n",
    "LEFT JOIN\n",
    "  (SELECT AddressID AS AddressID_Addr, AddressLine1, AddressLine2, ModifiedDate FROM src_address) a\n",
    "  ON coalesce(c.AddressID, 0) = coalesce(a.AddressID_Addr, 0)\n",
    "WHERE COALESCE(CAST(s.`{store_col}` AS BIGINT), CAST(c.`{cust_col}` AS BIGINT)) IS NOT NULL\n",
    "\"\"\"\n",
    "print(\"\\n--- Creating vw_reseller_source with SQL using chosen columns ---\\n\")\n",
    "print(sql[:1000], \"...\")  # print a snippet for review\n",
    "spark.sql(sql)\n",
    "print(\"\\nvw_reseller_source created successfully. Run: spark.table('vw_reseller_source').show(5) to confirm.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-12-09 19_14_47",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
