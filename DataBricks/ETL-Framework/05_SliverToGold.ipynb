{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "339a2c3f-a875-48dc-9de8-0b62095a9c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_product_subcategory/\n",
    "/mnt/adls/project/gold/dim_employee/\n",
    "abfss://project@scrgvkrmade.dfs.core.windows.net/gold/dim_employee/\n",
    "\n",
    "[\"EmployeeKey\",\"EmployeeAlternateKey\",\"EmployeeNationalIDAlternateKey\",\"JobTitle\",\"HireDate\",\"BirthDate\",\"MaritalStatus\",\"Gender\",\"SalariedFlag\",\"VacationHours\",\"SickLeaveHours\",\"CurrentFlag\",\"EffectiveDate\",\"EndDate\",\"IsCurrent\",\"__source_path\",\"LoadTS\",\"RowHash\"]\n",
    "\n",
    "([\"__ingest_ts\",\"_ingest_ts\",\"__source_path\",\"_source_path\",\"_batch_id\",\"__row_hash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59693c7b-7110-42f0-9064-5b8ef0285ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Silver_To_Gold - simple upsert (beginner)\n",
    "# =========================\n",
    "\n",
    "import json, datetime, traceback\n",
    "from pyspark.sql.functions import col, trim, current_timestamp, lit, concat_ws, sha2, to_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Widgets\n",
    "dbutils.widgets.text(\"Source_path\", \"\")         # silver delta path\n",
    "dbutils.widgets.text(\"Target_path\", \"\")         # gold delta path\n",
    "dbutils.widgets.text(\"pk_columns\", \"\")          # JSON array or CSV e.g. [\"CurrencyAlternateKey\"]\n",
    "dbutils.widgets.text(\"column_list\", \"\")         # optional business columns list\n",
    "dbutils.widgets.text(\"table_name\", \"\")          # friendly name\n",
    "dbutils.widgets.text(\"merge_flag\", \"true\")      # true -> MERGE upsert, false -> overwrite\n",
    "dbutils.widgets.text(\"incremental_flag\", \"false\")\n",
    "dbutils.widgets.text(\"watermark\", \"\")           # e.g. \"2025-12-02T07:40:46\"\n",
    "dbutils.widgets.text(\"direct_account_key\", \"\")\n",
    "\n",
    "# Read widgets\n",
    "Source_path = dbutils.widgets.get(\"Source_path\").strip()\n",
    "Target_path = dbutils.widgets.get(\"Target_path\").strip()\n",
    "pk_raw = dbutils.widgets.get(\"pk_columns\").strip()\n",
    "col_list_raw = dbutils.widgets.get(\"column_list\").strip()\n",
    "merge_flag = dbutils.widgets.get(\"merge_flag\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "incremental_flag = dbutils.widgets.get(\"incremental_flag\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "watermark = dbutils.widgets.get(\"watermark\").strip()\n",
    "direct_key = dbutils.widgets.get(\"direct_account_key\").strip()\n",
    "\n",
    "# Basic validation\n",
    "if not Source_path or not Target_path:\n",
    "    raise RuntimeError(\"Provide Source_path and Target_path widgets.\")\n",
    "\n",
    "# Optional storage key\n",
    "if direct_key:\n",
    "    k = direct_key.strip().strip('\"').strip(\"'\")\n",
    "    acct = None\n",
    "    for p in (Source_path, Target_path):\n",
    "        if p and \"@\" in p:\n",
    "            acct = p.split(\"@\",1)[1].split(\".\")[0]; break\n",
    "    if not acct:\n",
    "        acct = \"scrgvkrmade\"\n",
    "    spark.conf.set(f\"fs.azure.account.key.{acct}.dfs.core.windows.net\", k)\n",
    "    print(\"Configured storage key for account:\", acct)\n",
    "else:\n",
    "    print(\"No storage key passed; rely on cluster permission.\")\n",
    "\n",
    "# parse lists\n",
    "def parse_list(txt):\n",
    "    if not txt:\n",
    "        return []\n",
    "    try:\n",
    "        v = json.loads(txt)\n",
    "        if isinstance(v, list):\n",
    "            return [str(x).strip() for x in v if str(x).strip()]\n",
    "    except:\n",
    "        return [p.strip() for p in txt.split(\",\") if p.strip()]\n",
    "    return []\n",
    "\n",
    "pk_columns = parse_list(pk_raw)\n",
    "business_columns = parse_list(col_list_raw)\n",
    "print(\"PK columns:\", pk_columns)\n",
    "print(\"Business columns:\", business_columns)\n",
    "\n",
    "# Read source (delta preferred)\n",
    "try:\n",
    "    src = spark.read.format(\"delta\").load(Source_path)\n",
    "    print(\"Loaded Silver Delta:\", Source_path)\n",
    "except Exception:\n",
    "    src = spark.read.parquet(Source_path)\n",
    "    print(\"Loaded Silver Parquet:\", Source_path)\n",
    "\n",
    "print(\"Source columns:\", src.columns)\n",
    "display(src.limit(5))\n",
    "\n",
    "# Incremental filter if requested\n",
    "if incremental_flag:\n",
    "    if not watermark:\n",
    "        raise RuntimeError(\"incremental_flag=true requires watermark.\")\n",
    "    ts_col = None\n",
    "    for cand in (\"ModifiedDate\",\"__ingest_ts\",\"_ingestion_ts\"):\n",
    "        if cand in src.columns:\n",
    "            ts_col = cand; break\n",
    "    if not ts_col:\n",
    "        raise RuntimeError(\"No timestamp column found for incremental filtering.\")\n",
    "    try:\n",
    "        src = src.withColumn(ts_col, to_timestamp(col(ts_col)))\n",
    "    except Exception:\n",
    "        pass\n",
    "    src = src.filter(col(ts_col) > lit(watermark))\n",
    "    print(\"Rows after watermark filter:\", src.count())\n",
    "    display(src.limit(5))\n",
    "else:\n",
    "    print(\"Full load (no incremental filter).\")\n",
    "\n",
    "except Exception:\n",
    "else:\n",
    "    exclude = set([\"__ingest_ts\",\"_ingest_ts\",\"__source_path\",\"_source_path\",\"_batch_id\",\"__row_hash\"])\n",
    "    hash_cols = [c for c in src.columns if c not in exclude]\n",
    "\n",
    "if not hash_cols:\n",
    "    raise RuntimeError(\"No columns available for business hash.\")\n",
    "src = src.withColumn(\"__business_hash\", sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in hash_cols]), 256))\n",
    "print(\"Computed __business_hash on:\", hash_cols)\n",
    "display(src.limit(5))\n",
    "\n",
    "# Choose match key(s)\n",
    "valid_pk = [p for p in pk_columns if p in src.columns]\n",
    "if not valid_pk:\n",
    "    print(\"No valid PK found in source; falling back to __business_hash for matching.\")\n",
    "    valid_pk = [\"__business_hash\"]\n",
    "\n",
    "# Write initial or merge/upsert\n",
    "target_exists = True\n",
    "try:\n",
    "    spark.read.format(\"delta\").load(Target_path)\n",
    "except Exception:\n",
    "    target_exists = False\n",
    "\n",
    "if not target_exists:\n",
    "    print(\"Target not found. Writing initial Gold table.\")\n",
    "    out = src.withColumn(\"LoadTS\", current_timestamp())\n",
    "    out.write.format(\"delta\").mode(\"overwrite\").save(Target_path)\n",
    "    print(\"Initial Gold written.\")\n",
    "else:\n",
    "    if not merge_flag:\n",
    "        print(\"merge_flag=false -> overwrite target.\")\n",
    "        out = src.withColumn(\"LoadTS\", current_timestamp())\n",
    "        out.write.format(\"delta\").mode(\"overwrite\").save(Target_path)\n",
    "        print(\"Overwrite done.\")\n",
    "    else:\n",
    "        print(\"merge_flag=true -> performing upsert MERGE using PKs:\", valid_pk)\n",
    "        dt = DeltaTable.forPath(spark, Target_path)\n",
    "        join_cond = \" AND \".join([f\"target.`{c}` = source.`{c}`\" for c in valid_pk])\n",
    "        dt.alias(\"target\").merge(\n",
    "            src.alias(\"source\"),\n",
    "            join_cond\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "        print(\"Merge completed.\")\n",
    "\n",
    "# Validate\n",
    "try:\n",
    "    tgt = spark.read.format(\"delta\").load(Target_path)\n",
    "    print(\"Gold rows approx:\", tgt.count())\n",
    "    display(tgt.limit(5))\n",
    "except Exception:\n",
    "    print(\"Could not read gold target for validation.\")\n",
    "\n",
    "print(\"Silver_To_Gold finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba3ebb2d-250f-4fa7-ae49-1dce5821601d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# SIMPLE DIM LOADER (SCD2 friendly) - beginner style\n",
    "# - Reads one or more Silver sources (semicolon-separated)\n",
    "# - Builds business columns, surrogate key, SCD2 fields\n",
    "# - MERGES into Gold target using pk_columns (or __business_hash fallback)\n",
    "# ====================================================\n",
    "\n",
    "import json, datetime, uuid, traceback\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, current_timestamp, lit, concat_ws, sha2, to_timestamp, year, row_number, monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --------- WIDGETS (set these in notebook or from ADF) ----------\n",
    "dbutils.widgets.text(\"Source_path\", \"\")        # semicolon-separated Silver path(s)\n",
    "dbutils.widgets.text(\"Target_path\", \"\")        # Gold path where dim will be written\n",
    "dbutils.widgets.text(\"pk_columns\", \"\")         # JSON array or CSV, e.g. '[\"CurrencyAlternateKey\"]'\n",
    "dbutils.widgets.text(\"column_list\", \"\")        # JSON array or CSV of business columns to keep in dim\n",
    "dbutils.widgets.text(\"table_name\", \"\")         # friendly name, e.g., dbo.DimCurrency\n",
    "dbutils.widgets.text(\"merge_flag\", \"true\")     # true -> MERGE (SCD2 upsert)\n",
    "dbutils.widgets.text(\"direct_account_key\", \"\") # optional\n",
    "\n",
    "# --------- read widgets ----------\n",
    "Source_path_raw = dbutils.widgets.get(\"Source_path\").strip()\n",
    "Target_path = dbutils.widgets.get(\"Target_path\").strip()\n",
    "pk_raw = dbutils.widgets.get(\"pk_columns\").strip()\n",
    "col_list_raw = dbutils.widgets.get(\"column_list\").strip()\n",
    "table_name = dbutils.widgets.get(\"table_name\").strip()\n",
    "merge_flag = dbutils.widgets.get(\"merge_flag\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "direct_key = dbutils.widgets.get(\"direct_account_key\").strip()\n",
    "\n",
    "print(\"Dim loader params:\")\n",
    "print(\" Source:\", Source_path_raw)\n",
    "print(\" Target:\", Target_path)\n",
    "print(\" table_name:\", table_name)\n",
    "print(\" merge_flag:\", merge_flag)\n",
    "\n",
    "# --------- optional: configure storage key if passed ----------\n",
    "if direct_key:\n",
    "    k = direct_key.strip().strip('\"').strip(\"'\")\n",
    "    acct = None\n",
    "    for p in (Source_path_raw, Target_path):\n",
    "        if p and \"@\" in p:\n",
    "            try:\n",
    "                acct = p.split(\"@\",1)[1].split(\".\")[0]; break\n",
    "            except:\n",
    "                pass\n",
    "    if not acct:\n",
    "        acct = \"scrgvkrmade\"\n",
    "    spark.conf.set(f\"fs.azure.account.key.{acct}.dfs.core.windows.net\", k)\n",
    "    print(\"Configured storage key for\", acct)\n",
    "\n",
    "# --------- helpers to parse lists ----------\n",
    "def parse_list(txt):\n",
    "    if not txt:\n",
    "        return []\n",
    "    try:\n",
    "        parsed = json.loads(txt)\n",
    "        if isinstance(parsed, list):\n",
    "            return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "    except:\n",
    "        return [p.strip() for p in txt.split(\",\") if p.strip()]\n",
    "    return []\n",
    "\n",
    "pk_columns = parse_list(pk_raw)\n",
    "business_columns = parse_list(col_list_raw)\n",
    "\n",
    "# --------- read and union source paths ----------\n",
    "src_paths = [p.strip() for p in Source_path_raw.split(\";\") if p.strip()]\n",
    "if not src_paths:\n",
    "    raise RuntimeError(\"Provide Source_path (one or more paths separated by ';').\")\n",
    "\n",
    "df_list = []\n",
    "for p in src_paths:\n",
    "    try:\n",
    "        tmp = spark.read.format(\"delta\").load(p)\n",
    "    except Exception:\n",
    "        tmp = spark.read.parquet(p)\n",
    "    df_list.append(tmp)\n",
    "\n",
    "from functools import reduce\n",
    "df = reduce(lambda a,b: a.unionByName(b, allowMissingColumns=True), df_list)\n",
    "\n",
    "print(\"Source columns:\", df.columns)\n",
    "display(df.limit(5))\n",
    "\n",
    "# --------- cleaning: trim strings ----------\n",
    "string_cols = [c for c,t in df.dtypes if t == \"string\"]\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "# --------- select business columns (if provided) ----------\n",
    "keep_cols = []\n",
    "if business_columns:\n",
    "    keep_cols = [c for c in business_columns if c in df.columns]\n",
    "# ensure PKs present\n",
    "for k in pk_columns:\n",
    "    if k in df.columns and k not in keep_cols:\n",
    "        keep_cols.append(k)\n",
    "# always keep audit cols and business hash if present\n",
    "for a in [\"__ingest_ts\",\"__source_file\",\"__source_path\",\"__batch_id\",\"__row_hash\"]:\n",
    "    if a in df.columns and a not in keep_cols:\n",
    "        keep_cols.append(a)\n",
    "\n",
    "if keep_cols:\n",
    "    df = df.select(*keep_cols)\n",
    "\n",
    "print(\"Columns used for dim:\", df.columns)\n",
    "display(df.limit(5))\n",
    "\n",
    "# --------- compute business hash for change detection ----------\n",
    "# use business_columns if provided else all columns\n",
    "if business_columns:\n",
    "    hash_cols = [c for c in business_columns if c in df.columns]\n",
    "else:\n",
    "    exclude = set([\"__ingest_ts\",\"__source_path\",\"__batch_id\",\"__row_hash\"])\n",
    "    hash_cols = [c for c in df.columns if c not in exclude]\n",
    "\n",
    "if not hash_cols:\n",
    "    raise RuntimeError(\"No columns available for business hash.\")\n",
    "\n",
    "df = df.withColumn(\"__business_hash\", sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in hash_cols]), 256))\n",
    "print(\"Computed business hash on:\", hash_cols)\n",
    "display(df.limit(5))\n",
    "\n",
    "# --------- prepare merge key(s) ----------\n",
    "valid_pk = [p for p in pk_columns if p in df.columns]\n",
    "if not valid_pk:\n",
    "    print(\"No valid PK found, falling back to __business_hash for key\")\n",
    "    valid_pk = [\"__business_hash\"]\n",
    "print(\"Merge keys:\", valid_pk)\n",
    "\n",
    "# --------- create surrogate_key for incoming rows (simple approach) ----------\n",
    "# We'll add surrogate_key only when creating new rows (during initial create or insert).\n",
    "df = df.withColumn(\"_miid\", monotonically_increasing_id())\n",
    "win = Window.orderBy(col(\"_miid\"))\n",
    "df = df.withColumn(\"_rn\", row_number().over(win)).drop(\"_miid\")  # temporary row number\n",
    "\n",
    "# --------- does target exist? ----------\n",
    "target_exists = True\n",
    "try:\n",
    "    spark.read.format(\"delta\").load(Target_path)\n",
    "except Exception:\n",
    "    target_exists = False\n",
    "\n",
    "if not target_exists:\n",
    "    print(\"Target not found -> creating initial Gold dimension.\")\n",
    "    # assign surrogate keys starting at 1\n",
    "    df = df.withColumn(\"surrogate_key\", (col(\"_rn\")).cast(\"long\")).drop(\"_rn\")\n",
    "    df = df.withColumn(\"effective_from\", current_timestamp()).withColumn(\"effective_to\", lit(None).cast(\"timestamp\")).withColumn(\"current_flag\", lit(True))\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(Target_path)\n",
    "    print(\"Initial Gold dimension created at:\", Target_path)\n",
    "else:\n",
    "    if not merge_flag:\n",
    "        print(\"merge_flag=false -> overwrite target\")\n",
    "        out = df.withColumn(\"LoadTS\", current_timestamp()).drop(\"_rn\")\n",
    "        out.write.format(\"delta\").mode(\"overwrite\").save(Target_path)\n",
    "        print(\"Overwrite completed.\")\n",
    "    else:\n",
    "        # SCD2-ish simple approach: expire current rows where business hash changed, then insert new rows\n",
    "        print(\"merge_flag=true -> performing SCD2 steps (expire + insert).\")\n",
    "        dt = DeltaTable.forPath(spark, Target_path)\n",
    "        # expire: set current_flag=false where pk matches and business hash differs\n",
    "        match_on_pk = \" AND \".join([f\"target.`{p}` = source.`{p}`\" for p in valid_pk])\n",
    "        expire_cond = match_on_pk + \" AND target.current_flag = true AND target.__business_hash <> source.__business_hash\"\n",
    "        dt.alias(\"target\").merge(\n",
    "            df.alias(\"source\"),\n",
    "            expire_cond\n",
    "        ).whenMatchedUpdate(set = {\"current_flag\": lit(False), \"effective_to\": \"current_timestamp()\"}).execute()\n",
    "        print(\"Expired previous records where business changed.\")\n",
    "\n",
    "        # assign new surrogate keys to incoming rows\n",
    "        try:\n",
    "            max_sk_row = spark.read.format(\"delta\").load(Target_path).selectExpr(\"max(surrogate_key) as m\").collect()[0]\n",
    "            max_sk = int(max_sk_row[\"m\"]) if max_sk_row and max_sk_row[\"m\"] is not None else 0\n",
    "        except Exception:\n",
    "            max_sk = 0\n",
    "\n",
    "        new_rows = df.withColumn(\"_miid2\", monotonically_increasing_id())\n",
    "        w2 = Window.orderBy(col(\"_miid2\"))\n",
    "        new_rows = new_rows.withColumn(\"_rn2\", row_number().over(w2))\n",
    "        new_rows = new_rows.withColumn(\"surrogate_key\", (col(\"_rn2\") + lit(max_sk)).cast(\"long\")).drop(\"_miid2\",\"_rn2\",\"_rn\")\n",
    "        new_rows = new_rows.withColumn(\"effective_from\", current_timestamp()).withColumn(\"effective_to\", lit(None).cast(\"timestamp\")).withColumn(\"current_flag\", lit(True))\n",
    "\n",
    "        # insert new rows (only non-matching current rows)\n",
    "        tmp_view = \"_tmp_inserts_\" + uuid.uuid4().hex[:8]\n",
    "        new_rows.createOrReplaceTempView(tmp_view)\n",
    "        merge_sql = f\"\"\"\n",
    "            MERGE INTO delta.`{Target_path}` AS target\n",
    "            USING (SELECT * FROM {tmp_view}) AS source\n",
    "            ON {\" AND \".join([f\"target.`{p}` = source.`{p}`\" for p in valid_pk])} AND target.current_flag = true\n",
    "            WHEN NOT MATCHED THEN\n",
    "              INSERT ({', '.join([f'`{c}`' for c in new_rows.columns])})\n",
    "              VALUES ({', '.join([f'source.`{c}`' for c in new_rows.columns])})\n",
    "        \"\"\"\n",
    "        spark.sql(merge_sql)\n",
    "        print(\"Inserted new SCD rows.\")\n",
    "# final validation\n",
    "try:\n",
    "    tgt = spark.read.format(\"delta\").load(Target_path)\n",
    "    print(\"Gold rows approx:\", tgt.count())\n",
    "    display(tgt.limit(5))\n",
    "except Exception as e:\n",
    "    print(\"Could not read target for validation:\", e)\n",
    "\n",
    "print(\"Dim loader finished for\", table_name)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_SliverToGold",
   "widgets": {
    "Source_path": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_product_subcategory/",
     "nuid": "1175ba90-5601-4349-a540-a0fd81ce621a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Source_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Source_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_path": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/gold/dim/dim_employee/",
     "nuid": "4fce45b3-3d8d-41bc-86ab-ae09a0f58347",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "column_list": {
     "currentValue": "[\"EmployeeKey\",\"EmployeeAlternateKey\",\"EmployeeNationalIDAlternateKey\",\"JobTitle\",\"HireDate\",\"BirthDate\",\"MaritalStatus\",\"Gender\",\"SalariedFlag\",\"VacationHours\",\"SickLeaveHours\",\"CurrentFlag\",\"EffectiveDate\",\"EndDate\",\"IsCurrent\",\"SourcePath\",\"LoadTS\",\"RowHash\"]",
     "nuid": "68103b13-dee9-46aa-a36d-bd0f66314818",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "column_list",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "column_list",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "direct_account_key": {
     "currentValue": "E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==",
     "nuid": "75268211-7c2b-4894-99f0-a02abdfc21bc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "direct_account_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "direct_account_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "incremental_flag": {
     "currentValue": "false",
     "nuid": "bddb4bd3-f128-4fda-a12f-49760220f562",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "incremental_flag",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": null,
      "name": "incremental_flag",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "merge_flag": {
     "currentValue": "true",
     "nuid": "171a590c-a926-4013-9754-00ecf0b33030",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "true",
      "label": null,
      "name": "merge_flag",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "true",
      "label": null,
      "name": "merge_flag",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "pk_columns": {
     "currentValue": "EmployeeNationalIDAlternateKey",
     "nuid": "94c87bf8-3a07-4958-a383-49fdc9b58293",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "pk_columns",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "pk_columns",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "dbo.DimEmployee",
     "nuid": "ac22a007-a8e6-498b-b3fb-4c004914d7b7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "watermark": {
     "currentValue": "",
     "nuid": "b9d03c4f-8055-4925-87d6-e2e4749e362d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "watermark",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "watermark",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
