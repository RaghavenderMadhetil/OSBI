{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbc697a8-7302-4335-8804-fa101bd5b7c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **dim_currency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c96537-a6d1-4606-806a-4e798f11db54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, sha2, concat_ws, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- 1. Define Paths and Mappings ---\n",
    "\n",
    "# Source Path (Input)\n",
    "CURRENCY_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_currency/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/gold/dim/dim_currency/\"\n",
    "\n",
    "# Define the Primary Key column for matching existing rows\n",
    "PK_COL = \"CurrencyName\"\n",
    "\n",
    "# Columns needed for the Gold table\n",
    "# Note: Source columns are in stg_currency, Target columns are in dbo.DimCurrency\n",
    "CURRENCY_MAPPING = [\n",
    "    (\"CurrencyCode\", \"CurrencyAlternateKey\"), # Source PK becomes Target Alternate Key\n",
    "    (\"Name\", \"CurrencyName\"),\n",
    "    (\"ModifiedDate\", \"ModifiedDate\")\n",
    "]\n",
    "\n",
    "# Configure Authentication (use the DFS endpoint)\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "print(\"Configured Spark authentication.\")\n",
    "\n",
    "\n",
    "# --- 2. Read and Prepare Source Data (df_source) ---\n",
    "print(f\"\\nReading Currency data from: {CURRENCY_PATH}\")\n",
    "df_source = spark.read.format(\"delta\").load(CURRENCY_PATH)\n",
    "\n",
    "# Select and Rename columns\n",
    "selected_expr = [col(src).alias(tgt) for src, tgt in CURRENCY_MAPPING if src in df_source.columns]\n",
    "df_source = df_source.select(*selected_expr)\n",
    "\n",
    "# Create a hash for change detection (simplest way to see if data changed)\n",
    "# Use the core business columns for hashing\n",
    "hash_cols = [\"CurrencyAlternateKey\", \"CurrencyName\"]\n",
    "df_source = df_source.withColumn(\n",
    "    \"__row_hash\",\n",
    "    sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in hash_cols]), 256)\n",
    ")\n",
    "\n",
    "# Add required DimCurrency columns (placeholders)\n",
    "df_source = df_source.withColumn(\"LoadTS\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"IsCurrent\", lit(True))\n",
    "df_source = df_source.withColumn(\"CurrencyKey\", lit(None).cast(\"long\")) # Placeholder for surrogate key\n",
    "\n",
    "\n",
    "# --- 3. Check and Create Target Table (if needed) ---\n",
    "target_exists = DeltaTable.isDeltaTable(spark, TARGET_PATH)\n",
    "\n",
    "if not target_exists:\n",
    "    print(f\"Target table not found. Creating initial DimCurrency table at: {TARGET_PATH}\")\n",
    "    # We must include all required DimCurrency columns for the first write\n",
    "    final_cols = [\"CurrencyKey\", \"CurrencyAlternateKey\", \"CurrencyName\", \"ModifiedDate\",\"LoadTS\", \"__row_hash\", \"IsCurrent\"]\n",
    "    \n",
    "    # Select only the required columns and write the initial table\n",
    "    df_source.select(*final_cols).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(TARGET_PATH)\n",
    "    print(\"Initial DimCurrency table created. Skipping merge.\")\n",
    "\n",
    "else:\n",
    "    # --- 4. Perform MERGE (Incremental Upsert) ---\n",
    "    print(f\"\\nTarget table exists. Performing MERGE (Incremental Load)...\")\n",
    "    \n",
    "    dt_target = DeltaTable.forPath(spark, TARGET_PATH)\n",
    "    \n",
    "    # Define the condition to match an existing row (Primary Key)\n",
    "    join_cond = f\"target.{PK_COL} = source.{PK_COL}\"\n",
    "    \n",
    "    # Define the condition to detect a change (Row Hash)\n",
    "    change_cond = \"target.__row_hash != source.__row_hash\"\n",
    "\n",
    "    dt_target.alias(\"target\").merge(\n",
    "        df_source.alias(\"source\"),\n",
    "        join_cond\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=change_cond,\n",
    "        set = {\n",
    "            # Only update if the hash has changed\n",
    "            \"CurrencyName\": \"source.CurrencyName\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\"\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"CurrencyKey\": \"NULL\", # Key will be populated elsewhere or remains NULL\n",
    "            \"CurrencyAlternateKey\": \"source.CurrencyAlternateKey\",\n",
    "            \"CurrencyName\": \"source.CurrencyName\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\",\n",
    "            \"IsCurrent\": \"source.IsCurrent\"\n",
    "        }\n",
    "    ) \\\n",
    "    .execute()\n",
    "    \n",
    "    print(\"MERGE (Incremental Load) complete. ✅\")\n",
    "\n",
    "# --- 5. Final Validation (Optional) ---\n",
    "# Read the final Gold table to check row count\n",
    "# tgt = spark.read.format(\"delta\").load(TARGET_PATH)\n",
    "# print(f\"\\nFinal DimCurrency rows: {tgt.count()}\")\n",
    "# display(tgt.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aaae37b-59f6-4261-9d49-7ae394968f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read table root\n",
    "df = spark.read.format(\"delta\").load(\"wasbs://project@scrgvkrmade.blob.core.windows.net/gold/dim/dim_currency\")\n",
    "\n",
    "# read a specific partition (also allowed)\n",
    "df_part = spark.read.format(\"delta\").load(\n",
    "    \"wasbs://project@scrgvkrmade.blob.core.windows.net/gold/dim/dim_currency\"\n",
    ")\n",
    "display(df_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b4d5c4b-be9e-4e1d-803a-8cb3c114294a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **dim_employee**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8d54df-dc53-46c2-ab16-6476add18742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# KEY for ABFSS (dfs endpoint) - needed for writing Delta\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.scrgvkrmade.dfs.core.windows.net\",\n",
    "    \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    ")\n",
    "\n",
    "# Read from BLOB (works)\n",
    "df = spark.read.parquet(\n",
    "    \"wasbs://project@scrgvkrmade.blob.core.windows.net/bronze/ResellerSales/HumanResources.Employee/*/part-*.parquet\"\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91562415-11ac-4a5f-8904-1cbecf36f0f1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765135029609}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read table root\n",
    "df = spark.read.format(\"delta\").load(\"wasbs://project@scrgvkrmade.blob.core.windows.net/silver/stg_employee\")\n",
    "\n",
    "# read a specific partition (also allowed)\n",
    "df_part = spark.read.format(\"delta\").load(\n",
    "    \"wasbs://project@scrgvkrmade.blob.core.windows.net/silver/stg_employee\"\n",
    ")\n",
    "display(df_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6cf530-a7e9-43a4-aa52-1f5579d32ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, sha2, concat_ws, current_timestamp, year, to_timestamp, coalesce\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- 1. Define Paths and Mappings ---\n",
    "\n",
    "# Source Path (Input)\n",
    "EMPLOYEE_SOURCE_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/stg_employee/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/gold/dim/dim_employee/\"\n",
    "\n",
    "# Define the Primary Key column for matching existing rows (using the Alternate Key)\n",
    "PK_COL = \"EmployeeNationalIDAlternateKey\" # Target Column Name\n",
    "\n",
    "# Columns needed for the Gold table (Source Column, Target Column)\n",
    "# NOTE: We map BusinessEntityID to ParentEmployeeNationalIDAlternateKey as we don't have\n",
    "# parent data in this simple source. We also use placeholders for missing columns.\n",
    "EMPLOYEE_MAPPING = [\n",
    "    # Alternate Keys (for matching)\n",
    "    (\"NationalIDNumber\", \"EmployeeNationalIDAlternateKey\"),\n",
    "    (\"BusinessEntityID\", \"ParentEmployeeNationalIDAlternateKey\"), \n",
    "    \n",
    "    # Core Attributes (use placeholders/NULLs for missing columns)\n",
    "    # Placeholder for SalesTerritoryKey (usually joined from another table)\n",
    "    # (Missing from source: FirstName, LastName, MiddleName, NameStyle, Title, Phone, etc.)\n",
    "    (\"JobTitle\", \"Title\"),\n",
    "    (\"HireDate\", \"HireDate\"),\n",
    "    (\"BirthDate\", \"BirthDate\"),\n",
    "    (\"LoginID\", \"LoginID\"),\n",
    "    (\"MaritalStatus\", \"MaritalStatus\"),\n",
    "    (\"SalariedFlag\", \"SalariedFlag\"),\n",
    "    (\"Gender\", \"Gender\"),\n",
    "    (\"VacationHours\", \"VacationHours\"),\n",
    "    (\"SickLeaveHours\", \"SickLeaveHours\"),\n",
    "    (\"CurrentFlag\", \"CurrentFlag\"),\n",
    "    (\"ModifiedDate\", \"ModifiedDate\"),\n",
    "    # Audit Columns\n",
    "     (\"_ingest_ts\", \"_ingest_ts\"),\n",
    "    (\"__source_path\", \"__source_path\")\n",
    "]\n",
    "\n",
    "# Configure Authentication (use the DFS endpoint)\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "print(\"Configured Spark authentication.\")\n",
    "\n",
    "\n",
    "# --- 2. Read and Prepare Source Data (df_source) ---\n",
    "print(f\"\\nReading Currency data from: {EMPLOYEE_SOURCE_PATH}\")\n",
    "df_source = spark.read.format(\"delta\").load(EMPLOYEE_SOURCE_PATH)\n",
    "\n",
    "\n",
    "# Select and Rename columns\n",
    "selected_expr = [col(src).alias(tgt) for src, tgt in EMPLOYEE_MAPPING if src in df_source.columns]\n",
    "df_source = df_source.select(*selected_expr)\n",
    "\n",
    "# Add placeholder columns for missing attributes in DimEmployee\n",
    "# This ensures the target schema is complete\n",
    "df_source = df_source.withColumn(\"EmployeeKey\", lit(None).cast(\"long\"))\n",
    "df_source = df_source.withColumn(\"ParentEmployeeKey\", lit(None).cast(\"long\"))\n",
    "df_source = df_source.withColumn(\"SalesTerritoryKey\", lit(-1).cast(\"integer\"))\n",
    "df_source = df_source.withColumn(\"FirstName\", lit(\"N/A\"))\n",
    "df_source = df_source.withColumn(\"LastName\", lit(\"N/A\"))\n",
    "df_source = df_source.withColumn(\"MiddleName\", lit(None).cast(\"string\"))\n",
    "df_source = df_source.withColumn(\"NameStyle\", lit(0).cast(\"integer\")) # 0 for Western, 1 for Eastern\n",
    "df_source = df_source.withColumn(\"EmailAddress\", lit(None).cast(\"string\"))\n",
    "df_source = df_source.withColumn(\"Phone\", lit(None).cast(\"string\"))\n",
    "df_source = df_source.withColumn(\"EmergencyContactName\", lit(\"N/A\"))\n",
    "df_source = df_source.withColumn(\"EmergencyContactPhone\", lit(\"N/A\"))\n",
    "df_source = df_source.withColumn(\"PayFrequency\", lit(1).cast(\"integer\")) # Placeholder\n",
    "df_source = df_source.withColumn(\"BaseRate\", lit(0.0).cast(\"decimal(18,4)\"))\n",
    "df_source = df_source.withColumn(\"SalesPersonFlag\", lit(False))\n",
    "df_source = df_source.withColumn(\"DepartmentName\", lit(None).cast(\"string\"))\n",
    "df_source = df_source.withColumn(\"StartDate\", to_timestamp(col(\"HireDate\")))\n",
    "df_source = df_source.withColumn(\"EndDate\", lit(None).cast(\"timestamp\"))\n",
    "df_source = df_source.withColumn(\"Status\", lit(\"Current\"))\n",
    "df_source = df_source.withColumn(\"EmployeePhoto\", lit(None).cast(\"binary\"))\n",
    "df_source = df_source.withColumn(\"ModifiedDate\", lit(None).cast(\"timestamp\"))\n",
    "df_source = df_source.withColumn(\"_ingest_ts\", lit(None).cast(\"timestamp\"))\n",
    "df_source = df_source.withColumn(\"__source_path\", lit(None).cast(\"string\"))\n",
    "\n",
    "# Create a hash for change detection (using core business keys and mutable attributes)\n",
    "hash_cols = [\n",
    "    \"EmployeeNationalIDAlternateKey\", \"Title\", \"MaritalStatus\", \n",
    "    \"Gender\", \"SalariedFlag\", \"VacationHours\", \"SickLeaveHours\"\n",
    "]\n",
    "df_source = df_source.withColumn(\n",
    "    \"__row_hash\",\n",
    "    sha2(concat_ws(\"||\", *[coalesce(col(c).cast(\"string\"), lit(\"\")) for c in hash_cols]), 256)\n",
    ")\n",
    "\n",
    "# Add audit columns\n",
    "df_source = df_source.withColumn(\"LoadTS\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"IsCurrent\", lit(True)) # Using IsCurrent for SDC Type 1/Type 2 flag\n",
    "df_source = df_source.withColumn(\"_year\", year(to_timestamp(col(\"HireDate\")))) # For partitioning\n",
    "\n",
    "\n",
    "# --- 3. Check and Create Target Table (if needed) ---\n",
    "target_exists = DeltaTable.isDeltaTable(spark, TARGET_PATH)\n",
    "\n",
    "# List of ALL columns in the target DimEmployee table\n",
    "GOLD_COLUMNS = [\n",
    "    \"EmployeeKey\", \"ParentEmployeeKey\", \"EmployeeNationalIDAlternateKey\", \"ParentEmployeeNationalIDAlternateKey\", \n",
    "    \"SalesTerritoryKey\", \"FirstName\", \"LastName\", \"MiddleName\", \"NameStyle\", \"Title\", \"HireDate\", \n",
    "    \"BirthDate\", \"LoginID\", \"EmailAddress\", \"Phone\", \"MaritalStatus\", \"EmergencyContactName\", \n",
    "    \"EmergencyContactPhone\", \"SalariedFlag\", \"Gender\", \"PayFrequency\", \"BaseRate\", \"VacationHours\", \n",
    "    \"SickLeaveHours\", \"CurrentFlag\", \"SalesPersonFlag\", \"DepartmentName\", \"StartDate\", \"EndDate\", \n",
    "    \"Status\", \"EmployeePhoto\", \"ModifiedDate\", \"LoadTS\", \"__row_hash\", \"IsCurrent\", \"_year\"\n",
    "]\n",
    "\n",
    "\n",
    "if not target_exists:\n",
    "    print(f\"Target table not found. Creating initial DimEmployee table at: {TARGET_PATH}\")\n",
    "    \n",
    "    # Select only the required columns and write the initial table\n",
    "    df_source.select(*GOLD_COLUMNS).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"_year\") \\\n",
    "        .save(TARGET_PATH)\n",
    "    print(\"Initial DimEmployee table created. Skipping merge.\")\n",
    "\n",
    "else:\n",
    "    # --- 4. Perform MERGE (Incremental Upsert) ---\n",
    "    print(f\"\\nTarget table exists. Performing MERGE (Incremental Load)...\")\n",
    "    \n",
    "    dt_target = DeltaTable.forPath(spark, TARGET_PATH)\n",
    "    \n",
    "    # Define the condition to match an existing row (Alternate Key)\n",
    "    join_cond = f\"target.{PK_COL} = source.{PK_COL}\"\n",
    "    \n",
    "    # Define the condition to detect a change (Row Hash)\n",
    "    change_cond = \"target.__row_hash != source.__row_hash\"\n",
    "\n",
    "    # Create the dictionary for all columns to update\n",
    "    update_set = {col_name: f\"source.{col_name}\" for col_name in GOLD_COLUMNS if col_name not in [\"EmployeeNationalIDAlternateKey\", \"_year\", \"IsCurrent\"]} # Don't update SK, IsCurrent, or partition year\n",
    "    \n",
    "    # Create the dictionary for all columns to insert\n",
    "    insert_values = {col_name: f\"source.{col_name}\" for col_name in GOLD_COLUMNS}\n",
    "\n",
    "\n",
    "    dt_target.alias(\"target\").merge(\n",
    "        df_source.alias(\"source\"),\n",
    "        join_cond\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=change_cond,\n",
    "        set = update_set\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values = insert_values\n",
    "    ) \\\n",
    "    .execute()\n",
    "    \n",
    "    print(\"MERGE (Incremental Load) complete. ✅\")\n",
    "\n",
    "print(\"DimEmployee job finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f6d2f6f-92ba-45b6-8203-d900a9c33664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read table root\n",
    "df = spark.read.format(\"delta\").load(\"wasbs://project@scrgvkrmade.blob.core.windows.net/gold/dim/dim_currency\")\n",
    "\n",
    "# read a specific partition (also allowed)\n",
    "df_part = spark.read.format(\"delta\").load(\n",
    "    \"wasbs://project@scrgvkrmade.blob.core.windows.net/gold/dim/dim_currency\"\n",
    ")\n",
    "display(df_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f90f5e-1236-40a5-b64c-9ee8a18ffc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read table root\n",
    "df = spark.read.format(\"delta\").load(\"wasbs://project@scrgvkrmade.blob.core.windows.net/gold/dim/dim_employee\")\n",
    "\n",
    "# read a specific partition (also allowed)\n",
    "df_part = spark.read.format(\"delta\").load(\n",
    "    \"wasbs://project@scrgvkrmade.blob.core.windows.net/gold/dim/dim_employee\"\n",
    ")\n",
    "display(df_part)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_SliverToGold_Dim_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
