{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44d5e2c-bbd8-4b72-b0b6-ee589d7a69b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RUN ONCE: create utils_etl.py\n",
    "module_code = r'''\n",
    "# utils_etl.py\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import trim, col, year, to_timestamp, coalesce, lit\n",
    "import datetime, json\n",
    "\n",
    "def read_sql_metadata(jdbc_url: str, jdbc_user: str, jdbc_pass: str, table_name: str = \"dbo.sql_table_metadata\") -> DataFrame:\n",
    "    df = (spark.read.format(\"jdbc\")\n",
    "            .option(\"url\", jdbc_url)\n",
    "            .option(\"dbtable\", table_name)\n",
    "            .option(\"user\", jdbc_user)\n",
    "            .option(\"password\", jdbc_pass)\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "            .load())\n",
    "    return df\n",
    "\n",
    "def _normalize_columns_from_row(row_dict: dict) -> list:\n",
    "    if row_dict.get(\"column_list\"):\n",
    "        cols = [c.strip() for c in row_dict.get(\"column_list\").split(\",\") if c.strip() != \"\"]\n",
    "        if cols:\n",
    "            return cols\n",
    "    if row_dict.get(\"column_names_json\"):\n",
    "        try:\n",
    "            parsed = json.loads(row_dict.get(\"column_names_json\"))\n",
    "            if isinstance(parsed, list):\n",
    "                if all(isinstance(x, str) for x in parsed):\n",
    "                    return [x.strip() for x in parsed if x and str(x).strip() != \"\"]\n",
    "                else:\n",
    "                    names = []\n",
    "                    for el in parsed:\n",
    "                        if isinstance(el, dict) and el.get(\"name\"):\n",
    "                            names.append(str(el.get(\"name\")).strip())\n",
    "                    if names:\n",
    "                        return names\n",
    "        except Exception:\n",
    "            pass\n",
    "    if row_dict.get(\"column_defs_json\"):\n",
    "        try:\n",
    "            parsed = json.loads(row_dict.get(\"column_defs_json\"))\n",
    "            if isinstance(parsed, list):\n",
    "                names = []\n",
    "                for el in parsed:\n",
    "                    if isinstance(el, dict):\n",
    "                        for key in (\"name\",\"column_name\",\"col_name\"):\n",
    "                            if el.get(key):\n",
    "                                names.append(str(el.get(key)).strip())\n",
    "                                break\n",
    "                if names:\n",
    "                    return names\n",
    "        except Exception:\n",
    "            pass\n",
    "    return []\n",
    "\n",
    "def metadata_to_map(meta_df: DataFrame) -> dict:\n",
    "    rows = meta_df.collect()\n",
    "    m = {}\n",
    "    for r in rows:\n",
    "        d = r.asDict()\n",
    "        table_name = d.get(\"table_name\") or d.get(\"table\")\n",
    "        file_name = d.get(\"file_name\") or d.get(\"filename\") or None\n",
    "        cols = _normalize_columns_from_row(d)\n",
    "        year_col = d.get(\"year_column\") or d.get(\"yearcol\") or d.get(\"date_column\") or None\n",
    "        logical_table = d.get(\"table_name\") or d.get(\"table\") or (file_name.split(\".\")[0] if file_name else (table_name if table_name else None))\n",
    "        meta = {\"columns\": cols, \"year_column\": year_col, \"table_name\": logical_table}\n",
    "        if table_name:\n",
    "            m[str(table_name).strip()] = meta\n",
    "        if file_name:\n",
    "            m[str(file_name).strip()] = meta\n",
    "            if isinstance(file_name, str) and \".\" in file_name:\n",
    "                m[str(file_name).split(\".\")[0].strip()] = meta\n",
    "    return m\n",
    "\n",
    "def add_headers(df: DataFrame, columns: list) -> DataFrame:\n",
    "    new_names = []\n",
    "    for i, old in enumerate(df.columns):\n",
    "        if i < len(columns):\n",
    "            new_names.append(columns[i])\n",
    "        else:\n",
    "            new_names.append(f\"_c{i}\")\n",
    "    return df.toDF(*new_names)\n",
    "\n",
    "def extract_year_column(df: DataFrame, year_column_hint: str = None):\n",
    "    current_year = datetime.datetime.utcnow().year\n",
    "    cols = df.columns\n",
    "    used = None\n",
    "    df2 = df\n",
    "    if year_column_hint and year_column_hint in cols:\n",
    "        used = year_column_hint\n",
    "        try:\n",
    "            df2 = df2.withColumn(\"_year\", year(to_timestamp(col(used))))\n",
    "        except Exception:\n",
    "            df2 = df2.withColumn(\"_year\", col(used).cast(\"int\"))\n",
    "    else:\n",
    "        candidates = [c for c in cols if \"modified\" in c.lower() or \"year\" in c.lower() or \"date\" in c.lower()]\n",
    "        if candidates:\n",
    "            used = candidates[0]\n",
    "            try:\n",
    "                df2 = df2.withColumn(\"_year\", year(to_timestamp(col(used))))\n",
    "            except Exception:\n",
    "                df2 = df2.withColumn(\"_year\", col(used).cast(\"int\"))\n",
    "        else:\n",
    "            df2 = df2.withColumn(\"_year\", lit(current_year))\n",
    "    df2 = df2.withColumn(\"_year\", coalesce(col(\"_year\").cast(\"int\"), lit(current_year)))\n",
    "    return df2, used\n",
    "\n",
    "def write_parquet_by_year(df_with_year: DataFrame, bronze_base_path: str, table_name: str,\n",
    "                          compression: str = \"snappy\", coalesce_out: bool = True, write_mode: str = \"overwrite\"):\n",
    "    years = [r[\"_year\"] for r in df_with_year.select(\"_year\").distinct().collect()]\n",
    "    for y in years:\n",
    "        out_path = f\"{bronze_base_path.rstrip('/')}/{table_name}/{y}\"\n",
    "        df_year = df_with_year.filter(col(\"_year\") == y).drop(\"_year\")\n",
    "        writer = df_year.coalesce(1) if coalesce_out else df_year\n",
    "        writer.write.mode(write_mode).option(\"compression\", compression).parquet(out_path)\n",
    "        print(f\"Wrote parquet to: {out_path}\")\n",
    "'''\n",
    "dbutils.fs.put(\"/FileStore/utils_etl.py\", module_code, True)\n",
    "print(\"utils_etl.py written to /FileStore\")\n",
    "display(dbutils.fs.ls(\"/FileStore\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils_etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
