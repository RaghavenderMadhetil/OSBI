{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922e2020-b73b-4f50-a9ee-4ef7b1f606a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. SETUP: Import the necessary tools ðŸ› ï¸\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, trim, to_timestamp, coalesce, sha2, \n",
    "    concat_ws, year, when, regexp_replace\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "import datetime\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# 1a. Define File Locations (Paths)\n",
    "SOURCE_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Production.Product/\" # Changed path to Product source for clarity\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/dim/dim_product/\"\n",
    "\n",
    "# 1b. Define the Unique Identifier (Primary Key) - REFLECTS NEW MAPPING\n",
    "# Keys must be: ProductKey (Source ID), ProductAlternateKey (Product Number), ProductSubcategoryKey\n",
    "PK_RAW = \"ProductKey,ProductAlternateKey,ProductSubcategoryKey\"\n",
    "PRIMARY_KEYS = [c.strip() for c in PK_RAW.split(\",\") if c.strip()]\n",
    "\n",
    "# 1c. Define Column Renaming and Mapping (Bronze to Silver)\n",
    "COLUMN_MAP = {\n",
    "    \"ProductID\": \"ProductKey\", # Source ID -> Surrogate Key (New Mapping)\n",
    "    \"ProductSubcategoryID\": \"ProductSubcategoryKey\", \n",
    "    \"WeightUnitMeasureCode\": \"WeightUnitMeasureCode\",\n",
    "    \"SizeUnitMeasureCode\": \"SizeUnitMeasureCode\",\n",
    "    \"Name\": \"EnglishProductName\", \n",
    "    \"ProductNumber\": \"ProductAlternateKey\", # Product Number -> Alternate Key (New Mapping)\n",
    "    \"StandardCost\": \"StandardCost\",\n",
    "    \"FinishedGoodsFlag\": \"FinishedGoodsFlag\",\n",
    "    \"Color\": \"Color\",\n",
    "    \"SafetyStockLevel\": \"SafetyStockLevel\",\n",
    "    \"ReorderPoint\": \"ReorderPoint\",\n",
    "    \"ListPrice\": \"ListPrice\",\n",
    "    \"Size\": \"Size\",\n",
    "    \"Weight\": \"Weight\",\n",
    "    \"DaysToManufacture\": \"DaysToManufacture\",\n",
    "    \"ProductLine\": \"ProductLine\",\n",
    "    \"Class\": \"Class\",\n",
    "    \"Style\": \"Style\",\n",
    "    \"SellStartDate\": \"StartDate\",\n",
    "    \"SellEndDate\": \"EndDate\",\n",
    "    \"ModifiedDate\": \"ModifiedDate\"\n",
    "}\n",
    "\n",
    "# 1d. Setup Storage Access (Authentication)\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", account_key)\n",
    "PK_COLS = PRIMARY_KEYS\n",
    "print(f\"Using Primary Key for MERGE: {PK_COLS}\")\n",
    "\n",
    "\n",
    "# --- 2. READ AND CLEAN THE DATA ---\n",
    "df_source = (spark.read\n",
    "                  .option(\"mergeSchema\", \"true\")\n",
    "                  .option(\"recursiveFileLookup\", \"true\")\n",
    "                  .parquet(SOURCE_PATH))\n",
    "\n",
    "# 2a. Rename Columns\n",
    "for source_col, target_col in COLUMN_MAP.items():\n",
    "    if source_col in df_source.columns:\n",
    "        df_source = df_source.withColumnRenamed(source_col, target_col)\n",
    "        \n",
    "# 2b. Data Cleaning Steps\n",
    "print(\"2. Cleaning and preparing data...\")\n",
    "for c in [\"EnglishProductName\", \"Color\", \"Size\", \"ProductLine\", \"Class\", \"Style\"]:\n",
    "    if c in df_source.columns:\n",
    "        df_source = df_source.withColumn(c, trim(coalesce(col(c).cast(\"string\"), lit(\"N/A\"))))\n",
    "\n",
    "df_source = df_source.withColumn(\"StartDate\", to_timestamp(col(\"StartDate\")))\n",
    "df_source = df_source.withColumn(\"EndDate\", to_timestamp(col(\"EndDate\")))\n",
    "df_source = df_source.withColumn(\"ModifiedDate\", to_timestamp(col(\"ModifiedDate\")))\n",
    "df_source = df_source.withColumn(\"StandardCost\", col(\"StandardCost\").cast(\"decimal(19,4)\"))\n",
    "df_source = df_source.withColumn(\"ListPrice\", col(\"ListPrice\").cast(\"decimal(19,4)\"))\n",
    "df_source = df_source.withColumn(\"Weight\", col(\"Weight\").cast(\"decimal(8,2)\"))\n",
    "df_source = df_source.dropDuplicates(PK_COLS)\n",
    "\n",
    "\n",
    "# --- 3. ADD METADATA AND DERIVED COLUMNS ---\n",
    "print(\"3. Adding metadata and required Silver columns...\")\n",
    "\n",
    "# 3a. Audit Columns and Hash\n",
    "df_source = df_source.withColumn(\"LoadTS\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"_year\", year(col(\"ModifiedDate\")))\n",
    "# FIX: Updated hash_cols to use ProductAlternateKey instead of ProductNumber\n",
    "hash_cols = [\"ProductKey\", \"ProductAlternateKey\", \"ProductSubcategoryKey\", \"StandardCost\", \"ListPrice\", \"FinishedGoodsFlag\", \"Color\", \"Weight\", \"Size\"]\n",
    "df_source = df_source.withColumn(\n",
    "    \"__row_hash\",\n",
    "    sha2(concat_ws(\"||\", *[coalesce(col(c).cast(\"string\"), lit(\"\")) for c in hash_cols]), 256)\n",
    ")\n",
    "df_source = df_source.withColumn(\"__ingest_ts\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"__source_path\", lit(SOURCE_PATH)) \n",
    "df_source = df_source.withColumn(\"__target_path\", lit(TARGET_PATH)) \n",
    "df_source = df_source.withColumn(\"__batch_id\", lit(\"Batch-\" + datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')))\n",
    "\n",
    "# 3b. Key and Placeholder Columns\n",
    "# FIX: Removed the line setting ProductKey to NULL since it's now mapped from ProductID\n",
    "df_source = df_source.withColumn(\"Status\", when(col(\"EndDate\").isNull(), lit(\"Current\")).otherwise(lit(\"Discontinued\")))\n",
    "df_source = df_source.withColumn(\"SizeRange\", lit(None).cast(\"string\")) \n",
    "df_source = df_source.withColumn(\"DealerPrice\", lit(None).cast(\"decimal(19,4)\")) \n",
    "df_source = df_source.withColumn(\"ModelName\", lit(None).cast(\"string\")) \n",
    "df_source = df_source.withColumn(\"IsCurrent\", lit(True)) \n",
    "df_source = df_source.withColumn(\"SpanishProductName\", lit(None).cast(\"string\"))\n",
    "df_source = df_source.withColumn(\"FrenchProductName\", lit(None).cast(\"string\"))\n",
    "df_source = df_source.withColumn(\"LargePhoto\", lit(None).cast(\"string\"))\n",
    "df_source = df_source.withColumn(\"EnglishDescription\", lit(None).cast(\"string\"))\n",
    "\n",
    "\n",
    "# --- 4. LOAD INTO DELTA LAKE USING MERGE ---\n",
    "\n",
    "# 4a. EXPLICIT ALL_TARGET_COLS List\n",
    "ALL_TARGET_COLS = [\n",
    "    # Keys\n",
    "    \"ProductKey\", \"ProductAlternateKey\", \"ProductSubcategoryKey\",\n",
    "    \n",
    "    # Mapped Data\n",
    "    \"WeightUnitMeasureCode\", \"SizeUnitMeasureCode\", \"EnglishProductName\", \"StandardCost\", \n",
    "    \"FinishedGoodsFlag\", \"Color\", \"SafetyStockLevel\", \"ReorderPoint\", \"ListPrice\", \n",
    "    \"Size\", \"Weight\", \"DaysToManufacture\", \"ProductLine\", \"Class\", \"Style\", \n",
    "    \n",
    "    # Derived/Placeholder (NULL) Columns\n",
    "    \"SpanishProductName\", \"FrenchProductName\", \"SizeRange\", \"DealerPrice\", \"ModelName\", \n",
    "    \"LargePhoto\", \"EnglishDescription\", \n",
    "    \n",
    "    # SCD/Status/Date Columns\n",
    "    \"StartDate\", \"EndDate\", \"Status\", \"ModifiedDate\", \"IsCurrent\",\n",
    "    \n",
    "    # Audit Columns\n",
    "    \"LoadTS\", \"__row_hash\", \"_year\", \"__ingest_ts\", \"__source_path\", \n",
    "    \"__target_path\", \"__batch_id\"\n",
    "]\n",
    "\n",
    "target_exists = DeltaTable.isDeltaTable(spark, TARGET_PATH)\n",
    "\n",
    "if not target_exists:\n",
    "    print(f\"\\n4. Target table not found. Creating initial table at: {TARGET_PATH}\")\n",
    "    \n",
    "    # Initial Write: Select only the required columns and save\n",
    "    df_source.select(*[c for c in ALL_TARGET_COLS if c in df_source.columns]).write \\\n",
    "        .format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(\"_year\").save(TARGET_PATH)\n",
    "    print(\"Initial table created. âœ…\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n4. Target table exists. Performing Incremental MERGE...\")\n",
    "    dt_target = DeltaTable.forPath(spark, TARGET_PATH)\n",
    "    \n",
    "    # join_cond is now correct: target.ProductKey, target.ProductAlternateKey, target.ProductSubcategoryKey\n",
    "    join_cond = \" AND \".join([f\"target.{c} = source.{c}\" for c in PK_COLS])\n",
    "    change_cond = \"target.__row_hash != source.__row_hash\"\n",
    "    \n",
    "    dt_target.alias(\"target\").merge(df_source.alias(\"source\"), join_cond) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=change_cond,\n",
    "        set = {\n",
    "            \"StandardCost\": \"source.StandardCost\", \n",
    "            \"ListPrice\": \"source.ListPrice\",\n",
    "            \"Color\": \"source.Color\",\n",
    "            \"Weight\": \"source.Weight\",\n",
    "            \"Size\": \"source.Size\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            \"Status\": \"source.Status\",\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\",\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        # --- EXPLICIT INSERT VALUES FOR PRODUCT ---\n",
    "        values = {\n",
    "            \"ProductKey\": \"source.ProductKey\",\n",
    "            \"ProductAlternateKey\": \"source.ProductAlternateKey\",\n",
    "            \"ProductSubcategoryKey\": \"source.ProductSubcategoryKey\",\n",
    "            \"WeightUnitMeasureCode\": \"source.WeightUnitMeasureCode\",\n",
    "            \"SizeUnitMeasureCode\": \"source.SizeUnitMeasureCode\",\n",
    "            \"EnglishProductName\": \"source.EnglishProductName\",\n",
    "            \"StandardCost\": \"source.StandardCost\",\n",
    "            \"FinishedGoodsFlag\": \"source.FinishedGoodsFlag\",\n",
    "            \"Color\": \"source.Color\",\n",
    "            \"SafetyStockLevel\": \"source.SafetyStockLevel\",\n",
    "            \"ReorderPoint\": \"source.ReorderPoint\",\n",
    "            \"ListPrice\": \"source.ListPrice\",\n",
    "            \"Size\": \"source.Size\",\n",
    "            \"Weight\": \"source.Weight\",\n",
    "            \"DaysToManufacture\": \"source.DaysToManufacture\",\n",
    "            \"ProductLine\": \"source.ProductLine\",\n",
    "            \"Class\": \"source.Class\",\n",
    "            \"Style\": \"source.Style\",\n",
    "            \"StartDate\": \"source.StartDate\",\n",
    "            \"EndDate\": \"source.EndDate\",\n",
    "            \"Status\": \"source.Status\",\n",
    "            \"IsCurrent\": \"source.IsCurrent\",\n",
    "            \"SpanishProductName\": \"source.SpanishProductName\", \n",
    "            \"FrenchProductName\": \"source.FrenchProductName\", \n",
    "            \"SizeRange\": \"source.SizeRange\", \n",
    "            \"DealerPrice\": \"source.DealerPrice\", \n",
    "            \"ModelName\": \"source.ModelName\", \n",
    "            \"LargePhoto\": \"source.LargePhoto\", \n",
    "            \"EnglishDescription\": \"source.EnglishDescription\", \n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\",\n",
    "            \"_year\": \"source._year\",\n",
    "            \"__ingest_ts\": \"source.__ingest_ts\",\n",
    "            \"__source_path\": \"source.__source_path\",\n",
    "            \"__target_path\": \"source.__target_path\",\n",
    "            \"__batch_id\": \"source.__batch_id\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\"\n",
    "        }\n",
    "    ) \\\n",
    "    .execute()\n",
    "    \n",
    "    print(\"MERGE (Incremental Load) complete. âœ…\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_BronzeToSliver_Merge_Product",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
