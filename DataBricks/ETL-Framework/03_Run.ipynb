{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a3c83d9-15e1-4283-9693-1be91bedd00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 03_Run - ETL runner (end-to-end)\n",
    "# ==============================\n",
    "# Requirements:\n",
    "# - 01_Config must run first in the same Databricks job to set spark.conf and taskValues\n",
    "# - 02_Utils notebook must be in same folder and will be loaded with %run\n",
    "#\n",
    "# This notebook:\n",
    "#  - reads BASE_RAW_PATH / BASE_BRONZE_PATH and FILE_NAME from taskValues set by 01_Config\n",
    "#  - reads metadata from Azure SQL (jdbc widget inputs)\n",
    "#  - reads CSV (no header), applies header, extracts year, writes parquet to bronze/<domain>/<layer>/<table>/<year>/\n",
    "# ==============================\n",
    "#/Users/u2786997@uel.ac.uk/OSBI/DataBricks/ETL-Framework/02_Utils\n",
    "\n",
    "#Path (/Users/u2786997@uel.ac.uk/OSBI/DataBricks/ETL-Framework/02_Utils.py) doesn't exist.\n",
    "# 0) include helper functions from 02_Utils (make sure 02_Utils is in same folder)\n",
    "# Use %run to load functions into this notebook's namespace\n",
    "try:\n",
    "    # relative run â€” works when notebooks are in the same folder\n",
    "    %run /Users/u2786997@uel.ac.uk/OSBI/DataBricks/ETL-Framework/02_Utils\n",
    "    #%run ./02_Utils\n",
    "except Exception as e:\n",
    "    # If %run fails, provide a clearer error\n",
    "    raise Exception(\"Could not load 02_Utils. Ensure a notebook named '02_Utils' exists in the same folder.\") from e\n",
    "\n",
    "# 1) Widgets for JDBC (ADF should pass these)\n",
    "dbutils.widgets.text(\"jdbc_hostname\", \"\")   # ex: sqlsrv-rm-dev.database.windows.net\n",
    "dbutils.widgets.text(\"jdbc_database\", \"\")   # ex: sqldb-rm-dev\n",
    "dbutils.widgets.text(\"jdbc_user\", \"\")       # ex: sqladminuser@sqlsrv-rm-dev\n",
    "dbutils.widgets.text(\"jdbc_password\", \"\")   # pass the password (securely via ADF)\n",
    "\n",
    "JDBC_HOST = dbutils.widgets.get(\"jdbc_hostname\").strip()\n",
    "JDBC_DB = dbutils.widgets.get(\"jdbc_database\").strip()\n",
    "JDBC_USER = dbutils.widgets.get(\"jdbc_user\").strip()\n",
    "JDBC_PASS = dbutils.widgets.get(\"jdbc_password\").strip()\n",
    "\n",
    "# 2) Read values set by 01_Config (taskValues)\n",
    "try:\n",
    "    BASE_RAW_PATH = dbutils.jobs.taskValues.get(taskKey=\"BASE_RAW_PATH\", key=\"BASE_RAW_PATH\")\n",
    "    BASE_BRONZE_PATH = dbutils.jobs.taskValues.get(taskKey=\"BASE_BRONZE_PATH\", key=\"BASE_BRONZE_PATH\")\n",
    "    DOMAIN = dbutils.jobs.taskValues.get(taskKey=\"DOMAIN\", key=\"DOMAIN\")\n",
    "    LAYER = dbutils.jobs.taskValues.get(taskKey=\"LAYER\", key=\"LAYER\")\n",
    "    FILE_NAME = dbutils.jobs.taskValues.get(taskKey=\"FILE_NAME\", key=\"FILE_NAME\")\n",
    "except Exception:\n",
    "    # fallback - try widgets (if running interactively)\n",
    "    BASE_RAW_PATH = dbutils.widgets.get(\"BASE_RAW_PATH\") if \"BASE_RAW_PATH\" in dbutils.widgets.getArgumentNames() else None\n",
    "    BASE_BRONZE_PATH = dbutils.widgets.get(\"BASE_BRONZE_PATH\") if \"BASE_BRONZE_PATH\" in dbutils.widgets.getArgumentNames() else None\n",
    "    DOMAIN = dbutils.widgets.get(\"domain\") if \"domain\" in dbutils.widgets.getArgumentNames() else None\n",
    "    LAYER = dbutils.widgets.get(\"layer\") if \"layer\" in dbutils.widgets.getArgumentNames() else None\n",
    "    FILE_NAME = dbutils.widgets.get(\"file_name\") if \"file_name\" in dbutils.widgets.getArgumentNames() else None\n",
    "\n",
    "# Safety checks\n",
    "if not BASE_RAW_PATH or not BASE_BRONZE_PATH:\n",
    "    raise Exception(\"BASE_RAW_PATH or BASE_BRONZE_PATH not set. Run 01_Config first (in same job) or pass widgets.\")\n",
    "\n",
    "if not FILE_NAME:\n",
    "    raise Exception(\"FILE_NAME not provided. Pass via 01_Config or as widget file_name.\")\n",
    "\n",
    "if not (JDBC_HOST and JDBC_DB and JDBC_USER and JDBC_PASS):\n",
    "    raise Exception(\"JDBC parameters missing. Provide jdbc_hostname, jdbc_database, jdbc_user, jdbc_password as notebook widgets.\")\n",
    "\n",
    "print(\"RUN CONFIGURATION:\")\n",
    "print(\" BASE_RAW_PATH:\", BASE_RAW_PATH)\n",
    "print(\" BASE_BRONZE_PATH:\", BASE_BRONZE_PATH)\n",
    "print(\" DOMAIN:\", DOMAIN, \" LAYER:\", LAYER)\n",
    "print(\" FILE_NAME:\", FILE_NAME)\n",
    "print(\" JDBC_HOST:\", JDBC_HOST, \" JDBC_DB:\", JDBC_DB, \" JDBC_USER:\", JDBC_USER)\n",
    "\n",
    "# 3) Build JDBC URL\n",
    "jdbc_url = f\"jdbc:sqlserver://{JDBC_HOST}:1433;database={JDBC_DB};encrypt=true;trustServerCertificate=false;\"\n",
    "\n",
    "# 4) Read metadata table from SQL\n",
    "print(\"Reading metadata table dbo.sql_table_metadata from SQL...\")\n",
    "meta_df = read_sql_metadata(jdbc_url, JDBC_USER, JDBC_PASS, table_name=\"dbo.sql_table_metadata\")\n",
    "print(\"Metadata rows count:\", meta_df.count())\n",
    "\n",
    "meta_map = metadata_to_map(meta_df)\n",
    "if FILE_NAME not in meta_map:\n",
    "    raise Exception(f\"Metadata for file '{FILE_NAME}' not found in dbo.sql_table_metadata. Available keys: {list(meta_map.keys())}\")\n",
    "\n",
    "meta = meta_map[FILE_NAME]\n",
    "columns = meta.get(\"columns\", [])\n",
    "year_hint = meta.get(\"year_column\")\n",
    "table_name = meta.get(\"table_name\") or FILE_NAME.split(\".\")[0]\n",
    "\n",
    "print(f\"Metadata for {FILE_NAME}: table_name={table_name} year_hint={year_hint} columns={columns}\")\n",
    "\n",
    "# 5) Build raw file path and read CSV (no header)\n",
    "raw_file_path = BASE_RAW_PATH.rstrip(\"/\") + \"/\" + FILE_NAME.lstrip(\"/\")\n",
    "print(\"Reading raw CSV from:\", raw_file_path)\n",
    "\n",
    "df_raw = (spark.read\n",
    "            .option(\"header\", \"false\")\n",
    "            .option(\"sep\", \",\")\n",
    "            .option(\"inferSchema\", \"false\")\n",
    "            .csv(raw_file_path))\n",
    "\n",
    "print(\"Raw schema:\", df_raw.schema)\n",
    "print(\"Raw row sample:\")\n",
    "display(df_raw.limit(5))\n",
    "\n",
    "# 6) Apply headers from metadata\n",
    "df_named = add_headers(df_raw, columns)\n",
    "# trim whitespace for safety\n",
    "from pyspark.sql.functions import trim\n",
    "for c in df_named.columns:\n",
    "    df_named = df_named.withColumn(c, trim(col(c)))\n",
    "\n",
    "print(\"After applying headers. Schema:\")\n",
    "display(df_named.limit(5))\n",
    "\n",
    "# 7) Extract or compute year\n",
    "df_with_year, used_year = extract_year_column(df_named, year_hint)\n",
    "print(\"Used year column:\", used_year)\n",
    "print(\"Distinct years found:\")\n",
    "display(df_with_year.select(\"_year\").distinct())\n",
    "\n",
    "# 8) Write parquet to bronze path by year\n",
    "bronze_base = BASE_BRONZE_PATH.rstrip(\"/\") + f\"/{DOMAIN}/{LAYER}\"\n",
    "print(\"Writing parquet to bronze base:\", bronze_base)\n",
    "\n",
    "# Use coalesce(1) for single file per year (small data). Set coalesce_out=False for big data.\n",
    "write_parquet_by_year(df_with_year, bronze_base, table_name, compression=\"snappy\", coalesce_out=True, write_mode=\"overwrite\")\n",
    "\n",
    "# 9) List and show written files (for confirmation)\n",
    "from pyspark.sql.functions import col as _col\n",
    "years = [r[\"_year\"] for r in df_with_year.select(\"_year\").distinct().collect()]\n",
    "print(\"Written output folders and files:\")\n",
    "for y in years:\n",
    "    out_path = f\"{bronze_base.rstrip('/')}/{table_name}/{y}\"\n",
    "    try:\n",
    "        files = dbutils.fs.ls(out_path)\n",
    "        print(f\" Folder: {out_path}\")\n",
    "        for f in files:\n",
    "            print(\"  -\", f.path)\n",
    "    except Exception as e:\n",
    "        print(\"  Could not list\", out_path, \"Exception:\", e)\n",
    "\n",
    "print(\"03_Run completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Run",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
