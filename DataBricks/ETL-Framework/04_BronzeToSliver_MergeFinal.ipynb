{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2048e4ab-862f-4ae1-b9f4-63ac05861385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 1. Define Paths and Mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5963da1-282a-4e2e-82e7-87476f3d3909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, sha2, concat_ws, current_timestamp, coalesce, trim, to_timestamp, year\n",
    "from delta.tables import DeltaTable\n",
    "import datetime\n",
    "\n",
    "# --- 1. Define Paths and Mappings ---\n",
    "\n",
    "# Source Path (Input)\n",
    "SOURCE_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/HumanResources.Employee/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/dim/dim_Employee/\"\n",
    "\n",
    "# Define the Primary Key column(s) as a comma-separated string\n",
    "PK_RAW = \"CurrencyAlternateKey,CurrencyName\" \n",
    "\n",
    "# --- PARSE COMPOSITE KEY ---\n",
    "PK_COLS = [c.strip() for c in PK_RAW.split(\",\") if c.strip()]\n",
    "print(f\"Using Composite Primary Key for MERGE: {PK_COLS}\")\n",
    "\n",
    "# Columns needed for the Gold table\n",
    "CURRENCY_MAPPING = [\n",
    "    (\"CurrencyCode\", \"CurrencyAlternateKey\"), # Source column, Target column\n",
    "    (\"Name\", \"CurrencyName\"),\n",
    "    (\"ModifiedDate\", \"ModifiedDate\")\n",
    "]\n",
    "\n",
    "# Configure Authentication (use the DFS endpoint)\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "print(\"Configured Spark authentication.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c479cc7f-7b8c-45f5-9f56-376fc7305c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Read, Clean, and Prepare Source Data (df_source) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f24446-1954-427b-98c7-fe17c9bca01f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Read, Clean, and Prepare Source Data (df_source) ---\n",
    "print(f\"\\nReading Currency data from: {SOURCE_PATH}\")\n",
    "#df_source = spark.read.format(\"parquet\").load(SOURCE_PATH)\n",
    "df_source = (spark.read\n",
    "              .option(\"mergeSchema\", \"true\")\n",
    "              .option(\"recursiveFileLookup\", \"true\")\n",
    "              .parquet(SOURCE_PATH))\n",
    "print(\"Read OK. Rows:\", df_source.count(), \"Columns:\", df_source.columns)\n",
    "display(df_source.limit(5))\n",
    "\n",
    "# Select and Rename columns using the manual mapping\n",
    "selected_expr = [col(src).alias(tgt) for src, tgt in CURRENCY_MAPPING if src in df_source.columns]\n",
    "df_source = df_source.select(*selected_expr)\n",
    "\n",
    "# --- 2a. Data Cleaning and Deduplication ---\n",
    "\n",
    "print(\"Applying simple data cleaning transformations...\")\n",
    "\n",
    "# 1. Standardize and Clean String Columns (trim only)\n",
    "df_source = df_source.withColumn(\n",
    "    \"CurrencyAlternateKey\",\n",
    "    trim(col(\"CurrencyAlternateKey\")) \n",
    ").withColumn(\n",
    "    \"CurrencyName\",\n",
    "    trim(col(\"CurrencyName\"))\n",
    ")\n",
    "\n",
    "# 2. Handle Nulls \n",
    "df_source = df_source.withColumn(\n",
    "    \"CurrencyAlternateKey\",\n",
    "    coalesce(col(\"CurrencyAlternateKey\"), lit(\"N/A\")) # Ensure key is not null\n",
    ").withColumn(\n",
    "    \"CurrencyName\",\n",
    "    coalesce(col(\"CurrencyName\"), lit(\"Unknown Currency\")) # Ensure name is not null\n",
    ")\n",
    "\n",
    "# 3. Type Casting\n",
    "df_source = df_source.withColumn(\n",
    "    \"ModifiedDate\",\n",
    "    to_timestamp(col(\"ModifiedDate\")) # Cast ModifiedDate to a proper Timestamp type\n",
    ")\n",
    "\n",
    "# 4. Check for duplicates and keep one (Deduplication)\n",
    "initial_count = df_source.count()\n",
    "df_source = df_source.dropDuplicates(PK_COLS)\n",
    "final_count = df_source.count()\n",
    "\n",
    "print(f\"Deduplication complete. Removed {initial_count - final_count} duplicates based on {PK_COLS}.\")\n",
    "\n",
    "# --- 2b. Add Audit and Hash Columns ---\n",
    "\n",
    "# Add Audit Columns (ALWAYS create)\n",
    "df_source = df_source.withColumn(\"__ingest_ts\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"__source_path\", lit(SOURCE_PATH)) \n",
    "df_source = df_source.withColumn(\"__target_path\", lit(TARGET_PATH)) \n",
    "df_source = df_source.withColumn(\"__batch_id\", lit(\"Batch-\" + datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')))\n",
    "\n",
    "# Create a hash for change detection\n",
    "hash_cols = [\"CurrencyAlternateKey\", \"CurrencyName\"]\n",
    "df_source = df_source.withColumn(\n",
    "    \"__row_hash\",\n",
    "    sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in hash_cols]), 256)\n",
    ")\n",
    "\n",
    "# Add required DimCurrency columns (placeholders)\n",
    "df_source = df_source.withColumn(\"LoadTS\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"IsCurrent\", lit(True))\n",
    "df_source = df_source.withColumn(\"CurrencyKey\", lit(None).cast(\"long\")) # Placeholder for surrogate key\n",
    "df_source = df_source.withColumn(\"_year\", year(col(\"ModifiedDate\"))) # year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f306d8b-e8ab-4385-9c94-11de59df3710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. MERGE (Incremental Upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc493fb-a6f6-4c2e-bc0f-a592ae9159c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Check and Create Target Table (if needed) ---\n",
    "target_exists = DeltaTable.isDeltaTable(spark, TARGET_PATH)\n",
    "\n",
    "# List of all final columns for the DimCurrency schema\n",
    "# ALL_TARGET_COLS = [\n",
    "#     #\"CurrencyKey\", \n",
    "#     \"CurrencyAlternateKey\", \"CurrencyName\", \"ModifiedDate\", \"LoadTS\", \n",
    "#     \"__row_hash\", \"IsCurrent\", \"_year\", \"__ingest_ts\", \"__source_path\", \"__target_path\", \"__batch_id\"\n",
    "# ]\n",
    "ALL_TARGET_COLS = [\n",
    "    \"CurrencyAlternateKey\", \"CurrencyName\", \"ModifiedDate\", \"LoadTS\", \n",
    "    \"__row_hash\", \"IsCurrent\", \"_year\", \"__ingest_ts\", \"__source_path\", \"__target_path\", \"__batch_id\"\n",
    "]\n",
    "\n",
    "\n",
    "if not target_exists:\n",
    "    print(f\"Target table not found. Creating initial DimCurrency table at: {TARGET_PATH}\")\n",
    "    \n",
    "    # Select only the required columns and write the initial table\n",
    "    df_source.select(*[c for c in ALL_TARGET_COLS if c in df_source.columns]).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\")\\\n",
    "        .partitionBy(\"_year\")\\\n",
    "        .save(TARGET_PATH)\n",
    "    print(\"Initial DimCurrency table created. Skipping merge.\")\n",
    "\n",
    "else:\n",
    "    # --- 4. Perform MERGE (Incremental Upsert) ---\n",
    "    print(f\"\\nTarget table exists. Performing MERGE (Incremental Load)...\")\n",
    "    \n",
    "    dt_target = DeltaTable.forPath(spark, TARGET_PATH)\n",
    "    \n",
    "    # Define the condition to match an existing row (Primary Key/Alternate Key)\n",
    "    join_cond = f\"target.{PK_COLS} = source.{PK_COLS}\"\n",
    "    \n",
    "    # Define the condition to detect a change (Row Hash)\n",
    "    change_cond = \"target.__row_hash != source.__row_hash\"\n",
    "\n",
    "    dt_target.alias(\"target\").merge(\n",
    "        df_source.alias(\"source\"),\n",
    "        join_cond\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=change_cond,\n",
    "        set = {\n",
    "            # Update mutable data columns\n",
    "            \"CurrencyName\": \"source.CurrencyName\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            # Update audit columns on change\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\",\n",
    "            \"__ingest_ts\": \"source.__ingest_ts\",\n",
    "            \"__batch_id\": \"source.__batch_id\"\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values = {\n",
    "            # Insert all columns for new records\n",
    "            #\"CurrencyKey\": \"NULL\",\n",
    "            \"CurrencyAlternateKey\": \"source.CurrencyAlternateKey\",\n",
    "            \"CurrencyName\": \"source.CurrencyName\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\",\n",
    "            \"IsCurrent\": \"source.IsCurrent\",\n",
    "            \"_year\": \"source._year\",            \n",
    "            \"__ingest_ts\": \"source.__ingest_ts\",\n",
    "            \"__source_path\": \"source.__source_path\",\n",
    "            \"__target_path\": \"source.__target_path\",\n",
    "            \"__batch_id\": \"source.__batch_id\"\n",
    "        }\n",
    "    ) \\\n",
    "    .execute()\n",
    "    \n",
    "    print(\"MERGE (Incremental Load) complete. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893763d6-ead5-4a9a-a3f0-5a590883da36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# KEY for ABFSS (dfs endpoint) - needed for writing Delta\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.scrgvkrmade.dfs.core.windows.net\",\n",
    "    \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    ")\n",
    "\n",
    "# Read from BLOB (works)\n",
    "df = spark.read.parquet(\n",
    "    \"wasbs://project@scrgvkrmade.blob.core.windows.net/bronze/ResellerSales/Sales.Currency/*/part-*.parquet\"\n",
    ")\n",
    "\n",
    "#display(df)\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532fd2d9-8414-45c3-956b-58df41a35302",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765263419114}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read table root\n",
    "df = spark.read.format(\"delta\").load(\"wasbs://project@scrgvkrmade.blob.core.windows.net/silver/dim/dim_currency\")\n",
    "\n",
    "# read a specific partition (also allowed)\n",
    "df_part = spark.read.format(\"delta\").load(\n",
    "    \"wasbs://project@scrgvkrmade.blob.core.windows.net/silver/dim/dim_currency\"\n",
    ")\n",
    "display(df_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2841625d-ae71-4dc5-a9dc-b7ec2aba2a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lit, sha2, concat_ws, current_timestamp, coalesce, trim, \n",
    "    to_timestamp, year\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "import datetime\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "# --- 1. Define Paths and Mappings ---\n",
    "\n",
    "# Source Path (Input)\n",
    "\n",
    "# Source Path (Input)\n",
    "SOURCE_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Currency/\"\n",
    "TARGET_PATH = \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/dim/dim_currency/\"\n",
    "\n",
    "# Define the Primary Key column(s) as a comma-separated string (Composite Key)\n",
    "PK_RAW = \"CurrencyAlternateKey,CurrencyName\" \n",
    "\n",
    "# --- PARSE COMPOSITE KEY ---\n",
    "PK_COLS = [c.strip() for c in PK_RAW.split(\",\") if c.strip()]\n",
    "print(f\"Using Composite Primary Key for MERGE: {PK_COLS}\")\n",
    "# ---------------------------\n",
    "\n",
    "# Columns needed for the Gold DimCurrency table\n",
    "# Mapping format: (\"Source_Column_Name\", \"Target_Column_Name\")\n",
    "CURRENCY_MAPPING = [\n",
    "    (\"CurrencyCode\", \"CurrencyAlternateKey\"), \n",
    "    (\"Name\", \"CurrencyName\"),\n",
    "    (\"ModifiedDate\", \"ModifiedDate\")\n",
    "]\n",
    "\n",
    "# Configure Authentication (REPLACE WITH REAL KEY)\n",
    "storage_account_name = \"scrgvkrmade\"\n",
    "account_key = \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\"\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "print(\"Configured Spark authentication.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# --- 2. Read, Clean, and Prepare Source Data (df_source) ---\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nReading Currency data from: {SOURCE_PATH} (Multiple Parquet files)\")\n",
    "try:\n",
    "    df_source = (spark.read\n",
    "                  .option(\"mergeSchema\", \"true\")\n",
    "                  .option(\"recursiveFileLookup\", \"true\")\n",
    "                  .parquet(SOURCE_PATH))\n",
    "    print(f\"Read OK. Rows: {df_source.count()} Columns: {df_source.columns}\")\n",
    "except Exception:\n",
    "    print(\"Failed to read multiple Parquet files. Check path and permissions.\")\n",
    "    traceback.print_exc(file=sys.stdout)\n",
    "    raise\n",
    "\n",
    "# Select and Rename columns using the manual mapping\n",
    "selected_expr = [col(src).alias(tgt) for src, tgt in CURRENCY_MAPPING if src in df_source.columns]\n",
    "df_source = df_source.select(*selected_expr)\n",
    "\n",
    "# --- 2a. Data Cleaning and Deduplication ---\n",
    "\n",
    "print(\"Applying simple data cleaning transformations...\")\n",
    "\n",
    "# 1. Standardize and Clean String Columns (trim only)\n",
    "df_source = df_source.withColumn(\"CurrencyAlternateKey\", trim(col(\"CurrencyAlternateKey\")))\n",
    "df_source = df_source.withColumn(\"CurrencyName\", trim(col(\"CurrencyName\")))\n",
    "\n",
    "# 2. Handle Nulls \n",
    "df_source = df_source.withColumn(\n",
    "    \"CurrencyAlternateKey\",\n",
    "    coalesce(col(\"CurrencyAlternateKey\"), lit(\"N/A\"))\n",
    ").withColumn(\n",
    "    \"CurrencyName\",\n",
    "    coalesce(col(\"CurrencyName\"), lit(\"Unknown Currency\"))\n",
    ")\n",
    "\n",
    "# 3. Type Casting (Crucial for year extraction)\n",
    "df_source = df_source.withColumn(\n",
    "    \"ModifiedDate\",\n",
    "    to_timestamp(col(\"ModifiedDate\"))\n",
    ")\n",
    "\n",
    "# 4. Deduplication\n",
    "initial_count = df_source.count()\n",
    "df_source = df_source.dropDuplicates(PK_COLS)\n",
    "final_count = df_source.count()\n",
    "print(f\"Deduplication complete. Removed {initial_count - final_count} duplicates based on {PK_COLS}.\")\n",
    "\n",
    "\n",
    "# --- 2b. Add Audit and Hash Columns ---\n",
    "\n",
    "# Add Audit Columns\n",
    "df_source = df_source.withColumn(\"__ingest_ts\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"__source_path\", lit(SOURCE_PATH)) \n",
    "df_source = df_source.withColumn(\"__target_path\", lit(TARGET_PATH)) \n",
    "df_source = df_source.withColumn(\"__batch_id\", lit(\"Batch-\" + datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')))\n",
    "\n",
    "# Create a hash for change detection\n",
    "hash_cols = [\"CurrencyAlternateKey\", \"CurrencyName\"]\n",
    "df_source = df_source.withColumn(\n",
    "    \"__row_hash\",\n",
    "    sha2(concat_ws(\"||\", *[coalesce(col(c).cast(\"string\"), lit(\"\")) for c in hash_cols]), 256)\n",
    ")\n",
    "\n",
    "# Add required DimCurrency columns (placeholders)\n",
    "df_source = df_source.withColumn(\"LoadTS\", current_timestamp())\n",
    "df_source = df_source.withColumn(\"IsCurrent\", lit(True))\n",
    "df_source = df_source.withColumn(\"CurrencyKey\", lit(None).cast(\"long\")) # Placeholder for surrogate key\n",
    "\n",
    "# ADD _year for Partitioning (Using the imported 'year' function)\n",
    "df_source = df_source.withColumn(\"_year\", year(col(\"ModifiedDate\")))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# --- 3. Check and Create Target Table (if needed) and MERGE ---\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "target_exists = DeltaTable.isDeltaTable(spark, TARGET_PATH)\n",
    "\n",
    "# List of all final columns for the DimCurrency schema\n",
    "ALL_TARGET_COLS = [\n",
    "     \"CurrencyAlternateKey\", \"CurrencyName\", \"ModifiedDate\", \"LoadTS\", \n",
    "    \"__row_hash\", \"IsCurrent\", \"_year\", \"__ingest_ts\", \"__source_path\", \"__target_path\", \"__batch_id\"\n",
    "]\n",
    "\n",
    "\n",
    "if not target_exists:\n",
    "    print(f\"Target table not found. Creating initial Delta table at: {TARGET_PATH}\")\n",
    "    \n",
    "    df_source.select(*[c for c in ALL_TARGET_COLS if c in df_source.columns]).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .partitionBy(\"_year\") \\\n",
    "        .save(TARGET_PATH)\n",
    "    print(\"Initial write completed.\")\n",
    "\n",
    "else:\n",
    "    # --- Perform MERGE (Incremental Upsert) ---\n",
    "    print(f\"\\nTarget table exists. Performing MERGE (Incremental Load)...\")\n",
    "    \n",
    "    dt_target = DeltaTable.forPath(spark, TARGET_PATH)\n",
    "    \n",
    "    # Define the condition to match an existing row (Composite Primary Key)\n",
    "    # This correctly generates the SQL join string: \"target.Col1 = source.Col1 AND target.Col2 = source.Col2\"\n",
    "    join_cond = \" AND \".join([f\"target.{c} = source.{c}\" for c in PK_COLS])\n",
    "    \n",
    "    # Define the condition to detect a change (Row Hash)\n",
    "    change_cond = \"target.__row_hash != source.__row_hash\"\n",
    "    \n",
    "    print(f\"MERGE Join Condition: {join_cond}\")\n",
    "\n",
    "    dt_target.alias(\"target\").merge(\n",
    "        df_source.alias(\"source\"),\n",
    "        join_cond\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=change_cond,\n",
    "        set = {\n",
    "            # Update mutable data and audit columns\n",
    "            \"CurrencyName\": \"source.CurrencyName\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\",\n",
    "            \"__ingest_ts\": \"source.__ingest_ts\",\n",
    "            \"__batch_id\": \"source.__batch_id\"\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        # Insert all columns for new records\n",
    "        #values = {c: f\"source.{c}\" for c in ALL_TARGET_COLS if c in df_source.columns}\n",
    "        values = {\n",
    "            # Insert all columns for new records\n",
    "            #\"CurrencyKey\": \"NULL\",\n",
    "            \"CurrencyAlternateKey\": \"source.CurrencyAlternateKey\",\n",
    "            \"CurrencyName\": \"source.CurrencyName\",\n",
    "            \"ModifiedDate\": \"source.ModifiedDate\",\n",
    "            \"LoadTS\": \"source.LoadTS\",\n",
    "            \"__row_hash\": \"source.__row_hash\",\n",
    "            \"IsCurrent\": \"source.IsCurrent\",\n",
    "            \"_year\": \"source._year\",            \n",
    "            \"__ingest_ts\": \"source.__ingest_ts\",\n",
    "            \"__source_path\": \"source.__source_path\",\n",
    "            \"__target_path\": \"source.__target_path\",\n",
    "            \"__batch_id\": \"source.__batch_id\"\n",
    "        }\n",
    "    ) \\\n",
    "    .execute()\n",
    "    \n",
    "    print(\"MERGE (Incremental Load) complete. âœ…\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# --- 4. Validation (Optional) ---\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nValidating output by reading back Target_path:\", TARGET_PATH)\n",
    "try:\n",
    "    tgt = spark.read.format(\"delta\").load(TARGET_PATH)\n",
    "    print(\"Rows in Gold:\", tgt.count())\n",
    "except Exception:\n",
    "    print(\"Could not read back Delta target.\")\n",
    "    traceback.print_exc(file=sys.stdout)\n",
    "    \n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_BronzeToSliver_MergeFinal",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
