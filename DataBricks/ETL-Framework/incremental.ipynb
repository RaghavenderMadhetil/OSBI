{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e569c546-11ed-4ea3-b88a-89763f9fe366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 03_Run - ETL runner (delimiter detection + headers + parquet/delta + incremental + audit columns)\n",
    "# Paste this entire content into a Databricks Python notebook named \"03_Run\"\n",
    "\n",
    "import sys, importlib, re, traceback, json, uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, trim, lit, current_timestamp, to_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ensure utils module importable from /FileStore\n",
    "if \"/dbfs/FileStore\" not in sys.path:\n",
    "    sys.path.insert(0, \"/dbfs/FileStore\")\n",
    "try:\n",
    "    import utils_etl\n",
    "    importlib.reload(utils_etl)\n",
    "    print(\"Imported utils_etl OK.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to import utils_etl - ensure /FileStore/utils_etl.py exists\")\n",
    "    raise\n",
    "\n",
    "# ==============\n",
    "# Widgets (ADF should pass)\n",
    "# ==============\n",
    "dbutils.widgets.text(\"domain\", \"Finance\")\n",
    "dbutils.widgets.text(\"file_name\", \"\")            # e.g. Sales.Currency or Sales.Currency.csv\n",
    "dbutils.widgets.text(\"column_list\", \"\")          # JSON array OR CSV\n",
    "dbutils.widgets.text(\"year_column\", \"\")          # e.g. ModifiedDate (optional)\n",
    "dbutils.widgets.text(\"table_name\", \"\")           # folder name override (optional)\n",
    "\n",
    "dbutils.widgets.text(\"direct_account_key\", \"\")   # raw account key (recommended)\n",
    "dbutils.widgets.text(\"BASE_RAW_PATH\", \"\")        # optional override\n",
    "dbutils.widgets.text(\"BASE_BRONZE_PATH\", \"\")     # optional override\n",
    "dbutils.widgets.text(\"include_layer\", \"false\")   # if you want /<layer> in path\n",
    "\n",
    "# incremental / target options\n",
    "dbutils.widgets.text(\"incremental\", \"true\")     # true|false\n",
    "dbutils.widgets.text(\"target_format\", \"parquet\")# parquet|delta\n",
    "dbutils.widgets.text(\"merge\", \"false\")          # true|false (only for delta)\n",
    "dbutils.widgets.text(\"pk_columns\", \"\")          # comma-separated, required for merge\n",
    "\n",
    "# read widget values\n",
    "DOMAIN = dbutils.widgets.get(\"domain\").strip()\n",
    "FILE_NAME_IN = dbutils.widgets.get(\"file_name\").strip()\n",
    "COLUMN_LIST_WIDGET = dbutils.widgets.get(\"column_list\").strip()\n",
    "YEAR_COLUMN_WIDGET = dbutils.widgets.get(\"year_column\").strip()\n",
    "TABLE_NAME_WIDGET = dbutils.widgets.get(\"table_name\").strip()\n",
    "DIRECT_KEY = dbutils.widgets.get(\"direct_account_key\").strip()\n",
    "BASE_RAW_WIDGET = dbutils.widgets.get(\"BASE_RAW_PATH\").strip()\n",
    "BASE_BRONZE_WIDGET = dbutils.widgets.get(\"BASE_BRONZE_PATH\").strip()\n",
    "INCLUDE_LAYER = dbutils.widgets.get(\"include_layer\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "\n",
    "INCREMENTAL = dbutils.widgets.get(\"incremental\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "TARGET_FORMAT = dbutils.widgets.get(\"target_format\").strip().lower()  # parquet or delta\n",
    "MERGE = dbutils.widgets.get(\"merge\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "PK_COLUMNS_WIDGET = dbutils.widgets.get(\"pk_columns\").strip()\n",
    "\n",
    "# storage config defaults\n",
    "STORAGE_ACCOUNT = \"scrgvkrmade\"\n",
    "RAW_CONTAINER = \"project\"\n",
    "BRONZE_CONTAINER = \"bronze\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def detect_delimiter(file_path, sample_size=8192):\n",
    "    \"\"\"Return best-guess delimiter from sample.\"\"\"\n",
    "    text = None\n",
    "    try:\n",
    "        text = dbutils.fs.head(file_path, sample_size)\n",
    "    except Exception:\n",
    "        return \",\"\n",
    "    counts = {\",\": text.count(\",\"), \"|\": text.count(\"|\"), \";\": text.count(\";\"), \"\\t\": text.count(\"\\t\")}\n",
    "    delim = max(counts, key=counts.get)\n",
    "    if counts[delim] == 0:\n",
    "        return \",\"\n",
    "    return delim\n",
    "\n",
    "def set_storage_key(k):\n",
    "    if not k:\n",
    "        return\n",
    "    if (k.startswith('\"') and k.endswith('\"')) or (k.startswith(\"'\") and k.endswith(\"'\")):\n",
    "        k = k[1:-1]\n",
    "    k = k.strip()\n",
    "    if not re.fullmatch(r\"[A-Za-z0-9+/=]{20,300}\", k):\n",
    "        raise Exception(\"direct_account_key looks invalid; pass raw account key (base64), not connection string\")\n",
    "    spark.conf.set(f\"fs.azure.account.key.{STORAGE_ACCOUNT}.dfs.core.windows.net\", k)\n",
    "    print(\"Set storage key for\", STORAGE_ACCOUNT)\n",
    "    # quick test\n",
    "    try:\n",
    "        display(dbutils.fs.ls(f\"abfss://{RAW_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/\"))\n",
    "    except Exception as e:\n",
    "        print(\"Warning: test listing raw container failed (invalid key or container).\")\n",
    "        raise\n",
    "\n",
    "# resolve base paths task or widget\n",
    "def task_or_widget(key, widget_name):\n",
    "    try:\n",
    "        v = dbutils.jobs.taskValues.get(taskKey=key, key=key)\n",
    "        if hasattr(v,\"value\"): return v.value\n",
    "        return v\n",
    "    except Exception:\n",
    "        return dbutils.widgets.get(widget_name).strip()\n",
    "\n",
    "# ------------------ set storage key early ------------------\n",
    "if DIRECT_KEY:\n",
    "    set_storage_key(DIRECT_KEY)\n",
    "else:\n",
    "    print(\"No direct_account_key provided; cluster auth must provide access to ADLS.\")\n",
    "\n",
    "# base path resolution\n",
    "BASE_RAW_TASK = task_or_widget(\"BASE_RAW_PATH\",\"BASE_RAW_PATH\")\n",
    "BASE_BRONZE_TASK = task_or_widget(\"BASE_BRONZE_PATH\",\"BASE_BRONZE_PATH\")\n",
    "\n",
    "if BASE_RAW_TASK:\n",
    "    BASE_RAW_PATH = BASE_RAW_TASK\n",
    "elif BASE_RAW_WIDGET:\n",
    "    BASE_RAW_PATH = BASE_RAW_WIDGET\n",
    "elif DIRECT_KEY:\n",
    "    BASE_RAW_PATH = f\"abfss://{RAW_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "else:\n",
    "    raise Exception(\"BASE_RAW_PATH not resolved. Provide direct_account_key or BASE_RAW_PATH widget or run 01_Config.\")\n",
    "\n",
    "if BASE_BRONZE_TASK:\n",
    "    BASE_BRONZE_PATH = BASE_BRONZE_TASK\n",
    "elif BASE_BRONZE_WIDGET:\n",
    "    BASE_BRONZE_PATH = BASE_BRONZE_WIDGET\n",
    "elif DIRECT_KEY:\n",
    "    BASE_BRONZE_PATH = f\"abfss://{BRONZE_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "else:\n",
    "    raise Exception(\"BASE_BRONZE_PATH not resolved. Provide direct_account_key or BASE_BRONZE_PATH widget or run 01_Config.\")\n",
    "\n",
    "print(\"BASE_RAW_PATH:\", BASE_RAW_PATH)\n",
    "print(\"BASE_BRONZE_PATH:\", BASE_BRONZE_PATH)\n",
    "print(\"INCREMENTAL:\", INCREMENTAL, \"TARGET_FORMAT:\", TARGET_FORMAT, \"MERGE:\", MERGE)\n",
    "\n",
    "# ---------- normalize file and metadata ----------\n",
    "if not FILE_NAME_IN:\n",
    "    raise Exception(\"file_name widget required (e.g. Sales.Currency.csv)\")\n",
    "file_key = FILE_NAME_IN\n",
    "file_key_no_ext = file_key[:-4] if file_key.lower().endswith(\".csv\") else file_key\n",
    "folder_table_name = TABLE_NAME_WIDGET if TABLE_NAME_WIDGET else file_key_no_ext\n",
    "\n",
    "# parse column_list\n",
    "def parse_column_list(text):\n",
    "    txt = text.strip()\n",
    "    if not txt:\n",
    "        return []\n",
    "    txt = txt.replace('\"\"','\"')\n",
    "    if txt.startswith(\"[\") and txt.endswith(\"]\"):\n",
    "        try:\n",
    "            arr = json.loads(txt)\n",
    "            return [str(x).strip() for x in arr if x and str(x).strip()!=\"\"]\n",
    "        except Exception:\n",
    "            txt2 = txt.strip(\"[]\")\n",
    "            return [c.strip().strip('\"').strip(\"'\") for c in txt2.split(\",\") if c.strip()!='']\n",
    "    return [c.strip().strip('\"').strip(\"'\") for c in txt.split(\",\") if c.strip()!='']\n",
    "\n",
    "if COLUMN_LIST_WIDGET:\n",
    "    columns = parse_column_list(COLUMN_LIST_WIDGET)\n",
    "else:\n",
    "    raise Exception(\"column_list missing. Pass column_list from ADF Lookup (JSON array or CSV).\")\n",
    "\n",
    "if not columns:\n",
    "    raise Exception(\"Parsed columns list is empty. Provide valid column_list.\")\n",
    "\n",
    "# determine year/timestamp column to use for incremental\n",
    "year_hint = YEAR_COLUMN_WIDGET if YEAR_COLUMN_WIDGET else None\n",
    "if not year_hint and any(c.lower()==\"modifieddate\" for c in columns):\n",
    "    year_hint = [c for c in columns if c.lower()==\"modifieddate\"][0]\n",
    "    print(\"Defaulting year_column to ModifiedDate\")\n",
    "\n",
    "if not year_hint and INCREMENTAL:\n",
    "    print(\"Warning: incremental=true but no year_column provided. Incremental load will compare _ingest_ts only (may load all rows).\")\n",
    "\n",
    "# ---------- read raw csv ----------\n",
    "raw_read_name = FILE_NAME_IN if FILE_NAME_IN and FILE_NAME_IN.strip()!=\"\" else file_key_no_ext + \".csv\"\n",
    "raw_path = BASE_RAW_PATH.rstrip(\"/\") + \"/\" + raw_read_name.lstrip(\"/\")\n",
    "print(\"Raw file path:\", raw_path)\n",
    "\n",
    "detected_sep = detect_delimiter(raw_path, sample_size=8192)\n",
    "print(\"Detected delimiter:\", repr(detected_sep))\n",
    "\n",
    "try:\n",
    "    df_raw = (spark.read\n",
    "                .option(\"header\",\"false\")\n",
    "                .option(\"sep\", detected_sep)\n",
    "                .option(\"quote\", '\"')\n",
    "                .option(\"escape\", \"\\\\\")\n",
    "                .option(\"multiLine\", \"true\")\n",
    "                .option(\"inferSchema\", \"false\")\n",
    "                .csv(raw_path))\n",
    "    print(\"Raw row count:\", df_raw.count())\n",
    "    display(df_raw.limit(5))\n",
    "except Exception:\n",
    "    print(\"Fallback to comma separator\")\n",
    "    df_raw = (spark.read\n",
    "                .option(\"header\",\"false\")\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"quote\", '\"')\n",
    "                .option(\"escape\", \"\\\\\")\n",
    "                .option(\"multiLine\", \"true\")\n",
    "                .option(\"inferSchema\", \"false\")\n",
    "                .csv(raw_path))\n",
    "    print(\"Fallback row count:\", df_raw.count())\n",
    "    display(df_raw.limit(5))\n",
    "\n",
    "# apply headers\n",
    "df_named = utils_etl.add_headers(df_raw, columns)\n",
    "for c in df_named.columns:\n",
    "    df_named = df_named.withColumn(c, trim(col(c)))\n",
    "\n",
    "print(\"After applying headers sample:\")\n",
    "display(df_named.limit(5))\n",
    "\n",
    "# ---------- prepare audit columns & normalized timestamp column ----------\n",
    "INGEST_RUNID = str(uuid.uuid4())\n",
    "INGEST_TS = datetime.utcnow().isoformat()  # string representation\n",
    "df_named = df_named.withColumn(\"_ingest_runid\", lit(INGEST_RUNID)) \\\n",
    "                   .withColumn(\"_ingest_ts\", lit(INGEST_TS)) \\\n",
    "                   .withColumn(\"_source_file\", lit(raw_read_name)) \\\n",
    "                   .withColumn(\"_source_path\", lit(raw_path))\n",
    "\n",
    "# normalize year/timestamp column: create _src_ts (timestamp) and _year (int)\n",
    "# If year_hint provided, try to parse it to timestamp; otherwise create _src_ts from _ingest_ts\n",
    "if year_hint and year_hint in df_named.columns:\n",
    "    # attempt to convert the source column to timestamp (accepts many formats)\n",
    "    try:\n",
    "        df_named = df_named.withColumn(\"_src_ts\", to_timestamp(col(year_hint)))\n",
    "    except Exception:\n",
    "        # fallback: cast to timestamp via to_timestamp\n",
    "        df_named = df_named.withColumn(\"_src_ts\", to_timestamp(col(year_hint)))\n",
    "else:\n",
    "    # no year column, fallback to current ingest ts\n",
    "    df_named = df_named.withColumn(\"_src_ts\", to_timestamp(lit(INGEST_TS)))\n",
    "\n",
    "# compute _year from _src_ts (fallback to ingest year)\n",
    "df_named = df_named.withColumn(\"_year\", col(\"_src_ts\").cast(\"date\"))\n",
    "# If you prefer integer year:\n",
    "from pyspark.sql.functions import year as spark_year\n",
    "df_named = df_named.withColumn(\"_year\", spark_year(col(\"_src_ts\")).cast(\"int\"))\n",
    "\n",
    "print(\"Sample with audit and _year/_src_ts:\")\n",
    "display(df_named.limit(5))\n",
    "\n",
    "# ---------- incremental filtering ----------\n",
    "def get_existing_max_src_ts(bronze_base, table_name):\n",
    "    \"\"\"Read existing target (parquet or delta) and return max(_src_ts) (timestamp) or None.\"\"\"\n",
    "    import pyspark.sql.functions as F\n",
    "    try:\n",
    "        # attempt to read any existing files under table path (uses Delta or parquet)\n",
    "        df_exist = None\n",
    "        if TARGET_FORMAT == \"delta\":\n",
    "            # delta read (if delta table exists at path)\n",
    "            try:\n",
    "                df_exist = spark.read.format(\"delta\").load(f\"{bronze_base.rstrip('/')}/{table_name}\")\n",
    "            except Exception:\n",
    "                # maybe delta table doesn't exist, fall back to parquet\n",
    "                df_exist = None\n",
    "        if df_exist is None:\n",
    "            # try parquet\n",
    "            df_exist = spark.read.parquet(f\"{bronze_base.rstrip('/')}/{table_name}\")\n",
    "        if \"_src_ts\" in df_exist.columns:\n",
    "            mx = df_exist.select(F.max(col(\"_src_ts\")).alias(\"mx\")).collect()[0][\"mx\"]\n",
    "            return mx\n",
    "        # else try year_column if present\n",
    "        if year_hint and year_hint in df_exist.columns:\n",
    "            mx = df_exist.select(F.max(to_timestamp(col(year_hint))).alias(\"mx\")).collect()[0][\"mx\"]\n",
    "            return mx\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # no existing data or error reading -> treat as no existing\n",
    "        print(\"No existing target data found or error reading existing:\", e)\n",
    "        return None\n",
    "\n",
    "bronze_base = BASE_BRONZE_PATH.rstrip(\"/\") + f\"/{DOMAIN}\" if not INCLUDE_LAYER else BASE_BRONZE_PATH.rstrip(\"/\") + f\"/{DOMAIN}/Bronze\"\n",
    "print(\"Bronze base:\", bronze_base)\n",
    "target_table_path = f\"{bronze_base.rstrip('/')}/{folder_table_name}\"\n",
    "\n",
    "existing_max = None\n",
    "if INCREMENTAL:\n",
    "    existing_max = get_existing_max_src_ts(bronze_base, folder_table_name)\n",
    "    print(\"Existing max _src_ts:\", existing_max)\n",
    "\n",
    "# filter incoming rows\n",
    "if INCREMENTAL and existing_max is not None:\n",
    "    # only new rows where _src_ts > existing_max\n",
    "    df_to_write = df_named.filter(col(\"_src_ts\") > lit(existing_max))\n",
    "    print(\"Rows after incremental filter:\", df_to_write.count())\n",
    "else:\n",
    "    df_to_write = df_named\n",
    "    print(\"No incremental filter applied (full load). Rows:\", df_to_write.count())\n",
    "\n",
    "if df_to_write.count() == 0:\n",
    "    print(\"No new rows to write. Exiting.\")\n",
    "    dbutils.notebook.exit(\"No new rows\")\n",
    "    \n",
    "# ---------- write: parquet or delta (with optional merge) ----------\n",
    "if TARGET_FORMAT == \"delta\" and MERGE:\n",
    "    # require PK columns\n",
    "    if not PK_COLUMNS_WIDGET:\n",
    "        raise Exception(\"merge=true but pk_columns not provided. Pass comma-separated PK column names in pk_columns widget.\")\n",
    "    pk_cols = [c.strip() for c in PK_COLUMNS_WIDGET.split(\",\") if c.strip()!='']\n",
    "    # create Delta table path if not exists, then perform MERGE using pk columns\n",
    "    from delta.tables import DeltaTable\n",
    "    import pyspark.sql.functions as F\n",
    "    # ensure target directory exists (or Delta will create it on first write below)\n",
    "    try:\n",
    "        # if delta table exists, perform merge\n",
    "        delta_exists = False\n",
    "        try:\n",
    "            DeltaTable.forPath(spark, target_table_path)\n",
    "            delta_exists = True\n",
    "        except Exception:\n",
    "            delta_exists = False\n",
    "\n",
    "        if not delta_exists:\n",
    "            # write initial delta (overwrite) then continue with merge semantics\n",
    "            df_to_write.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(target_table_path)\n",
    "            print(f\"Delta table created at {target_table_path}\")\n",
    "        else:\n",
    "            # perform merge: match on pk_cols\n",
    "            deltaTable = DeltaTable.forPath(spark, target_table_path)\n",
    "            alias_target = \"t\"\n",
    "            alias_source = \"s\"\n",
    "            # build merge condition\n",
    "            cond = \" AND \".join([f\"{alias_target}.{c} = {alias_source}.{c}\" for c in pk_cols])\n",
    "            # do MERGE: update matched, insert not matched\n",
    "            deltaTable.alias(alias_target).merge(\n",
    "                df_to_write.alias(alias_source),\n",
    "                cond\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "            print(\"Delta MERGE executed.\")\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "else:\n",
    "    # write parquet files (one folder per year)\n",
    "    print(\"Writing parquet to:\", target_table_path)\n",
    "    # reuse utils_etl writer which writes by year into path/<table>/<year>\n",
    "    utils_etl.write_parquet_by_year(df_to_write, bronze_base, folder_table_name, compression=\"snappy\", coalesce_out=True, write_mode=\"append\" if INCREMENTAL else \"overwrite\")\n",
    "\n",
    "# ---------- confirm output ----------\n",
    "years = [r[\"_year\"] for r in df_to_write.select(\"_year\").distinct().collect()]\n",
    "print(\"Outputs written for years:\", years)\n",
    "for y in years:\n",
    "    out_path = f\"{bronze_base.rstrip('/')}/{folder_table_name}/{y}\"\n",
    "    print(\"Listing:\", out_path)\n",
    "    try:\n",
    "        for f in dbutils.fs.ls(out_path):\n",
    "            print(\" -\", f.path)\n",
    "    except Exception as e:\n",
    "        print(\"Could not list:\", out_path, e)\n",
    "\n",
    "print(\"03_Run finished. IngestRunId:\", INGEST_RUNID)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "incremental",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
