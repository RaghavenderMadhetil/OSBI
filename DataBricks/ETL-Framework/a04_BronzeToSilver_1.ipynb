{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4f637c-f126-48a1-b877-382af76e01b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------- WIDGETS / DEFAULTS (ADF overrides these) ----------\n",
    "# Create these widgets in the notebook UI once (or ADF passes them)\n",
    "dbutils.widgets.text(\"Source_path\", \"abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/ResellerSales/Sales.Currency/\")\n",
    "dbutils.widgets.text(\"Target_path\", \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/\")\n",
    "dbutils.widgets.text(\"column_list\", '[\"CurrencyCode\",\"CurrencyName\",\"ModifiedDate\",\"__ingest_ts\",\"__source_file\",\"__source_path\",\"__batch_id\",\"__row_hash\"]')\n",
    "dbutils.widgets.text(\"direct_account_key\", \"E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==\")   # optional\n",
    "dbutils.widgets.text(\"domain\", \"ResellerSales\")\n",
    "dbutils.widgets.text(\"file_name\", \"Sales.Currency/*/*.parquet\")\n",
    "dbutils.widgets.text(\"table_name\", \"stg.currency\")\n",
    "dbutils.widgets.text(\"year_column\", \"ModifiedDate\")          # e.g. ModifiedDate\n",
    "dbutils.widgets.text(\"incremental_flag\", \"false\")\n",
    "dbutils.widgets.text(\"merge_flag\", \"true\")\n",
    "dbutils.widgets.text(\"pk_columns\", \"CurrencyAlternateKey\")\n",
    "dbutils.widgets.text(\"batch_name\", \"Batch_B\")\n",
    "dbutils.widgets.text(\"watermark\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a1eb5c-a54b-439b-829e-37471bfbbf6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Linear end-to-end (NO select_columns variable)\n",
    "# Read parquet -> clean/transform -> create/merge Delta\n",
    "import traceback, datetime\n",
    "from pyspark.sql.functions import col, trim, to_timestamp, year, lit, concat_ws, sha2, current_timestamp, coalesce, upper\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ---------------------------\n",
    "# Inputs (explicit parquet file path you provided)\n",
    "# ---------------------------\n",
    "src_parquet_path = \"wasbs://project@scrgvkrmade.blob.core.windows.net/bronze/ResellerSales/Sales.Currency/2019/part-00000-tid-4070521650027828931-365a17b5-0732-4562-a52e-a83db34c5cfe-119-1.c000.snappy.parquet\"\n",
    "\n",
    "# Target path via widget (ADF can override)\n",
    "dbutils.widgets.text(\"Target_path\", \"abfss://project@scrgvkrmade.dfs.core.windows.net/silver/\")\n",
    "Target_path = dbutils.widgets.get(\"Target_path\").strip()\n",
    "\n",
    "print(\"Source parquet (explicit):\", src_parquet_path)\n",
    "print(\"Target delta path:\", Target_path)\n",
    "\n",
    "# ---------------------------\n",
    "# READ SOURCE parquet (single file)\n",
    "# ---------------------------\n",
    "print(\"Reading parquet file...\")\n",
    "try:\n",
    "    df = spark.read.parquet(src_parquet_path)\n",
    "    print(\"Read OK.\")\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"Failed to read the specified parquet file.\")\n",
    "\n",
    "# show columns found in the parquet\n",
    "print(\"Source columns:\", df.columns)\n",
    "display(df.limit(5))\n",
    "\n",
    "# ---------------------------\n",
    "# CLEAN / TRANSFORM using ALL available columns\n",
    "# - Trim string columns\n",
    "# - Uppercase CurrencyCode (if present)\n",
    "# - Cast ModifiedDate to timestamp and add _year (if present)\n",
    "# - Fill simple nulls (e.g., CurrencyName)\n",
    "# - Add audit columns if missing\n",
    "# - Compute deterministic __row_hash\n",
    "# ---------------------------\n",
    "\n",
    "# 1) Trim all string columns\n",
    "string_cols = [c for c,d in df.dtypes if d == \"string\"]\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "# 2) Uppercase CurrencyCode if exists\n",
    "if \"CurrencyCode\" in df.columns:\n",
    "    df = df.withColumn(\"CurrencyCode\", upper(col(\"CurrencyCode\")))\n",
    "\n",
    "# 3) Cast ModifiedDate -> timestamp and add _year (fallback to current year if missing)\n",
    "if \"ModifiedDate\" in df.columns:\n",
    "    try:\n",
    "        df = df.withColumn(\"ModifiedDate\", to_timestamp(col(\"ModifiedDate\")))\n",
    "        df = df.withColumn(\"_year\", year(col(\"ModifiedDate\")))\n",
    "        print(\"Converted ModifiedDate to timestamp and added _year.\")\n",
    "    except Exception:\n",
    "        print(\"Could not cast ModifiedDate to timestamp; adding fallback _year.\")\n",
    "        if \"_year\" not in df.columns:\n",
    "            df = df.withColumn(\"_year\", lit(datetime.datetime.utcnow().year))\n",
    "else:\n",
    "    if \"_year\" not in df.columns:\n",
    "        df = df.withColumn(\"_year\", lit(datetime.datetime.utcnow().year))\n",
    "        print(\"Added fallback _year with current year.\")\n",
    "\n",
    "# 4) Fill some sensible nulls\n",
    "if \"CurrencyName\" in df.columns:\n",
    "    df = df.withColumn(\"CurrencyName\", coalesce(col(\"CurrencyName\"), lit(\"\")))\n",
    "\n",
    "# 5) Add audit columns if missing\n",
    "if \"__ingest_ts\" not in df.columns:\n",
    "    df = df.withColumn(\"__ingest_ts\", current_timestamp())\n",
    "if \"__source_file\" not in df.columns:\n",
    "    df = df.withColumn(\"__source_file\", lit(src_parquet_path.split(\"/\")[-1]))\n",
    "if \"__source_path\" not in df.columns:\n",
    "    df = df.withColumn(\"__source_path\", lit(src_parquet_path))\n",
    "if \"__batch_id\" not in df.columns:\n",
    "    dbatch = \"Batch-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    df = df.withColumn(\"__batch_id\", lit(dbatch))\n",
    "\n",
    "# 6) Compute deterministic row hash (__row_hash) using best available PK(s) or all columns\n",
    "pk_candidates = [c for c in [\"CurrencyAlternateKey\",\"CurrencyCode\"] if c in df.columns]\n",
    "if pk_candidates:\n",
    "    cols_for_hash = pk_candidates\n",
    "else:\n",
    "    cols_for_hash = df.columns\n",
    "\n",
    "concat_expr = concat_ws(\"||\", *[col(c).cast(\"string\") for c in cols_for_hash])\n",
    "df = df.withColumn(\"__row_hash\", sha2(concat_expr, 256))\n",
    "\n",
    "print(\"After cleaning/transforms. Columns now:\", df.columns)\n",
    "display(df.limit(5))\n",
    "\n",
    "# ---------------------------\n",
    "# DROP DUPLICATES by PK if available (keeps first encountered)\n",
    "# ---------------------------\n",
    "pk_for_merge = [\"CurrencyAlternateKey\"]\n",
    "pk_in_df = [p for p in pk_for_merge if p in df.columns]\n",
    "if pk_in_df:\n",
    "    before = None\n",
    "    try:\n",
    "        before = df.count()\n",
    "    except Exception:\n",
    "        pass\n",
    "    df = df.dropDuplicates(pk_in_df)\n",
    "    if before is not None:\n",
    "        after = df.count()\n",
    "        print(f\"Dropped duplicates by PK {pk_in_df}: {before-after} rows removed.\")\n",
    "else:\n",
    "    # if no CurrencyAlternateKey, dedupe by __row_hash\n",
    "    if \"__row_hash\" in df.columns:\n",
    "        df = df.dropDuplicates([\"__row_hash\"])\n",
    "        print(\"Dropped duplicates by __row_hash.\")\n",
    "    else:\n",
    "        print(\"No PK or __row_hash available for deduplication; skipping dropDuplicates.\")\n",
    "\n",
    "# ---------------------------\n",
    "# WRITE / MERGE into Delta\n",
    "# - If target doesn't exist: create initial delta partitioned by _year\n",
    "# - If target exists: MERGE using CurrencyAlternateKey if present else __row_hash\n",
    "# ---------------------------\n",
    "print(\"Preparing to write/merge to target Delta:\", Target_path)\n",
    "delta_exists = True\n",
    "try:\n",
    "    _ = spark.read.format(\"delta\").load(Target_path)\n",
    "except Exception:\n",
    "    delta_exists = False\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "if not delta_exists:\n",
    "    print(\"Target Delta not found -> creating initial Delta.\")\n",
    "    (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .partitionBy(\"_year\")\n",
    "        .save(Target_path))\n",
    "    print(\"Initial Delta created at:\", Target_path)\n",
    "else:\n",
    "    # choose merge PK: CurrencyAlternateKey if present, else __row_hash\n",
    "    if \"CurrencyAlternateKey\" in df.columns:\n",
    "        merge_pks = [\"CurrencyAlternateKey\"]\n",
    "    elif \"__row_hash\" in df.columns:\n",
    "        merge_pks = [\"__row_hash\"]\n",
    "    else:\n",
    "        raise RuntimeError(\"No column available for MERGE PK (neither CurrencyAlternateKey nor __row_hash).\")\n",
    "\n",
    "    # build merge condition\n",
    "    join_cond = \" AND \".join([f\"target.`{c}` = source.`{c}`\" for c in merge_pks])\n",
    "    print(\"Merging using keys:\", merge_pks)\n",
    "    dt = DeltaTable.forPath(spark, Target_path)\n",
    "    dt.alias(\"target\").merge(\n",
    "        df.alias(\"source\"),\n",
    "        join_cond\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "    print(\"MERGE completed into:\", Target_path)\n",
    "\n",
    "# ---------------------------\n",
    "# FINAL: show small sample from target\n",
    "# ---------------------------\n",
    "try:\n",
    "    tgt = spark.read.format(\"delta\").load(Target_path)\n",
    "    print(\"Target approx row count:\", tgt.count())\n",
    "    display(tgt.limit(5))\n",
    "except Exception as e:\n",
    "    print(\"Could not read target after write/merge:\", e)\n",
    "    try:\n",
    "        for f in dbutils.fs.ls(Target_path):\n",
    "            print(\"-\", f.path)\n",
    "    except Exception as e2:\n",
    "        print(\"Also failed to list target path:\", e2)\n",
    "\n",
    "print(\"End-to-end completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "a04_BronzeToSilver_1",
   "widgets": {
    "Target_path": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/",
     "nuid": "438a5e44-3d3e-4458-b8a6-62466a6a90f2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/silver/",
      "label": null,
      "name": "Target_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
