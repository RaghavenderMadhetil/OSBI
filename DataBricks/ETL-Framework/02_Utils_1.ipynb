{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8fb16e-ca87-44bf-b0db-42a95304eacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_Utils - helper functions for metadata-driven CSV -> Parquet ETL\n",
    "# No I/O in this notebook; only pure functions.\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import trim, col, year, to_timestamp, coalesce, lit\n",
    "import datetime\n",
    "\n",
    "def read_sql_metadata(jdbc_url: str, jdbc_user: str, jdbc_pass: str, table_name: str = \"dbo.sql_table_metadata\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read metadata table from Azure SQL and return a Spark DataFrame.\n",
    "    Expects columns:\n",
    "      - file_name (varchar)\n",
    "      - column_list (comma-separated string)\n",
    "      - year_column (optional)\n",
    "      - table_name (optional)\n",
    "    \"\"\"\n",
    "    df = (spark.read.format(\"jdbc\")\n",
    "            .option(\"url\", jdbc_url)\n",
    "            .option(\"dbtable\", table_name)\n",
    "            .option(\"user\", jdbc_user)\n",
    "            .option(\"password\", jdbc_pass)\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "            .load())\n",
    "    return df\n",
    "\n",
    "def metadata_to_map(meta_df: DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Convert metadata DataFrame to a Python dict:\n",
    "      { file_name -> {\"columns\": [...], \"year_column\": \"...\", \"table_name\": \"...\" } }\n",
    "    \"\"\"\n",
    "    rows = meta_df.collect()\n",
    "    m = {}\n",
    "    for r in rows:\n",
    "        d = r.asDict()\n",
    "        fname = d.get(\"file_name\")\n",
    "        cols_str = d.get(\"column_list\") or \"\"\n",
    "        cols_list = [c.strip() for c in cols_str.split(\",\") if c.strip() != \"\"]\n",
    "        year_col = d.get(\"year_column\") if \"year_column\" in d else None\n",
    "        table_name = d.get(\"table_name\") if \"table_name\" in d and d.get(\"table_name\") else (fname.split(\".\")[0] if fname else None)\n",
    "        m[fname] = {\"columns\": cols_list, \"year_column\": year_col, \"table_name\": table_name}\n",
    "    return m\n",
    "\n",
    "def add_headers(df: DataFrame, columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Rename dataframe columns using the metadata columns list.\n",
    "    If the CSV has more columns than metadata, fallback names _cN are used for extras.\n",
    "    \"\"\"\n",
    "    new_names = []\n",
    "    for i, old in enumerate(df.columns):\n",
    "        if i < len(columns):\n",
    "            new_names.append(columns[i])\n",
    "        else:\n",
    "            new_names.append(f\"_c{i}\")\n",
    "    return df.toDF(*new_names)\n",
    "\n",
    "def extract_year_column(df: DataFrame, year_column_hint: str = None) -> (DataFrame, str):\n",
    "    \"\"\"\n",
    "    Determine a year column and return (df_with__year, used_year_column_name).\n",
    "    Logic:\n",
    "      1. If year_column_hint is provided and exists, use it.\n",
    "      2. Else pick first column containing 'year' in name.\n",
    "      3. Else pick first column containing 'date' in name and extract year.\n",
    "      4. Else add current UTC year as _year.\n",
    "    The returned DataFrame always contains an integer column named '_year'.\n",
    "    \"\"\"\n",
    "    current_year = datetime.datetime.utcnow().year\n",
    "    cols = df.columns\n",
    "    used = None\n",
    "    df2 = df\n",
    "\n",
    "    if year_column_hint and year_column_hint in cols:\n",
    "        used = year_column_hint\n",
    "        try:\n",
    "            df2 = df2.withColumn(\"_year\", year(to_timestamp(col(used))))\n",
    "        except Exception:\n",
    "            df2 = df2.withColumn(\"_year\", col(used).cast(\"int\"))\n",
    "    else:\n",
    "        candidates = [c for c in cols if \"year\" in c.lower()]\n",
    "        if candidates:\n",
    "            used = candidates[0]\n",
    "            df2 = df2.withColumn(\"_year\", col(used).cast(\"int\"))\n",
    "        else:\n",
    "            date_candidates = [c for c in cols if \"date\" in c.lower()]\n",
    "            if date_candidates:\n",
    "                used = date_candidates[0]\n",
    "                try:\n",
    "                    df2 = df2.withColumn(\"_year\", year(to_timestamp(col(used))))\n",
    "                except Exception:\n",
    "                    df2 = df2.withColumn(\"_year\", col(used).cast(\"int\"))\n",
    "            else:\n",
    "                df2 = df2.withColumn(\"_year\", lit(current_year))\n",
    "\n",
    "    # Normalize and fill nulls with current year\n",
    "    df2 = df2.withColumn(\"_year\", coalesce(col(\"_year\").cast(\"int\"), lit(current_year)))\n",
    "    return df2, used\n",
    "\n",
    "def write_parquet_by_year(df_with_year: DataFrame, bronze_base_path: str, table_name: str,\n",
    "                          compression: str = \"snappy\", coalesce_out: bool = True, write_mode: str = \"overwrite\"):\n",
    "    \"\"\"\n",
    "    Write parquet files grouped by _year. Folder layout:\n",
    "      <bronze_base_path>/<table_name>/<year>/part-xxxx.snappy.parquet\n",
    "\n",
    "    Parameters:\n",
    "      - df_with_year: DataFrame that must contain integer column '_year'\n",
    "      - bronze_base_path: ABFSS/wasbs base path (no trailing slash recommended)\n",
    "      - table_name: logical table name (folder)\n",
    "      - compression: parquet compression (default snappy)\n",
    "      - coalesce_out: if True coalesce(1) to produce single file per year (small data). Set False for large data.\n",
    "      - write_mode: \"overwrite\" or \"append\"\n",
    "    \"\"\"\n",
    "    years = [r[\"_year\"] for r in df_with_year.select(\"_year\").distinct().collect()]\n",
    "    for y in years:\n",
    "        out_path = f\"{bronze_base_path.rstrip('/')}/{table_name}/{y}\"\n",
    "        df_year = df_with_year.filter(col(\"_year\") == y).drop(\"_year\")\n",
    "        writer = df_year.coalesce(1) if coalesce_out else df_year\n",
    "        writer.write.mode(write_mode).option(\"compression\", compression).parquet(out_path)\n",
    "        print(f\"Wrote parquet to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Utils_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
