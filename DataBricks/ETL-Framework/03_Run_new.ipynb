{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908e0c01-1781-42af-87d7-5e7c6cbd01a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sales.Store\n",
    "# [\"BusinessEntityID\", \"Name\", \"SalesPersonID\", \"Demographics\", \"rowguid\", \"ModifiedDate\"]\n",
    "# Store.csv\n",
    "\n",
    "\n",
    "# [\"SpecialOfferID\", \"Description\", \"DiscountPct\", \"Type\", \"Category\", \"StartDate\", \"EndDate\", \"MinQty\", \"MaxQty\", \"rowguid\", \"ModifiedDate\"]\n",
    "# Sales.SpecialOffer\n",
    "# SpecialOffer.csv\n",
    "\n",
    "# Year column used (hint): ModifiedDate\n",
    "\n",
    "# [CAST_INVALID_INPUT] The value 'ModifiedDate' of the type \"STRING\" cannot be cast to \"TIMESTAMP\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
    "# == DataFrame ==\n",
    "# \"to_timestamp\" was called from jdk.internal.reflect.GeneratedMethodAccessor899.invoke(Unknown Source)\n",
    "\n",
    "\n",
    "# Year column used (hint): ModifiedDate\n",
    "\n",
    "# [CAST_INVALID_INPUT] The value 'ModifiedDate' of the type \"STRING\" cannot be cast to \"TIMESTAMP\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
    "# == DataFrame ==\n",
    "# \"to_timestamp\" was called from jdk.internal.reflect.GeneratedMethodAccessor899.invoke(Unknown Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9c3bad-c4c7-4da3-80f9-16d353573305",
     "showTitle": false,
     "tableResultSettingsMap": {
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764672015721}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 03_Run - ETL runner (delimiter detection + headers + audit columns + parquet)\n",
    "# Paste this entire content into a Databricks notebook named \"03_Run\"\n",
    "import sys, importlib, re, traceback, json, uuid\n",
    "from pyspark.sql.functions import col, trim, current_timestamp, current_date, lit\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Ensure utils module is importable from /FileStore\n",
    "if \"/dbfs/FileStore\" not in sys.path:\n",
    "    sys.path.insert(0, \"/dbfs/FileStore\")\n",
    "try:\n",
    "    import utils_etl\n",
    "    importlib.reload(utils_etl)\n",
    "    print(\"Imported utils_etl OK.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to import utils_etl - ensure /FileStore/utils_etl.py exists\")\n",
    "    raise\n",
    "\n",
    "# ----------------------\n",
    "# Widgets / parameters (ADF should pass these)\n",
    "# ----------------------\n",
    "dbutils.widgets.text(\"domain\", \"\")\n",
    "dbutils.widgets.text(\"file_name\", \"\")            # e.g. Sales.Currency or Sales.Currency.csv\n",
    "dbutils.widgets.text(\"column_list\", \"\")          # JSON array OR CSV\n",
    "dbutils.widgets.text(\"year_column\", \"\")          # e.g. ModifiedDate (optional)\n",
    "dbutils.widgets.text(\"table_name\", \"\")           # folder name override (optional)\n",
    "dbutils.widgets.text(\"batch_name\", \"\")           # optional batch id/name from ADF\n",
    "\n",
    "dbutils.widgets.text(\"direct_account_key\", \"\")   # raw storage account key (base64) - recommended secure\n",
    "dbutils.widgets.text(\"BASE_RAW_PATH\", \"\")        # optional override e.g. abfss://project@acct.dfs.core.windows.net\n",
    "dbutils.widgets.text(\"BASE_BRONZE_PATH\", \"\")     # optional override e.g. abfss://bronze@acct.dfs.core.windows.net\n",
    "dbutils.widgets.text(\"include_layer\", \"false\")   # set true to include /<layer> in path (optional)\n",
    "\n",
    "# Read widget values\n",
    "DOMAIN = dbutils.widgets.get(\"domain\").strip()\n",
    "FILE_NAME_IN = dbutils.widgets.get(\"file_name\").strip()\n",
    "COLUMN_LIST_WIDGET = dbutils.widgets.get(\"column_list\").strip()\n",
    "YEAR_COLUMN_WIDGET = dbutils.widgets.get(\"year_column\").strip()\n",
    "TABLE_NAME_WIDGET = dbutils.widgets.get(\"table_name\").strip()\n",
    "BATCH_NAME_WIDGET = dbutils.widgets.get(\"batch_name\").strip()\n",
    "\n",
    "DIRECT_KEY = dbutils.widgets.get(\"direct_account_key\").strip()\n",
    "BASE_RAW_WIDGET = dbutils.widgets.get(\"BASE_RAW_PATH\").strip()\n",
    "BASE_BRONZE_WIDGET = dbutils.widgets.get(\"BASE_BRONZE_PATH\").strip()\n",
    "INCLUDE_LAYER = dbutils.widgets.get(\"include_layer\").strip().lower() in (\"true\",\"1\",\"yes\",\"y\")\n",
    "\n",
    "# Storage config\n",
    "STORAGE_ACCOUNT = \"scrgvkrmade\"\n",
    "RAW_CONTAINER = \"project\"\n",
    "BRONZE_CONTAINER = \"bronze\"\n",
    "\n",
    "# --- helper: detect delimiter by sampling head of file ---\n",
    "def detect_delimiter(file_path, sample_size=8192):\n",
    "    \"\"\"\n",
    "    Returns a best-guess delimiter from [',','|',';','\\t'] by counting occurrences in sample.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = dbutils.fs.head(file_path, sample_size)\n",
    "    except Exception:\n",
    "        # if head fails (e.g. file not found) return default comma\n",
    "        return \",\"\n",
    "    counts = {\n",
    "        \",\": text.count(\",\"),\n",
    "        \"|\": text.count(\"|\"),\n",
    "        \";\": text.count(\";\"),\n",
    "        \"\\t\": text.count(\"\\t\")\n",
    "    }\n",
    "    delim = max(counts, key=counts.get)\n",
    "    if counts[delim] == 0:\n",
    "        return \",\"\n",
    "    return delim\n",
    "\n",
    "# --- set storage key if passed (must do before abfss access) ---\n",
    "def set_storage_key(k):\n",
    "    if not k:\n",
    "        return\n",
    "    # remove surrounding quotes\n",
    "    if (k.startswith('\"') and k.endswith('\"')) or (k.startswith(\"'\") and k.endswith(\"'\")):\n",
    "        k = k[1:-1]\n",
    "    k = k.strip()\n",
    "    if not re.fullmatch(r\"[A-Za-z0-9+/=]{20,300}\", k):\n",
    "        raise Exception(\"direct_account_key looks invalid; ensure you're passing raw account key (base64), not connection string\")\n",
    "    spark.conf.set(f\"fs.azure.account.key.{STORAGE_ACCOUNT}.dfs.core.windows.net\", k)\n",
    "    print(\"Set storage key for\", STORAGE_ACCOUNT)\n",
    "    # quick test listing (optional)\n",
    "    try:\n",
    "        display(dbutils.fs.ls(f\"abfss://{RAW_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/\"))\n",
    "    except Exception as e:\n",
    "        print(\"Warning: test listing raw container failed (invalid key or container).\")\n",
    "        raise\n",
    "\n",
    "if DIRECT_KEY:\n",
    "    set_storage_key(DIRECT_KEY)\n",
    "else:\n",
    "    print(\"No direct_account_key provided; cluster auth must have permissions to read ABFS.\")\n",
    "\n",
    "# resolve base paths (taskValues or widget fallback)\n",
    "def task_or_widget(key, widget_name):\n",
    "    try:\n",
    "        v = dbutils.jobs.taskValues.get(taskKey=key, key=key)\n",
    "        if hasattr(v, \"value\"):\n",
    "            return v.value\n",
    "        return v\n",
    "    except Exception:\n",
    "        return dbutils.widgets.get(widget_name).strip()\n",
    "\n",
    "BASE_RAW_TASK = task_or_widget(\"BASE_RAW_PATH\", \"BASE_RAW_PATH\")\n",
    "BASE_BRONZE_TASK = task_or_widget(\"BASE_BRONZE_PATH\", \"BASE_BRONZE_PATH\")\n",
    "\n",
    "if BASE_RAW_TASK:\n",
    "    BASE_RAW_PATH = BASE_RAW_TASK\n",
    "elif BASE_RAW_WIDGET:\n",
    "    BASE_RAW_PATH = BASE_RAW_WIDGET\n",
    "elif DIRECT_KEY:\n",
    "    BASE_RAW_PATH = f\"abfss://{RAW_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "else:\n",
    "    raise Exception(\"BASE_RAW_PATH not resolved. Provide direct_account_key or BASE_RAW_PATH widget or run 01_Config first.\")\n",
    "\n",
    "if BASE_BRONZE_TASK:\n",
    "    BASE_BRONZE_PATH = BASE_BRONZE_TASK\n",
    "elif BASE_BRONZE_WIDGET:\n",
    "    BASE_BRONZE_PATH = BASE_BRONZE_WIDGET\n",
    "elif DIRECT_KEY:\n",
    "    BASE_BRONZE_PATH = f\"abfss://{BRONZE_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "else:\n",
    "    raise Exception(\"BASE_BRONZE_PATH not resolved. Provide direct_account_key or BASE_BRONZE_PATH widget or run 01_Config first.\")\n",
    "\n",
    "print(\"BASE_RAW_PATH:\", BASE_RAW_PATH)\n",
    "print(\"BASE_BRONZE_PATH:\", BASE_BRONZE_PATH)\n",
    "\n",
    "# --- normalize file name and folder table name ---\n",
    "if not FILE_NAME_IN:\n",
    "    raise Exception(\"file_name widget required (e.g. Sales.Currency or Sales.Currency.csv)\")\n",
    "file_key = FILE_NAME_IN\n",
    "file_key_no_ext = file_key[:-4] if file_key.lower().endswith(\".csv\") else file_key\n",
    "folder_table_name = TABLE_NAME_WIDGET if TABLE_NAME_WIDGET else file_key_no_ext\n",
    "\n",
    "# --- parse column_list (accept JSON array or CSV) ---\n",
    "def parse_column_list(text):\n",
    "    txt = str(text).strip()\n",
    "    if not txt:\n",
    "        return []\n",
    "    txt = txt.replace('\"\"', '\"')\n",
    "    if txt.startswith(\"[\") and txt.endswith(\"]\"):\n",
    "        try:\n",
    "            arr = json.loads(txt)\n",
    "            if isinstance(arr, list):\n",
    "                return [str(x).strip() for x in arr if x and str(x).strip() != \"\"]\n",
    "        except Exception:\n",
    "            # fallback simple parse\n",
    "            txt2 = txt.strip(\"[]\")\n",
    "            return [c.strip().strip('\"').strip(\"'\") for c in txt2.split(\",\") if c.strip() != \"\"]\n",
    "    # comma-separated fallback\n",
    "    return [c.strip().strip('\"').strip(\"'\") for c in txt.split(\",\") if c.strip() != \"\"]\n",
    "\n",
    "if COLUMN_LIST_WIDGET:\n",
    "    columns = parse_column_list(COLUMN_LIST_WIDGET)\n",
    "else:\n",
    "    raise Exception(\"column_list missing. Pass column_list from ADF Lookup (JSON array or CSV).\")\n",
    "\n",
    "if not columns:\n",
    "    raise Exception(\"Parsed columns list is empty. Provide valid column_list.\")\n",
    "\n",
    "# default year_hint to YEAR_COLUMN_WIDGET or ModifiedDate if present\n",
    "year_hint = YEAR_COLUMN_WIDGET if YEAR_COLUMN_WIDGET else None\n",
    "if not year_hint and any(c.lower() == \"modifieddate\" for c in columns):\n",
    "    year_hint = [c for c in columns if c.lower() == \"modifieddate\"][0]\n",
    "    print(\"Using ModifiedDate as year_hint\")\n",
    "\n",
    "# --- build raw path and detect delimiter ---\n",
    "raw_read_name = FILE_NAME_IN if FILE_NAME_IN and FILE_NAME_IN.strip() != \"\" else file_key_no_ext + \".csv\"\n",
    "raw_path = BASE_RAW_PATH.rstrip(\"/\") + \"/\" + raw_read_name.lstrip(\"/\")\n",
    "print(\"Raw file path:\", raw_path)\n",
    "\n",
    "# detect delimiter by sampling first bytes\n",
    "detected_sep = detect_delimiter(raw_path, sample_size=8192)\n",
    "print(\"Detected delimiter:\", repr(detected_sep))\n",
    "\n",
    "# --- read CSV using detected delimiter and robust options ---\n",
    "try:\n",
    "    df_raw = (spark.read\n",
    "                .option(\"header\", \"false\")\n",
    "                .option(\"sep\", detected_sep)\n",
    "                .option(\"quote\", '\"')\n",
    "                .option(\"escape\", \"\\\\\")\n",
    "                .option(\"multiLine\", \"true\")\n",
    "                .option(\"inferSchema\", \"false\")\n",
    "                .csv(raw_path))\n",
    "    print(\"Raw row count:\", df_raw.count())\n",
    "    display(df_raw.limit(5))\n",
    "except Exception:\n",
    "    print(\"Failed to read CSV with detected delimiter; attempting fallback using comma.\")\n",
    "    try:\n",
    "        df_raw = (spark.read\n",
    "                    .option(\"header\", \"false\")\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"quote\", '\"')\n",
    "                    .option(\"escape\", \"\\\\\")\n",
    "                    .option(\"multiLine\", \"true\")\n",
    "                    .option(\"inferSchema\", \"false\")\n",
    "                    .csv(raw_path))\n",
    "        print(\"Fallback read succeeded (comma). Row count:\", df_raw.count())\n",
    "        display(df_raw.limit(5))\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# --- apply headers and trim whitespace ---\n",
    "df_named = utils_etl.add_headers(df_raw, columns)\n",
    "for c in df_named.columns:\n",
    "    df_named = df_named.withColumn(c, trim(col(c)))\n",
    "print(\"After applying headers sample:\")\n",
    "display(df_named.limit(5))\n",
    "\n",
    "# --- extract year column and add audit columns ---\n",
    "df_with_year, used_year = utils_etl.extract_year_column(df_named, year_hint)\n",
    "print(\"Year column used (hint):\", used_year)\n",
    "display(df_with_year.select(\"_year\").distinct())\n",
    "\n",
    "# create audit values\n",
    "_run_id = str(uuid.uuid4())\n",
    "# try to get job/run id from context (best-effort)\n",
    "_job_id = \"\"\n",
    "try:\n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    # different contexts expose different fields â€” best effort to fetch job/run id if available\n",
    "    try:\n",
    "        _job_id = str(ctx.currentRunId().getId())\n",
    "    except Exception:\n",
    "        try:\n",
    "            _job_id = str(ctx.jobId().get())\n",
    "        except Exception:\n",
    "            _job_id = \"\"\n",
    "except Exception:\n",
    "    _job_id = \"\"\n",
    "\n",
    "# add audit columns (keep _year)\n",
    "df_audited = (df_with_year\n",
    "                .withColumn(\"_ingestion_ts\", current_timestamp())\n",
    "                .withColumn(\"_ingestion_date\", current_date())\n",
    "                .withColumn(\"_source_file\", lit(raw_read_name))\n",
    "                .withColumn(\"_source_path\", lit(raw_path))\n",
    "                .withColumn(\"_job_id\", lit(_job_id))\n",
    "                .withColumn(\"_run_id\", lit(_run_id))\n",
    "                .withColumn(\"_batch_id\", lit(BATCH_NAME_WIDGET if BATCH_NAME_WIDGET else \"\")))\n",
    "\n",
    "print(\"Added audit columns. Sample:\")\n",
    "display(df_audited.limit(5))\n",
    "\n",
    "# ----------------------\n",
    "# Build bronze base path (no extra /Layer by default)\n",
    "# ----------------------\n",
    "if INCLUDE_LAYER:\n",
    "    bronze_base = BASE_BRONZE_PATH.rstrip(\"/\") + f\"/{DOMAIN}/{'Bronze'}\"\n",
    "else:\n",
    "    bronze_base = BASE_BRONZE_PATH.rstrip(\"/\") + f\"/{DOMAIN}\"\n",
    "\n",
    "print(\"Writing parquet to:\", bronze_base)\n",
    "\n",
    "# Write parquet by year (utils writes folder per year; it drops only the _year column before writing)\n",
    "utils_etl.write_parquet_by_year(df_audited, bronze_base, folder_table_name, compression=\"snappy\", coalesce_out=True, write_mode=\"overwrite\")\n",
    "\n",
    "# --- confirm outputs ---\n",
    "years = [r[\"_year\"] for r in df_audited.select(\"_year\").distinct().collect()]\n",
    "print(\"Outputs written for years:\", years)\n",
    "for y in years:\n",
    "    out_path = f\"{bronze_base.rstrip('/')}/{folder_table_name}/{y}\"\n",
    "    print(\"Listing:\", out_path)\n",
    "    try:\n",
    "        for f in dbutils.fs.ls(out_path):\n",
    "            print(\" -\", f.path)\n",
    "    except Exception as e:\n",
    "        print(\"Could not list:\", out_path, e)\n",
    "\n",
    "print(\"03_Run finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Run_new",
   "widgets": {
    "BASE_BRONZE_PATH": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/bronze/",
     "nuid": "6c44e155-8331-4f6b-8e98-31d033ca6012",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "BASE_BRONZE_PATH",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "BASE_BRONZE_PATH",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "BASE_RAW_PATH": {
     "currentValue": "abfss://project@scrgvkrmade.dfs.core.windows.net/raw/",
     "nuid": "9d69d7f4-4058-49f8-ace7-9d9c4392c84f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "BASE_RAW_PATH",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "BASE_RAW_PATH",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "batch_name": {
     "currentValue": "RS_Raw_06",
     "nuid": "b52e5cef-f173-4d5b-ad9c-d17c1d862899",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "batch_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "batch_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "column_list": {
     "currentValue": "[\"TerritoryID\", \"Name\", \"CountryRegionCode\", \"Group\", \"SalesYTD\", \"SalesLastYear\", \"CostYTD\", \"CostLastYear\", \"rowguid\", \"ModifiedDate\"]",
     "nuid": "e4f5af33-96aa-4848-a891-e2e9a4a2e4ad",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "column_list",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "column_list",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "direct_account_key": {
     "currentValue": "E4VB7pXWFXttUWbbSBPY35/Dvsw6Fs6XgIWLTj3lCS6v/jCEow9Uxs+r6Usobhenv14UdWEzb+R8+AStNyS0dg==",
     "nuid": "22540682-2e82-4258-a698-32b7e69557be",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "direct_account_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "direct_account_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "domain": {
     "currentValue": "ResellerSales",
     "nuid": "473dd358-d3e2-4e68-a6c9-cefbc078205a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "domain",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "domain",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "file_name": {
     "currentValue": "SalesTerritory.csv",
     "nuid": "3a0b4bd9-af1f-4e68-95c1-1f8a263378c1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "file_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "file_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "include_layer": {
     "currentValue": "false",
     "nuid": "93d630f0-4187-4f86-8380-2d221b5327a3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "include_layer",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": null,
      "name": "include_layer",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "Sales.SalesTerritory",
     "nuid": "6bce7378-8f28-489a-addf-cf78dda8b926",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "year_column": {
     "currentValue": "ModifiedDate",
     "nuid": "6e285f87-5a67-4b15-9a45-9b0764e48b28",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "year_column",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "year_column",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
